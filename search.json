[
  {
    "objectID": "posts/kaggle_nlp_disaster/main.html",
    "href": "posts/kaggle_nlp_disaster/main.html",
    "title": "Identificando desastres en Twitter con NLP",
    "section": "",
    "text": "Goal\nI’m learning NLP. So to do that I decided pass trough diverse NLP models, study the teory and code them! I think that it is a good method to learn Machine Learning things. So, Disclaimer: All the content on the notebooks is what I understood from diverse references (I will put the links), somethings could be wrong, If you find any mistake please let me know, so I can learn of it. Also, if you have some doubt, it will be a pleasure to me to answer it (as long as I have the answer).\nAt the end, I achieve a score of 0.843 in the LB. Is beatifull to see how you are improving the solutions step by step!\nSo, this will be the embedding Notebook, I will put the link to each specific notebook here (So it will be more readable).\nMethodologies & Notebooks:\n\n\n\nModel Notebook\nScore\n\n\n\n\nSimple Neural Network ( View on kaggle)\n0.56\n\n\nEmbeddings ( View on kaggle)\n0.797\n\n\nRecurrent Neural Networks ( View on kaggle)\n0.809\n\n\nBERT & HuggingFace ( View on kaggle)\n0.824\n\n\nMyBestSolution ( View on kaggle)\n0.843\n\n\n\n\n\nEDA\nHere I will do some preprocessing and split the data. I will use that data to each notebook!\nI think there is a lot of notebooks with a beatifull EDA, So I won’t take to much around this.\n\nLibraries\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\nData\n\ntrain_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntrain_df.head()\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n1\nNaN\nNaN\nOur Deeds are the Reason of this #earthquake M...\n1\n\n\n1\n4\nNaN\nNaN\nForest fire near La Ronge Sask. Canada\n1\n\n\n2\n5\nNaN\nNaN\nAll residents asked to 'shelter in place' are ...\n1\n\n\n3\n6\nNaN\nNaN\n13,000 people receive #wildfires evacuation or...\n1\n\n\n4\n7\nNaN\nNaN\nJust got sent this photo from Ruby #Alaska as ...\n1\n\n\n\n\n\n\n\n\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntest_df.head()\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n\n\n\n\n\n\n\n\n\nEDA\n\n# Target Proportion\nsns.countplot(data=train_df, x = \"target\")\n\n&lt;AxesSubplot:xlabel='target', ylabel='count'&gt;\n\n\n\n\n\nI think it is balanced!\n\n# Random example of disaster tweet\ntrain_df[train_df.target == 1].sample(1).text.values[0]\n\n'California Bush fires please evacuate affected areas ASAP when california govts advised you to do so http://t.co/ubVEVUuAch'\n\n\n\n# Random example of NO disaster tweet\ntrain_df[train_df.target == 0].sample(1).text.values[0]\n\n'Someone split a mudslide w me when I get off work'\n\n\n\n\nPre-processing\nI will do some preprocessing with Tensorflow!\n\n# Input Tensor Data\ntext = tf.data.Dataset.from_tensor_slices(train_df.text)\ntext\n\n2022-12-11 15:20:58.687517: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: (), types: tf.string&gt;\n\n\nNote that Im reading data from memory! If it would huge data I would be in troubles!\nOne advantage of initialize a Tensorflow dataset is that I will be able to create a data pipeline (batch, fetch, shuffle, etc.)\n\n# some samples\nlist(text.take(2).as_numpy_iterator())\n\n[b'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n b'Forest fire near La Ronge Sask. Canada']\n\n\nWe need to know that models don’t understand text by itself! Just numbers! For this, we vectorize the sentences. I don’t plan to use a model now, I would like to observe wich words are more present by target! (Also I don’t want to consider stopwords)\nI will use the tensorflow layer: Text Vectorization. from behind, it apply lowercase and delete punctuation. I also wants to remove stopwords, so I will build a custom standarization that do: 1. lowercase 2. strip punctuation 3. remove stop words!\nClick here if you don’t know what are stop words\n\n#### COUNT WORDS BY TARGET\n\ndef custom_standardization(inputs):\n    \"\"\"\n    Apply: lowercase, remove punctuation and stopwords\n    \"\"\"\n    PUNCTUATION = r'[!\"#$%&()\\*\\+,-\\./:;&lt;=&gt;?@\\[\\\\\\]^_`{|}~\\']'\n    lowercase = tf.strings.lower(inputs) # lowercase\n    strip = tf.strings.regex_replace(lowercase, PUNCTUATION, '') # strip punctuation\n    stopwrd = tf.strings.regex_replace(strip, r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*', '')\n    return stopwrd\n    \n\n# model to apply vectorize_layer with custom standardization\nvectorize_layer = tf.keras.layers.TextVectorization(output_mode = 'multi_hot', standardize = custom_standardization)\nvectorize_layer.adapt(text)\n\n# model to vectorize\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.Input(shape=(1,), dtype=tf.string))\nmodel.add(vectorize_layer)\n\n# make counter\ntrain_count = model.predict(text.batch(batch_size = len(text))) # predict to count \ntoken_counts = pd.DataFrame(columns = vectorize_layer.get_vocabulary(), data = train_count) # df with tokens and count\ntrain_df.rename(columns = {\"target\":\"disaster_target\"}, inplace = True) # rename target because there is a word target in data\ncount_df = pd.concat([train_df, token_counts], axis = 1) #concat\ngroup_count = count_df.iloc[:,4:].groupby(\"disaster_target\", as_index = False).sum() # count token for each target\nmelt_count = pd.melt(group_count, id_vars=[\"disaster_target\"], value_name = \"count\") # each token to row\nmelt_count.sort_values(by=[\"count\"], ascending = False).head(30)\n\n2022-12-11 15:20:58.997911: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: FutureWarning: This dataframe has a column name that matches the 'value_name' column name of the resulting Dataframe. In the future this will raise an error, please set the 'value_name' parameter of DataFrame.melt to a unique name.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\n\n\n\n\n\n\ndisaster_target\nvariable\ncount\n\n\n\n\n2\n0\nlike\n239.0\n\n\n4\n0\nim\n221.0\n\n\n6\n0\namp\n174.0\n\n\n12\n0\nnew\n163.0\n\n\n9\n1\nfire\n162.0\n\n\n10\n0\nget\n158.0\n\n\n22\n0\ndont\n136.0\n\n\n21\n1\nnews\n130.0\n\n\n18\n0\none\n122.0\n\n\n15\n1\nvia\n121.0\n\n\n42\n0\nbody\n110.0\n\n\n51\n1\ncalifornia\n108.0\n\n\n53\n1\nsuicide\n104.0\n\n\n17\n1\npeople\n101.0\n\n\n37\n1\npolice\n97.0\n\n\n35\n1\ndisaster\n96.0\n\n\n14\n0\nvia\n96.0\n\n\n7\n1\namp\n95.0\n\n\n38\n0\nwould\n93.0\n\n\n95\n1\nkilled\n90.0\n\n\n24\n0\nvideo\n90.0\n\n\n16\n0\npeople\n90.0\n\n\n3\n1\nlike\n88.0\n\n\n117\n1\nhiroshima\n84.0\n\n\n87\n1\nfires\n82.0\n\n\n62\n0\nknow\n82.0\n\n\n28\n0\n2\n81.0\n\n\n104\n0\nfull\n81.0\n\n\n84\n0\nlove\n81.0\n\n\n58\n0\ntime\n80.0\n\n\n\n\n\n\n\nAfter I build it I realized that it is not necessary to instantiate a model to use layers! 😅\n\n\n\nSplit Data\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(train_df[[col for col in train_df.columns if col != 'disaster_target']], train_df.disaster_target, test_size=0.2, random_state=13)\n\n\n# To csv (In some notebooks I will use this data)\npd.concat([X_train, y_train], axis = 1).to_csv('df_train.csv',index = False)\npd.concat([X_test, y_test], axis = 1).to_csv('df_test.csv',index = False)\n\nThis dataset is here: https://www.kaggle.com/datasets/diegomachado/df-split\n\n# Delete it from memory\ndel train_df, test_df, X_train, X_test, y_train, y_test\ngc.collect()\n\n671"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html",
    "title": "1. Simple Neural Network",
    "section": "",
    "text": "In this notebook I will use a simple Neural Network Arquitecture. We need to remember that we only can feed numbers to NN !\nThe Arquitecture will look like this:\n\n\n\nNLP-NN.png\n\n\nRemember that is belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\n\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-11 14:55:06.219408: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nInstantiate TextVectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 50 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-11 14:55:06.451771: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\n\n# Testing the Vectorization layer\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return vectorization_layer(text)\n\n# retrieve a batch (of 1 review and label) from the dataset\ntext_batch, label_batch = next(iter(train_ds))\nfirst_review, first_label = text_batch, label_batch\nprint(\"Review\", first_review)\nprint(\"Label\", first_label)\nprint(\"Vectorized review\", vectorize_text(first_review))\n\nReview tf.Tensor(b'Riot Kit Bah - part of the new concept Gear coming for Autumn/Winter\\n#menswear #fashion #urbanfashion\\xc2\\x89\\xc3\\x9b_ https://t.co/cCwzDTFbUS', shape=(), dtype=string)\nLabel tf.Tensor(0, shape=(), dtype=int64)\nVectorized review tf.Tensor(\n[[ 403 1278    1  572    6    2   44 3700    1  250   10    1    1  968\n  6559    1    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0]], shape=(1, 50), dtype=int64)\n\n\n🔍 We see that the length of the vector is output_sequence_length = 250, and if the statement is not that long, it is paded with 0. We also note that no integer is greater than 10000, since we set this as our maximum vocabulary of tokens (vocabulary_size()). We also see that there are some integers = 1. This in the vocabulary corresponds to UNK, that is, unknown words (tokens), this is because our vocabulary is limited (TextVectorization chooses the 10000 most frequent ones).\nAn improvement can be to analyze the length of the dataset sentences and choose an output_sequence_lenght based on this (to avoid to much pad)\n\nvectorization_layer.vocabulary_size()\n\n10000\n\n\n\n# To see the vocabulary\nvectorization_layer.get_vocabulary()[:10]\n\n['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nWe will build a simple Sequential Model composed by two Dense Layers. I dont take to much time tunning it.\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string), # Input Layer\n    vectorization_layer, # Vectorization Layer\n    layers.Dense(64),\n    #layers.Dropout(0.1),\n    layers.Dense(32),\n    #layers.Dropout(0.1),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 50)                0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                3264      \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 5,377\nTrainable params: 5,377\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n🔍 Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\n\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs)\n\nEpoch 1/100\n191/191 [==============================] - 2s 4ms/step - loss: 76.1297 - binary_accuracy: 0.5123 - val_loss: 30.4719 - val_binary_accuracy: 0.5207\nEpoch 2/100\n191/191 [==============================] - 1s 3ms/step - loss: 37.2703 - binary_accuracy: 0.5167 - val_loss: 22.0519 - val_binary_accuracy: 0.5194\nEpoch 3/100\n191/191 [==============================] - 1s 3ms/step - loss: 34.7663 - binary_accuracy: 0.5118 - val_loss: 27.2791 - val_binary_accuracy: 0.5141\nEpoch 4/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.8605 - binary_accuracy: 0.5135 - val_loss: 26.1287 - val_binary_accuracy: 0.5253\nEpoch 5/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.4162 - binary_accuracy: 0.5123 - val_loss: 24.3497 - val_binary_accuracy: 0.5108\nEpoch 6/100\n191/191 [==============================] - 1s 3ms/step - loss: 26.5699 - binary_accuracy: 0.5084 - val_loss: 17.0479 - val_binary_accuracy: 0.5148\nEpoch 7/100\n191/191 [==============================] - 1s 3ms/step - loss: 24.1579 - binary_accuracy: 0.5136 - val_loss: 24.3595 - val_binary_accuracy: 0.5351\nEpoch 8/100\n191/191 [==============================] - 1s 3ms/step - loss: 22.3645 - binary_accuracy: 0.5110 - val_loss: 18.3034 - val_binary_accuracy: 0.5108\nEpoch 9/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.4826 - binary_accuracy: 0.5059 - val_loss: 17.5856 - val_binary_accuracy: 0.5161\nEpoch 10/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.1403 - binary_accuracy: 0.5156 - val_loss: 15.4736 - val_binary_accuracy: 0.5299\nEpoch 11/100\n191/191 [==============================] - 1s 3ms/step - loss: 16.6029 - binary_accuracy: 0.5156 - val_loss: 13.5105 - val_binary_accuracy: 0.5220\nEpoch 12/100\n191/191 [==============================] - 0s 3ms/step - loss: 16.8898 - binary_accuracy: 0.5166 - val_loss: 13.1816 - val_binary_accuracy: 0.5253\nEpoch 13/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.9538 - binary_accuracy: 0.5151 - val_loss: 14.4587 - val_binary_accuracy: 0.5391\nEpoch 14/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.0781 - binary_accuracy: 0.5085 - val_loss: 11.7277 - val_binary_accuracy: 0.5312\nEpoch 15/100\n191/191 [==============================] - 1s 3ms/step - loss: 13.2963 - binary_accuracy: 0.5072 - val_loss: 8.4516 - val_binary_accuracy: 0.5167\nEpoch 16/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.8151 - binary_accuracy: 0.5125 - val_loss: 13.2906 - val_binary_accuracy: 0.5351\nEpoch 17/100\n191/191 [==============================] - 1s 3ms/step - loss: 12.2239 - binary_accuracy: 0.5136 - val_loss: 10.8708 - val_binary_accuracy: 0.5378\nEpoch 18/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.7989 - binary_accuracy: 0.5046 - val_loss: 8.7974 - val_binary_accuracy: 0.5299\nEpoch 19/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5223 - binary_accuracy: 0.5103 - val_loss: 9.4426 - val_binary_accuracy: 0.5253\nEpoch 20/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5585 - binary_accuracy: 0.5122 - val_loss: 9.8687 - val_binary_accuracy: 0.5246\nEpoch 21/100\n191/191 [==============================] - 1s 3ms/step - loss: 9.2616 - binary_accuracy: 0.5167 - val_loss: 11.9928 - val_binary_accuracy: 0.5253\nEpoch 22/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.1322 - binary_accuracy: 0.5213 - val_loss: 7.8273 - val_binary_accuracy: 0.4990\nEpoch 23/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.4561 - binary_accuracy: 0.5200 - val_loss: 9.2781 - val_binary_accuracy: 0.5174\nEpoch 24/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.0963 - binary_accuracy: 0.5166 - val_loss: 5.4576 - val_binary_accuracy: 0.5167\nEpoch 25/100\n191/191 [==============================] - 1s 3ms/step - loss: 7.1907 - binary_accuracy: 0.5122 - val_loss: 8.8754 - val_binary_accuracy: 0.5003\nEpoch 26/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.9971 - binary_accuracy: 0.5190 - val_loss: 7.2822 - val_binary_accuracy: 0.5095\nEpoch 27/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.5726 - binary_accuracy: 0.5105 - val_loss: 5.3474 - val_binary_accuracy: 0.4879\nEpoch 28/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.0869 - binary_accuracy: 0.5243 - val_loss: 5.1194 - val_binary_accuracy: 0.5174\nEpoch 29/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.7851 - binary_accuracy: 0.5125 - val_loss: 6.9937 - val_binary_accuracy: 0.5076\nEpoch 30/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.1723 - binary_accuracy: 0.5148 - val_loss: 5.7589 - val_binary_accuracy: 0.5062\nEpoch 31/100\n191/191 [==============================] - 1s 4ms/step - loss: 5.1466 - binary_accuracy: 0.5125 - val_loss: 3.9673 - val_binary_accuracy: 0.5181\nEpoch 32/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.2948 - binary_accuracy: 0.5186 - val_loss: 5.4504 - val_binary_accuracy: 0.5089\nEpoch 33/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.4029 - binary_accuracy: 0.5095 - val_loss: 2.8821 - val_binary_accuracy: 0.5128\nEpoch 34/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.0808 - binary_accuracy: 0.5099 - val_loss: 3.8832 - val_binary_accuracy: 0.5161\nEpoch 35/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7501 - binary_accuracy: 0.5174 - val_loss: 3.9605 - val_binary_accuracy: 0.5135\nEpoch 36/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7382 - binary_accuracy: 0.5102 - val_loss: 3.6448 - val_binary_accuracy: 0.5115\nEpoch 37/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.1055 - binary_accuracy: 0.5245 - val_loss: 3.9398 - val_binary_accuracy: 0.5056\nEpoch 38/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.9880 - binary_accuracy: 0.5151 - val_loss: 2.8102 - val_binary_accuracy: 0.5082\nEpoch 39/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.7847 - binary_accuracy: 0.5215 - val_loss: 2.1289 - val_binary_accuracy: 0.5187\nEpoch 40/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.8415 - binary_accuracy: 0.5241 - val_loss: 2.8795 - val_binary_accuracy: 0.5115\nEpoch 41/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.5144 - binary_accuracy: 0.5223 - val_loss: 2.5149 - val_binary_accuracy: 0.5213\nEpoch 42/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0964 - binary_accuracy: 0.5287 - val_loss: 2.0305 - val_binary_accuracy: 0.5207\nEpoch 43/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.1527 - binary_accuracy: 0.5158 - val_loss: 1.8447 - val_binary_accuracy: 0.5095\nEpoch 44/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0041 - binary_accuracy: 0.5296 - val_loss: 1.7671 - val_binary_accuracy: 0.5095\nEpoch 45/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.7718 - binary_accuracy: 0.5232 - val_loss: 2.0167 - val_binary_accuracy: 0.5121\nEpoch 46/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.8157 - binary_accuracy: 0.5159 - val_loss: 1.7764 - val_binary_accuracy: 0.5227\nEpoch 47/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6907 - binary_accuracy: 0.5281 - val_loss: 1.3804 - val_binary_accuracy: 0.5233\nEpoch 48/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.5388 - binary_accuracy: 0.5233 - val_loss: 1.4022 - val_binary_accuracy: 0.5213\nEpoch 49/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6031 - binary_accuracy: 0.5259 - val_loss: 1.3513 - val_binary_accuracy: 0.5253\nEpoch 50/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.3865 - binary_accuracy: 0.5271 - val_loss: 1.1486 - val_binary_accuracy: 0.5246\nEpoch 51/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2101 - binary_accuracy: 0.5269 - val_loss: 1.0696 - val_binary_accuracy: 0.5227\nEpoch 52/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2028 - binary_accuracy: 0.5368 - val_loss: 1.1062 - val_binary_accuracy: 0.5292\nEpoch 53/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1968 - binary_accuracy: 0.5332 - val_loss: 0.9421 - val_binary_accuracy: 0.5318\nEpoch 54/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1506 - binary_accuracy: 0.5340 - val_loss: 1.0106 - val_binary_accuracy: 0.5338\nEpoch 55/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2173 - binary_accuracy: 0.5281 - val_loss: 0.9825 - val_binary_accuracy: 0.5378\nEpoch 56/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1079 - binary_accuracy: 0.5296 - val_loss: 0.8303 - val_binary_accuracy: 0.5515\nEpoch 57/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9980 - binary_accuracy: 0.5378 - val_loss: 0.8406 - val_binary_accuracy: 0.5456\nEpoch 58/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1121 - binary_accuracy: 0.5363 - val_loss: 1.8118 - val_binary_accuracy: 0.5548\nEpoch 59/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0127 - binary_accuracy: 0.5388 - val_loss: 0.9639 - val_binary_accuracy: 0.5483\nEpoch 60/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9924 - binary_accuracy: 0.5374 - val_loss: 0.8626 - val_binary_accuracy: 0.5483\nEpoch 61/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8680 - binary_accuracy: 0.5399 - val_loss: 0.8083 - val_binary_accuracy: 0.5542\nEpoch 62/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5440 - val_loss: 1.2377 - val_binary_accuracy: 0.5581\nEpoch 63/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9144 - binary_accuracy: 0.5396 - val_loss: 0.7873 - val_binary_accuracy: 0.5522\nEpoch 64/100\n191/191 [==============================] - 0s 3ms/step - loss: 0.9946 - binary_accuracy: 0.5343 - val_loss: 0.8474 - val_binary_accuracy: 0.5463\nEpoch 65/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9548 - binary_accuracy: 0.5365 - val_loss: 0.7671 - val_binary_accuracy: 0.5568\nEpoch 66/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9172 - binary_accuracy: 0.5432 - val_loss: 1.1670 - val_binary_accuracy: 0.5535\nEpoch 67/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9938 - binary_accuracy: 0.5473 - val_loss: 1.7050 - val_binary_accuracy: 0.5575\nEpoch 68/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8954 - binary_accuracy: 0.5394 - val_loss: 0.7633 - val_binary_accuracy: 0.5483\nEpoch 69/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8815 - binary_accuracy: 0.5460 - val_loss: 0.9965 - val_binary_accuracy: 0.5561\nEpoch 70/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5438 - val_loss: 1.0431 - val_binary_accuracy: 0.5509\nEpoch 71/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8825 - binary_accuracy: 0.5483 - val_loss: 1.0019 - val_binary_accuracy: 0.5601\nEpoch 72/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9897 - binary_accuracy: 0.5479 - val_loss: 1.3509 - val_binary_accuracy: 0.5502\nEpoch 73/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9775 - binary_accuracy: 0.5427 - val_loss: 0.9652 - val_binary_accuracy: 0.5581\nEpoch 74/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0205 - binary_accuracy: 0.5445 - val_loss: 1.2212 - val_binary_accuracy: 0.5614\nEpoch 75/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0616 - binary_accuracy: 0.5432 - val_loss: 1.1886 - val_binary_accuracy: 0.5575\nEpoch 76/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9314 - binary_accuracy: 0.5468 - val_loss: 1.3687 - val_binary_accuracy: 0.5548\nEpoch 77/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8521 - binary_accuracy: 0.5470 - val_loss: 0.7695 - val_binary_accuracy: 0.5548\nEpoch 78/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8151 - binary_accuracy: 0.5517 - val_loss: 0.9690 - val_binary_accuracy: 0.5509\nEpoch 79/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9553 - binary_accuracy: 0.5453 - val_loss: 0.8552 - val_binary_accuracy: 0.5575\nEpoch 80/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8296 - binary_accuracy: 0.5512 - val_loss: 0.7398 - val_binary_accuracy: 0.5555\nEpoch 81/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8471 - binary_accuracy: 0.5548 - val_loss: 0.7572 - val_binary_accuracy: 0.5529\nEpoch 82/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8430 - binary_accuracy: 0.5456 - val_loss: 0.7894 - val_binary_accuracy: 0.5575\nEpoch 83/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8508 - binary_accuracy: 0.5471 - val_loss: 0.9579 - val_binary_accuracy: 0.5476\nEpoch 84/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8445 - binary_accuracy: 0.5537 - val_loss: 0.8346 - val_binary_accuracy: 0.5581\nEpoch 85/100\n191/191 [==============================] - 1s 4ms/step - loss: 0.8542 - binary_accuracy: 0.5511 - val_loss: 0.7923 - val_binary_accuracy: 0.5542\nEpoch 86/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9084 - binary_accuracy: 0.5481 - val_loss: 0.9421 - val_binary_accuracy: 0.5575\nEpoch 87/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9367 - binary_accuracy: 0.5463 - val_loss: 0.9959 - val_binary_accuracy: 0.5450\nEpoch 88/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9629 - binary_accuracy: 0.5498 - val_loss: 1.1058 - val_binary_accuracy: 0.5607\nEpoch 89/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9141 - binary_accuracy: 0.5365 - val_loss: 0.7433 - val_binary_accuracy: 0.5561\nEpoch 90/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8145 - binary_accuracy: 0.5478 - val_loss: 0.7720 - val_binary_accuracy: 0.5529\nEpoch 91/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.7985 - binary_accuracy: 0.5560 - val_loss: 0.7784 - val_binary_accuracy: 0.5581\nEpoch 92/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8700 - binary_accuracy: 0.5468 - val_loss: 0.8171 - val_binary_accuracy: 0.5594\nEpoch 93/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8868 - binary_accuracy: 0.5502 - val_loss: 0.9612 - val_binary_accuracy: 0.5594\nEpoch 94/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8591 - binary_accuracy: 0.5493 - val_loss: 0.8276 - val_binary_accuracy: 0.5581\nEpoch 95/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8683 - binary_accuracy: 0.5435 - val_loss: 0.8003 - val_binary_accuracy: 0.5581\nEpoch 96/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8546 - binary_accuracy: 0.5415 - val_loss: 0.7477 - val_binary_accuracy: 0.5555\nEpoch 97/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9054 - binary_accuracy: 0.5484 - val_loss: 1.2473 - val_binary_accuracy: 0.5575\nEpoch 98/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8245 - binary_accuracy: 0.5479 - val_loss: 0.7795 - val_binary_accuracy: 0.5515\nEpoch 99/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8624 - binary_accuracy: 0.5435 - val_loss: 0.9183 - val_binary_accuracy: 0.5529\nEpoch 100/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8200 - binary_accuracy: 0.5470 - val_loss: 0.7893 - val_binary_accuracy: 0.5509\n\n\n\n\n\nNow we will predict the test to do the submission\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[-0.83098686],\n       [-0.71497   ],\n       [-0.4031287 ],\n       ...,\n       [-0.3488861 ],\n       [-0.4957207 ],\n       [-1.3229517 ]], dtype=float32)\n\n\n📝 Note that those are the logits!\nTo get a prediction we will compute the sigmoid function and round it to 1 or 0! (Thats because they are 2 classes, if there would be multi class classification then we would need Softmax Function)\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds)) # We use simple round function\ntest_df[\"target\"] = test_df[\"target\"].astype(int) # Submission needs int prediction\n\n\ntest_df.target.value_counts()\n\n0    3023\n1     240\nName: target, dtype: int64\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n0\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n9\n0\n\n\n4\n11\n0\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n0\n\n\n3260\n10868\n0\n\n\n3261\n10874\n0\n\n\n3262\n10875\n0\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\n#sub.to_csv(\"NN_submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#libraries",
    "href": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#libraries",
    "title": "1. Simple Neural Network",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#data",
    "href": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#data",
    "title": "1. Simple Neural Network",
    "section": "",
    "text": "# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-11 14:55:06.219408: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nInstantiate TextVectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 50 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-11 14:55:06.451771: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\n\n# Testing the Vectorization layer\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return vectorization_layer(text)\n\n# retrieve a batch (of 1 review and label) from the dataset\ntext_batch, label_batch = next(iter(train_ds))\nfirst_review, first_label = text_batch, label_batch\nprint(\"Review\", first_review)\nprint(\"Label\", first_label)\nprint(\"Vectorized review\", vectorize_text(first_review))\n\nReview tf.Tensor(b'Riot Kit Bah - part of the new concept Gear coming for Autumn/Winter\\n#menswear #fashion #urbanfashion\\xc2\\x89\\xc3\\x9b_ https://t.co/cCwzDTFbUS', shape=(), dtype=string)\nLabel tf.Tensor(0, shape=(), dtype=int64)\nVectorized review tf.Tensor(\n[[ 403 1278    1  572    6    2   44 3700    1  250   10    1    1  968\n  6559    1    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0]], shape=(1, 50), dtype=int64)\n\n\n🔍 We see that the length of the vector is output_sequence_length = 250, and if the statement is not that long, it is paded with 0. We also note that no integer is greater than 10000, since we set this as our maximum vocabulary of tokens (vocabulary_size()). We also see that there are some integers = 1. This in the vocabulary corresponds to UNK, that is, unknown words (tokens), this is because our vocabulary is limited (TextVectorization chooses the 10000 most frequent ones).\nAn improvement can be to analyze the length of the dataset sentences and choose an output_sequence_lenght based on this (to avoid to much pad)\n\nvectorization_layer.vocabulary_size()\n\n10000\n\n\n\n# To see the vocabulary\nvectorization_layer.get_vocabulary()[:10]\n\n['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nWe will build a simple Sequential Model composed by two Dense Layers. I dont take to much time tunning it.\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string), # Input Layer\n    vectorization_layer, # Vectorization Layer\n    layers.Dense(64),\n    #layers.Dropout(0.1),\n    layers.Dense(32),\n    #layers.Dropout(0.1),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 50)                0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                3264      \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 5,377\nTrainable params: 5,377\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n🔍 Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\n\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs)\n\nEpoch 1/100\n191/191 [==============================] - 2s 4ms/step - loss: 76.1297 - binary_accuracy: 0.5123 - val_loss: 30.4719 - val_binary_accuracy: 0.5207\nEpoch 2/100\n191/191 [==============================] - 1s 3ms/step - loss: 37.2703 - binary_accuracy: 0.5167 - val_loss: 22.0519 - val_binary_accuracy: 0.5194\nEpoch 3/100\n191/191 [==============================] - 1s 3ms/step - loss: 34.7663 - binary_accuracy: 0.5118 - val_loss: 27.2791 - val_binary_accuracy: 0.5141\nEpoch 4/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.8605 - binary_accuracy: 0.5135 - val_loss: 26.1287 - val_binary_accuracy: 0.5253\nEpoch 5/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.4162 - binary_accuracy: 0.5123 - val_loss: 24.3497 - val_binary_accuracy: 0.5108\nEpoch 6/100\n191/191 [==============================] - 1s 3ms/step - loss: 26.5699 - binary_accuracy: 0.5084 - val_loss: 17.0479 - val_binary_accuracy: 0.5148\nEpoch 7/100\n191/191 [==============================] - 1s 3ms/step - loss: 24.1579 - binary_accuracy: 0.5136 - val_loss: 24.3595 - val_binary_accuracy: 0.5351\nEpoch 8/100\n191/191 [==============================] - 1s 3ms/step - loss: 22.3645 - binary_accuracy: 0.5110 - val_loss: 18.3034 - val_binary_accuracy: 0.5108\nEpoch 9/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.4826 - binary_accuracy: 0.5059 - val_loss: 17.5856 - val_binary_accuracy: 0.5161\nEpoch 10/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.1403 - binary_accuracy: 0.5156 - val_loss: 15.4736 - val_binary_accuracy: 0.5299\nEpoch 11/100\n191/191 [==============================] - 1s 3ms/step - loss: 16.6029 - binary_accuracy: 0.5156 - val_loss: 13.5105 - val_binary_accuracy: 0.5220\nEpoch 12/100\n191/191 [==============================] - 0s 3ms/step - loss: 16.8898 - binary_accuracy: 0.5166 - val_loss: 13.1816 - val_binary_accuracy: 0.5253\nEpoch 13/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.9538 - binary_accuracy: 0.5151 - val_loss: 14.4587 - val_binary_accuracy: 0.5391\nEpoch 14/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.0781 - binary_accuracy: 0.5085 - val_loss: 11.7277 - val_binary_accuracy: 0.5312\nEpoch 15/100\n191/191 [==============================] - 1s 3ms/step - loss: 13.2963 - binary_accuracy: 0.5072 - val_loss: 8.4516 - val_binary_accuracy: 0.5167\nEpoch 16/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.8151 - binary_accuracy: 0.5125 - val_loss: 13.2906 - val_binary_accuracy: 0.5351\nEpoch 17/100\n191/191 [==============================] - 1s 3ms/step - loss: 12.2239 - binary_accuracy: 0.5136 - val_loss: 10.8708 - val_binary_accuracy: 0.5378\nEpoch 18/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.7989 - binary_accuracy: 0.5046 - val_loss: 8.7974 - val_binary_accuracy: 0.5299\nEpoch 19/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5223 - binary_accuracy: 0.5103 - val_loss: 9.4426 - val_binary_accuracy: 0.5253\nEpoch 20/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5585 - binary_accuracy: 0.5122 - val_loss: 9.8687 - val_binary_accuracy: 0.5246\nEpoch 21/100\n191/191 [==============================] - 1s 3ms/step - loss: 9.2616 - binary_accuracy: 0.5167 - val_loss: 11.9928 - val_binary_accuracy: 0.5253\nEpoch 22/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.1322 - binary_accuracy: 0.5213 - val_loss: 7.8273 - val_binary_accuracy: 0.4990\nEpoch 23/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.4561 - binary_accuracy: 0.5200 - val_loss: 9.2781 - val_binary_accuracy: 0.5174\nEpoch 24/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.0963 - binary_accuracy: 0.5166 - val_loss: 5.4576 - val_binary_accuracy: 0.5167\nEpoch 25/100\n191/191 [==============================] - 1s 3ms/step - loss: 7.1907 - binary_accuracy: 0.5122 - val_loss: 8.8754 - val_binary_accuracy: 0.5003\nEpoch 26/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.9971 - binary_accuracy: 0.5190 - val_loss: 7.2822 - val_binary_accuracy: 0.5095\nEpoch 27/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.5726 - binary_accuracy: 0.5105 - val_loss: 5.3474 - val_binary_accuracy: 0.4879\nEpoch 28/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.0869 - binary_accuracy: 0.5243 - val_loss: 5.1194 - val_binary_accuracy: 0.5174\nEpoch 29/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.7851 - binary_accuracy: 0.5125 - val_loss: 6.9937 - val_binary_accuracy: 0.5076\nEpoch 30/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.1723 - binary_accuracy: 0.5148 - val_loss: 5.7589 - val_binary_accuracy: 0.5062\nEpoch 31/100\n191/191 [==============================] - 1s 4ms/step - loss: 5.1466 - binary_accuracy: 0.5125 - val_loss: 3.9673 - val_binary_accuracy: 0.5181\nEpoch 32/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.2948 - binary_accuracy: 0.5186 - val_loss: 5.4504 - val_binary_accuracy: 0.5089\nEpoch 33/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.4029 - binary_accuracy: 0.5095 - val_loss: 2.8821 - val_binary_accuracy: 0.5128\nEpoch 34/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.0808 - binary_accuracy: 0.5099 - val_loss: 3.8832 - val_binary_accuracy: 0.5161\nEpoch 35/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7501 - binary_accuracy: 0.5174 - val_loss: 3.9605 - val_binary_accuracy: 0.5135\nEpoch 36/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7382 - binary_accuracy: 0.5102 - val_loss: 3.6448 - val_binary_accuracy: 0.5115\nEpoch 37/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.1055 - binary_accuracy: 0.5245 - val_loss: 3.9398 - val_binary_accuracy: 0.5056\nEpoch 38/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.9880 - binary_accuracy: 0.5151 - val_loss: 2.8102 - val_binary_accuracy: 0.5082\nEpoch 39/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.7847 - binary_accuracy: 0.5215 - val_loss: 2.1289 - val_binary_accuracy: 0.5187\nEpoch 40/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.8415 - binary_accuracy: 0.5241 - val_loss: 2.8795 - val_binary_accuracy: 0.5115\nEpoch 41/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.5144 - binary_accuracy: 0.5223 - val_loss: 2.5149 - val_binary_accuracy: 0.5213\nEpoch 42/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0964 - binary_accuracy: 0.5287 - val_loss: 2.0305 - val_binary_accuracy: 0.5207\nEpoch 43/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.1527 - binary_accuracy: 0.5158 - val_loss: 1.8447 - val_binary_accuracy: 0.5095\nEpoch 44/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0041 - binary_accuracy: 0.5296 - val_loss: 1.7671 - val_binary_accuracy: 0.5095\nEpoch 45/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.7718 - binary_accuracy: 0.5232 - val_loss: 2.0167 - val_binary_accuracy: 0.5121\nEpoch 46/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.8157 - binary_accuracy: 0.5159 - val_loss: 1.7764 - val_binary_accuracy: 0.5227\nEpoch 47/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6907 - binary_accuracy: 0.5281 - val_loss: 1.3804 - val_binary_accuracy: 0.5233\nEpoch 48/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.5388 - binary_accuracy: 0.5233 - val_loss: 1.4022 - val_binary_accuracy: 0.5213\nEpoch 49/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6031 - binary_accuracy: 0.5259 - val_loss: 1.3513 - val_binary_accuracy: 0.5253\nEpoch 50/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.3865 - binary_accuracy: 0.5271 - val_loss: 1.1486 - val_binary_accuracy: 0.5246\nEpoch 51/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2101 - binary_accuracy: 0.5269 - val_loss: 1.0696 - val_binary_accuracy: 0.5227\nEpoch 52/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2028 - binary_accuracy: 0.5368 - val_loss: 1.1062 - val_binary_accuracy: 0.5292\nEpoch 53/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1968 - binary_accuracy: 0.5332 - val_loss: 0.9421 - val_binary_accuracy: 0.5318\nEpoch 54/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1506 - binary_accuracy: 0.5340 - val_loss: 1.0106 - val_binary_accuracy: 0.5338\nEpoch 55/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2173 - binary_accuracy: 0.5281 - val_loss: 0.9825 - val_binary_accuracy: 0.5378\nEpoch 56/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1079 - binary_accuracy: 0.5296 - val_loss: 0.8303 - val_binary_accuracy: 0.5515\nEpoch 57/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9980 - binary_accuracy: 0.5378 - val_loss: 0.8406 - val_binary_accuracy: 0.5456\nEpoch 58/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1121 - binary_accuracy: 0.5363 - val_loss: 1.8118 - val_binary_accuracy: 0.5548\nEpoch 59/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0127 - binary_accuracy: 0.5388 - val_loss: 0.9639 - val_binary_accuracy: 0.5483\nEpoch 60/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9924 - binary_accuracy: 0.5374 - val_loss: 0.8626 - val_binary_accuracy: 0.5483\nEpoch 61/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8680 - binary_accuracy: 0.5399 - val_loss: 0.8083 - val_binary_accuracy: 0.5542\nEpoch 62/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5440 - val_loss: 1.2377 - val_binary_accuracy: 0.5581\nEpoch 63/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9144 - binary_accuracy: 0.5396 - val_loss: 0.7873 - val_binary_accuracy: 0.5522\nEpoch 64/100\n191/191 [==============================] - 0s 3ms/step - loss: 0.9946 - binary_accuracy: 0.5343 - val_loss: 0.8474 - val_binary_accuracy: 0.5463\nEpoch 65/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9548 - binary_accuracy: 0.5365 - val_loss: 0.7671 - val_binary_accuracy: 0.5568\nEpoch 66/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9172 - binary_accuracy: 0.5432 - val_loss: 1.1670 - val_binary_accuracy: 0.5535\nEpoch 67/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9938 - binary_accuracy: 0.5473 - val_loss: 1.7050 - val_binary_accuracy: 0.5575\nEpoch 68/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8954 - binary_accuracy: 0.5394 - val_loss: 0.7633 - val_binary_accuracy: 0.5483\nEpoch 69/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8815 - binary_accuracy: 0.5460 - val_loss: 0.9965 - val_binary_accuracy: 0.5561\nEpoch 70/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5438 - val_loss: 1.0431 - val_binary_accuracy: 0.5509\nEpoch 71/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8825 - binary_accuracy: 0.5483 - val_loss: 1.0019 - val_binary_accuracy: 0.5601\nEpoch 72/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9897 - binary_accuracy: 0.5479 - val_loss: 1.3509 - val_binary_accuracy: 0.5502\nEpoch 73/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9775 - binary_accuracy: 0.5427 - val_loss: 0.9652 - val_binary_accuracy: 0.5581\nEpoch 74/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0205 - binary_accuracy: 0.5445 - val_loss: 1.2212 - val_binary_accuracy: 0.5614\nEpoch 75/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0616 - binary_accuracy: 0.5432 - val_loss: 1.1886 - val_binary_accuracy: 0.5575\nEpoch 76/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9314 - binary_accuracy: 0.5468 - val_loss: 1.3687 - val_binary_accuracy: 0.5548\nEpoch 77/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8521 - binary_accuracy: 0.5470 - val_loss: 0.7695 - val_binary_accuracy: 0.5548\nEpoch 78/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8151 - binary_accuracy: 0.5517 - val_loss: 0.9690 - val_binary_accuracy: 0.5509\nEpoch 79/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9553 - binary_accuracy: 0.5453 - val_loss: 0.8552 - val_binary_accuracy: 0.5575\nEpoch 80/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8296 - binary_accuracy: 0.5512 - val_loss: 0.7398 - val_binary_accuracy: 0.5555\nEpoch 81/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8471 - binary_accuracy: 0.5548 - val_loss: 0.7572 - val_binary_accuracy: 0.5529\nEpoch 82/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8430 - binary_accuracy: 0.5456 - val_loss: 0.7894 - val_binary_accuracy: 0.5575\nEpoch 83/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8508 - binary_accuracy: 0.5471 - val_loss: 0.9579 - val_binary_accuracy: 0.5476\nEpoch 84/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8445 - binary_accuracy: 0.5537 - val_loss: 0.8346 - val_binary_accuracy: 0.5581\nEpoch 85/100\n191/191 [==============================] - 1s 4ms/step - loss: 0.8542 - binary_accuracy: 0.5511 - val_loss: 0.7923 - val_binary_accuracy: 0.5542\nEpoch 86/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9084 - binary_accuracy: 0.5481 - val_loss: 0.9421 - val_binary_accuracy: 0.5575\nEpoch 87/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9367 - binary_accuracy: 0.5463 - val_loss: 0.9959 - val_binary_accuracy: 0.5450\nEpoch 88/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9629 - binary_accuracy: 0.5498 - val_loss: 1.1058 - val_binary_accuracy: 0.5607\nEpoch 89/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9141 - binary_accuracy: 0.5365 - val_loss: 0.7433 - val_binary_accuracy: 0.5561\nEpoch 90/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8145 - binary_accuracy: 0.5478 - val_loss: 0.7720 - val_binary_accuracy: 0.5529\nEpoch 91/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.7985 - binary_accuracy: 0.5560 - val_loss: 0.7784 - val_binary_accuracy: 0.5581\nEpoch 92/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8700 - binary_accuracy: 0.5468 - val_loss: 0.8171 - val_binary_accuracy: 0.5594\nEpoch 93/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8868 - binary_accuracy: 0.5502 - val_loss: 0.9612 - val_binary_accuracy: 0.5594\nEpoch 94/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8591 - binary_accuracy: 0.5493 - val_loss: 0.8276 - val_binary_accuracy: 0.5581\nEpoch 95/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8683 - binary_accuracy: 0.5435 - val_loss: 0.8003 - val_binary_accuracy: 0.5581\nEpoch 96/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8546 - binary_accuracy: 0.5415 - val_loss: 0.7477 - val_binary_accuracy: 0.5555\nEpoch 97/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9054 - binary_accuracy: 0.5484 - val_loss: 1.2473 - val_binary_accuracy: 0.5575\nEpoch 98/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8245 - binary_accuracy: 0.5479 - val_loss: 0.7795 - val_binary_accuracy: 0.5515\nEpoch 99/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8624 - binary_accuracy: 0.5435 - val_loss: 0.9183 - val_binary_accuracy: 0.5529\nEpoch 100/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8200 - binary_accuracy: 0.5470 - val_loss: 0.7893 - val_binary_accuracy: 0.5509\n\n\n\n\n\nNow we will predict the test to do the submission\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[-0.83098686],\n       [-0.71497   ],\n       [-0.4031287 ],\n       ...,\n       [-0.3488861 ],\n       [-0.4957207 ],\n       [-1.3229517 ]], dtype=float32)\n\n\n📝 Note that those are the logits!\nTo get a prediction we will compute the sigmoid function and round it to 1 or 0! (Thats because they are 2 classes, if there would be multi class classification then we would need Softmax Function)\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds)) # We use simple round function\ntest_df[\"target\"] = test_df[\"target\"].astype(int) # Submission needs int prediction\n\n\ntest_df.target.value_counts()\n\n0    3023\n1     240\nName: target, dtype: int64\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n0\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n9\n0\n\n\n4\n11\n0\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n0\n\n\n3260\n10868\n0\n\n\n3261\n10874\n0\n\n\n3262\n10875\n0\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\n#sub.to_csv(\"NN_submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "As we see in the previous notebooks. Embeddings solve the problem of have a good representation of text! But we still having some other problems:\nThe input of the Dense layers could vary in size! There are sequences of differents size! (Remember that we solve it with padding and truncation) Due to sequences could be large, there is a lot of computational costs! Layers are not sharing information! (They have different weighths) So we are not taking into account the order of the words, the context, or the words around To take care of this points, we will apply Recurrent Neural Networks in this notebook! 💥💥💥💥\nI recommend spend time in understand what is happen inside RNN. For this I see a lot of videos and read some blogs. I will let you some of them here:\n\nhttps://www.youtube.com/watch?v=Y2wfIKQyd1I\nhttps://www.tensorflow.org/text/tutorials/text_classification_rnn\nhttps://www.youtube.com/watch?v=AsNTP8Kwu80\n\n\n\n\nNLP-Embedding-RNN.png\n\n\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\n\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 02:13:04.769598: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 40 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 02:13:04.955961: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nAs Twitter Embedding is the best so far. We will continue use it!\nIn this case we will use the SimpleRNN Tensorflow Layer. Furthermore, we also going to use the Bidirectional Layer. This is basically to apply two RNN, one that process the sequence from left to right, and another that process the sequence from right to left. It makes sense because we will know all the sequence input at the moment we want to predict. Also, I prove with and without Bidirectional, and with Bidirectional improve a lot wit respect without it. I think that in timeseries is not a good idea because we don’t know the future at the moment we want predict.\nClick here to read about Bidirectional RNN\n👀 Also, note that now we will define explicitly the activation functions in the NN. Also we will apply the sigmoid function at the end. So know the loss functions should has: from_logits=False or let it by default. (I just do it because I want to prove both ways)\n\n# GloVe Twitter Embedding\nwv = KeyedVectors.load_word2vec_format('../input/twitter-word2vecs-wordvecs-from-godin/word2vec_twitter_tokens.bin', \n                                       binary=True,\n                                       unicode_errors='ignore')\n\n\n# Build embedding matrix \n\nvoc = vectorization_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\n# We have to construct the embedding matrix with weigths from our own vocabulary\n# shape embedding matrix : (vocab_size, embedding_dim)\nnum_tokens = len(voc)\nembedding_dim = 400 # we download glove 100 dimension\nhits = []\nmisses = []\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    if word in wv:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_vector = wv[word]\n        embedding_matrix[i] = embedding_vector\n        hits.append(word)\n    else:\n        misses.append(word)\nprint(\"Converted %d words (%d misses)\" % (len(hits), len(misses)))\n\nConverted 7873 words (2127 misses)\n\n\n\n# Model \nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False\n    ),\n    layers.SpatialDropout1D(0.3),\n    layers.Bidirectional(layers.SimpleRNN(64, dropout = 0.2, recurrent_dropout = 0.2)),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(16, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 40)                0         \n_________________________________________________________________\nembedding (Embedding)        (None, 40, 400)           4000000   \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 40, 400)           0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128)               59520     \n_________________________________________________________________\ndense (Dense)                (None, 32)                4128      \n_________________________________________________________________\ndropout (Dropout)            (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                528       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 4,064,193\nTrainable params: 64,193\nNon-trainable params: 4,000,000\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback]\n)\n\nEpoch 1/100\n191/191 [==============================] - 16s 70ms/step - loss: 0.6465 - binary_accuracy: 0.6245 - val_loss: 0.5218 - val_binary_accuracy: 0.7610\nEpoch 2/100\n191/191 [==============================] - 13s 67ms/step - loss: 0.5322 - binary_accuracy: 0.7484 - val_loss: 0.4914 - val_binary_accuracy: 0.7722\nEpoch 3/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.5156 - binary_accuracy: 0.7675 - val_loss: 0.4749 - val_binary_accuracy: 0.7846\nEpoch 4/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.5046 - binary_accuracy: 0.7768 - val_loss: 0.4774 - val_binary_accuracy: 0.7912\nEpoch 5/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4972 - binary_accuracy: 0.7811 - val_loss: 0.4751 - val_binary_accuracy: 0.7827\nEpoch 6/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4820 - binary_accuracy: 0.7806 - val_loss: 0.4750 - val_binary_accuracy: 0.7774\nEpoch 7/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4786 - binary_accuracy: 0.7836 - val_loss: 0.4745 - val_binary_accuracy: 0.7859\nEpoch 8/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4677 - binary_accuracy: 0.7870 - val_loss: 0.4880 - val_binary_accuracy: 0.7748\nEpoch 9/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4674 - binary_accuracy: 0.7885 - val_loss: 0.4692 - val_binary_accuracy: 0.7859\nEpoch 10/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4619 - binary_accuracy: 0.7941 - val_loss: 0.4784 - val_binary_accuracy: 0.7761\nEpoch 11/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4604 - binary_accuracy: 0.7957 - val_loss: 0.4947 - val_binary_accuracy: 0.7715\nEpoch 12/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4576 - binary_accuracy: 0.7995 - val_loss: 0.4750 - val_binary_accuracy: 0.7827\nEpoch 13/100\n191/191 [==============================] - 13s 68ms/step - loss: 0.4503 - binary_accuracy: 0.7972 - val_loss: 0.4668 - val_binary_accuracy: 0.7925\nEpoch 14/100\n191/191 [==============================] - 13s 71ms/step - loss: 0.4568 - binary_accuracy: 0.7979 - val_loss: 0.4693 - val_binary_accuracy: 0.7794\nEpoch 15/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4446 - binary_accuracy: 0.8015 - val_loss: 0.4820 - val_binary_accuracy: 0.7807\nEpoch 16/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4411 - binary_accuracy: 0.8048 - val_loss: 0.4815 - val_binary_accuracy: 0.7676\nEpoch 17/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4406 - binary_accuracy: 0.8061 - val_loss: 0.4686 - val_binary_accuracy: 0.7840\nEpoch 18/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4396 - binary_accuracy: 0.8003 - val_loss: 0.4630 - val_binary_accuracy: 0.7912\nEpoch 19/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4364 - binary_accuracy: 0.8080 - val_loss: 0.4975 - val_binary_accuracy: 0.7761\nEpoch 20/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4346 - binary_accuracy: 0.8025 - val_loss: 0.4834 - val_binary_accuracy: 0.7820\nEpoch 21/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4325 - binary_accuracy: 0.8026 - val_loss: 0.4844 - val_binary_accuracy: 0.7741\nEpoch 22/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4780 - binary_accuracy: 0.7767 - val_loss: 0.4947 - val_binary_accuracy: 0.7702\nEpoch 23/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4338 - binary_accuracy: 0.8046 - val_loss: 0.4666 - val_binary_accuracy: 0.7873\n\n\n\n\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"RNN_submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\n\n\n\nRNN has some benefits. But also some disadvantages (See the references). One of them, maybe the most named is the Exploding-Vanishing gradient problem!\n\nTo improve our solution, maybe we could try adding more RNN layers. Note that for this we need to set up return_sequences = True in the previous LSTM layers. Thats because by default the output of a RNN or LSTM layer is the last hidden state, but to feed to another LSTM we need a sequence (and this sequence is given thanks to return_sequences = True). And of course we could try hyperparameter optimization\n\nSo there are other arquitectures that take care of it. We will explore those now:\n\nLSTM\nGRU"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#libraries",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#libraries",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 02:13:04.769598: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 40 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 02:13:04.955961: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data-pipeline",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data-pipeline",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "Now we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#model",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#model",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "As Twitter Embedding is the best so far. We will continue use it!\nIn this case we will use the SimpleRNN Tensorflow Layer. Furthermore, we also going to use the Bidirectional Layer. This is basically to apply two RNN, one that process the sequence from left to right, and another that process the sequence from right to left. It makes sense because we will know all the sequence input at the moment we want to predict. Also, I prove with and without Bidirectional, and with Bidirectional improve a lot wit respect without it. I think that in timeseries is not a good idea because we don’t know the future at the moment we want predict.\nClick here to read about Bidirectional RNN\n👀 Also, note that now we will define explicitly the activation functions in the NN. Also we will apply the sigmoid function at the end. So know the loss functions should has: from_logits=False or let it by default. (I just do it because I want to prove both ways)\n\n# GloVe Twitter Embedding\nwv = KeyedVectors.load_word2vec_format('../input/twitter-word2vecs-wordvecs-from-godin/word2vec_twitter_tokens.bin', \n                                       binary=True,\n                                       unicode_errors='ignore')\n\n\n# Build embedding matrix \n\nvoc = vectorization_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\n# We have to construct the embedding matrix with weigths from our own vocabulary\n# shape embedding matrix : (vocab_size, embedding_dim)\nnum_tokens = len(voc)\nembedding_dim = 400 # we download glove 100 dimension\nhits = []\nmisses = []\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    if word in wv:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_vector = wv[word]\n        embedding_matrix[i] = embedding_vector\n        hits.append(word)\n    else:\n        misses.append(word)\nprint(\"Converted %d words (%d misses)\" % (len(hits), len(misses)))\n\nConverted 7873 words (2127 misses)\n\n\n\n# Model \nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False\n    ),\n    layers.SpatialDropout1D(0.3),\n    layers.Bidirectional(layers.SimpleRNN(64, dropout = 0.2, recurrent_dropout = 0.2)),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(16, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 40)                0         \n_________________________________________________________________\nembedding (Embedding)        (None, 40, 400)           4000000   \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 40, 400)           0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128)               59520     \n_________________________________________________________________\ndense (Dense)                (None, 32)                4128      \n_________________________________________________________________\ndropout (Dropout)            (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                528       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 4,064,193\nTrainable params: 64,193\nNon-trainable params: 4,000,000\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback]\n)\n\nEpoch 1/100\n191/191 [==============================] - 16s 70ms/step - loss: 0.6465 - binary_accuracy: 0.6245 - val_loss: 0.5218 - val_binary_accuracy: 0.7610\nEpoch 2/100\n191/191 [==============================] - 13s 67ms/step - loss: 0.5322 - binary_accuracy: 0.7484 - val_loss: 0.4914 - val_binary_accuracy: 0.7722\nEpoch 3/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.5156 - binary_accuracy: 0.7675 - val_loss: 0.4749 - val_binary_accuracy: 0.7846\nEpoch 4/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.5046 - binary_accuracy: 0.7768 - val_loss: 0.4774 - val_binary_accuracy: 0.7912\nEpoch 5/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4972 - binary_accuracy: 0.7811 - val_loss: 0.4751 - val_binary_accuracy: 0.7827\nEpoch 6/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4820 - binary_accuracy: 0.7806 - val_loss: 0.4750 - val_binary_accuracy: 0.7774\nEpoch 7/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4786 - binary_accuracy: 0.7836 - val_loss: 0.4745 - val_binary_accuracy: 0.7859\nEpoch 8/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4677 - binary_accuracy: 0.7870 - val_loss: 0.4880 - val_binary_accuracy: 0.7748\nEpoch 9/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4674 - binary_accuracy: 0.7885 - val_loss: 0.4692 - val_binary_accuracy: 0.7859\nEpoch 10/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4619 - binary_accuracy: 0.7941 - val_loss: 0.4784 - val_binary_accuracy: 0.7761\nEpoch 11/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4604 - binary_accuracy: 0.7957 - val_loss: 0.4947 - val_binary_accuracy: 0.7715\nEpoch 12/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4576 - binary_accuracy: 0.7995 - val_loss: 0.4750 - val_binary_accuracy: 0.7827\nEpoch 13/100\n191/191 [==============================] - 13s 68ms/step - loss: 0.4503 - binary_accuracy: 0.7972 - val_loss: 0.4668 - val_binary_accuracy: 0.7925\nEpoch 14/100\n191/191 [==============================] - 13s 71ms/step - loss: 0.4568 - binary_accuracy: 0.7979 - val_loss: 0.4693 - val_binary_accuracy: 0.7794\nEpoch 15/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4446 - binary_accuracy: 0.8015 - val_loss: 0.4820 - val_binary_accuracy: 0.7807\nEpoch 16/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4411 - binary_accuracy: 0.8048 - val_loss: 0.4815 - val_binary_accuracy: 0.7676\nEpoch 17/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4406 - binary_accuracy: 0.8061 - val_loss: 0.4686 - val_binary_accuracy: 0.7840\nEpoch 18/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4396 - binary_accuracy: 0.8003 - val_loss: 0.4630 - val_binary_accuracy: 0.7912\nEpoch 19/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4364 - binary_accuracy: 0.8080 - val_loss: 0.4975 - val_binary_accuracy: 0.7761\nEpoch 20/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4346 - binary_accuracy: 0.8025 - val_loss: 0.4834 - val_binary_accuracy: 0.7820\nEpoch 21/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4325 - binary_accuracy: 0.8026 - val_loss: 0.4844 - val_binary_accuracy: 0.7741\nEpoch 22/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4780 - binary_accuracy: 0.7767 - val_loss: 0.4947 - val_binary_accuracy: 0.7702\nEpoch 23/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4338 - binary_accuracy: 0.8046 - val_loss: 0.4666 - val_binary_accuracy: 0.7873"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"RNN_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "RNN has some benefits. But also some disadvantages (See the references). One of them, maybe the most named is the Exploding-Vanishing gradient problem!\n\nTo improve our solution, maybe we could try adding more RNN layers. Note that for this we need to set up return_sequences = True in the previous LSTM layers. Thats because by default the output of a RNN or LSTM layer is the last hidden state, but to feed to another LSTM we need a sequence (and this sequence is given thanks to return_sequences = True). And of course we could try hyperparameter optimization\n\nSo there are other arquitectures that take care of it. We will explore those now:\n\nLSTM\nGRU"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-1",
    "title": "3. Recurrent Neural Networks",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds.shape\n\n(3263, 1)\n\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"LSTM_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations-1",
    "title": "3. Recurrent Neural Networks",
    "section": "Observations:",
    "text": "Observations:\nNice, we find our better solution so far!\n\nTo improve our solution, maybe we could try adding more LSTM. Note that for this we need to set up return_sequences = True in the previous LSTM layers. Thats because by default the output of a RNN or LSTM layer is the last hidden state, but to feed to another LSTM we need a sequence (and this sequence is given thanks to return_sequences = True). And of course we could try hyperparameter optimization\n\nNow we will try GRU Recurrent Neural Network"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-2",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-2",
    "title": "3. Recurrent Neural Networks",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\n#sub.to_csv(\"GRU_submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nA little better than LSTM!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html",
    "title": "5. My best solution",
    "section": "",
    "text": "In this notebook I will use all the knowledge that we acquiring with the previous notebooks!\nIn this solution I will be using:\n\nTransformers\nHuggingFace\nPreprocessing\nTensorflow\nEarlyStopping\n\nand more..\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\n\n# A dependency of the preprocessing for BERT inputs\n!pip install -q -U \"tensorflow-text==2.8.*\"\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;3,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;2.10,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow&lt;3,&gt;=2.9.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow&lt;2.7.0,&gt;=2.6.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.28.0 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard&gt;=2.9.1, but you have tensorboard 2.8.0 which is incompatible.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\n\n\n\n# Try with a large model\nmodel_name = \"bert-large-uncased\"\n\n\nfrom transformers import AutoTokenizer\n\n# properly tokenization\ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\ntokenizer_max_length = 161\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"], truncation=True, padding=True, max_length=tokenizer_max_length)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI will try a preprocessing that I find here: (I lost the notebook! so sorry, please if someone find it let me know in the comments!)\n\n# Some preprocess \ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\n\n def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\nimport spacy\nimport re\nnlp = spacy.load('en_core_web_sm')\ndef preprocessing(text):\n    text = text.replace('#','')\n    text = decontracted(text)\n    text = re.sub('\\S*@\\S*\\s?','',text)\n    text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n    #token=[]\n    #result=''\n    #text = re.sub('[^A-z]', ' ',text.lower())\n\n    #text = nlp(text)\n    #for t in text:\n    #    if not t.is_stop and len(t)&gt;2:  \n    #        token.append(t.lemma_)\n    #result = ' '.join([i for i in token])\n\n    return text.strip()\n\n\ntrain.text = train.text.apply(lambda x : preprocessing(x)).astype(str)\ntest.text = test.text.apply(lambda x : preprocessing(x)).astype(str)\n\n\n# Save processed data to disk\nNEW_TRAIN_PATH = \"preprocessed_train.csv\"\nNEW_TEST_PATH = \"preprocessed_test.csv\"\n\ntrain.to_csv(NEW_TRAIN_PATH, index = False)\ntest.to_csv(NEW_TEST_PATH, index = False)\n\ndel train\ndel test\ngc.collect()\n\n856\n\n\n\n\n\n\n# Now We can use HF Datasets\n# We need to load our data, for this we use HF datasets\nfrom datasets import load_dataset\ndata_files = {\"train\": NEW_TRAIN_PATH,\n             \"test\": NEW_TEST_PATH}\ndataset = load_dataset(\"csv\", data_files = data_files, usecols = ['text', 'target'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n# Tokenization\ndataset = dataset.map(tokenize_dataset)\n\n\n\n\n\n\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\n# we transform the HF dataset into TF dataset\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = train_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True)\n\ntf_val_dataset = test_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False\n    )\n\n\n\n\nNow we will use restore_best_weights from EarlyStopping\n\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(6e-6),metrics = ['accuracy'])\n\n#checkpoint_filepath = 'checkpoint'\n#checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nearly_stop_callback = EarlyStopping(patience = 5, restore_best_weights = True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n#                              patience=2, min_lr=1e-9)\n\nmodel.fit(tf_train_dataset,\n         validation_data = tf_val_dataset,\n          epochs = 20,\n          callbacks = [early_stop_callback]\n         )\n\n\n\n\n2022-12-12 14:19:24.079862: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 125018112 exceeds 10% of free system memory.\n2022-12-12 14:19:24.239512: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.254974: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.272507: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.294475: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\nAll model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nEpoch 1/20\n380/380 [==============================] - 164s 344ms/step - loss: 0.4792 - accuracy: 0.7821 - val_loss: 0.4102 - val_accuracy: 0.8286\nEpoch 2/20\n380/380 [==============================] - 123s 325ms/step - loss: 0.3694 - accuracy: 0.8528 - val_loss: 0.3909 - val_accuracy: 0.8339\nEpoch 3/20\n380/380 [==============================] - 122s 321ms/step - loss: 0.3145 - accuracy: 0.8832 - val_loss: 0.4103 - val_accuracy: 0.8365\nEpoch 4/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2622 - accuracy: 0.9035 - val_loss: 0.4222 - val_accuracy: 0.8391\nEpoch 5/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2201 - accuracy: 0.9211 - val_loss: 0.4777 - val_accuracy: 0.8201\nEpoch 6/20\n380/380 [==============================] - 122s 320ms/step - loss: 0.1804 - accuracy: 0.9327 - val_loss: 0.4962 - val_accuracy: 0.8293\nEpoch 7/20\n380/380 [==============================] - 123s 323ms/step - loss: 0.1509 - accuracy: 0.9413 - val_loss: 0.5522 - val_accuracy: 0.8201\n\n\n&lt;keras.callbacks.History at 0x7fefa3775050&gt;\n\n\n\n\n\n\npred_dataset = load_dataset(\"csv\", data_files = \"/kaggle/input/nlp-getting-started/test.csv\", usecols = ['text'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\npred_dataset = pred_dataset.map(tokenize_dataset)\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_pred_dataset = pred_dataset['train'].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False)\n\n\npreds = model.predict(tf_pred_dataset)\n\n\npreds = np.argmax(preds['logits'], axis = 1)\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_df[\"target\"] = preds\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#libraries",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#libraries",
    "title": "5. My best solution",
    "section": "",
    "text": "# A dependency of the preprocessing for BERT inputs\n!pip install -q -U \"tensorflow-text==2.8.*\"\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;3,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;2.10,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow&lt;3,&gt;=2.9.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow&lt;2.7.0,&gt;=2.6.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.28.0 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard&gt;=2.9.1, but you have tensorboard 2.8.0 which is incompatible.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#huggingface-model",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#huggingface-model",
    "title": "5. My best solution",
    "section": "",
    "text": "# Try with a large model\nmodel_name = \"bert-large-uncased\"\n\n\nfrom transformers import AutoTokenizer\n\n# properly tokenization\ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\ntokenizer_max_length = 161\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"], truncation=True, padding=True, max_length=tokenizer_max_length)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#data-pre-processing",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#data-pre-processing",
    "title": "5. My best solution",
    "section": "",
    "text": "I will try a preprocessing that I find here: (I lost the notebook! so sorry, please if someone find it let me know in the comments!)\n\n# Some preprocess \ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\n\n def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\nimport spacy\nimport re\nnlp = spacy.load('en_core_web_sm')\ndef preprocessing(text):\n    text = text.replace('#','')\n    text = decontracted(text)\n    text = re.sub('\\S*@\\S*\\s?','',text)\n    text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n    #token=[]\n    #result=''\n    #text = re.sub('[^A-z]', ' ',text.lower())\n\n    #text = nlp(text)\n    #for t in text:\n    #    if not t.is_stop and len(t)&gt;2:  \n    #        token.append(t.lemma_)\n    #result = ' '.join([i for i in token])\n\n    return text.strip()\n\n\ntrain.text = train.text.apply(lambda x : preprocessing(x)).astype(str)\ntest.text = test.text.apply(lambda x : preprocessing(x)).astype(str)\n\n\n# Save processed data to disk\nNEW_TRAIN_PATH = \"preprocessed_train.csv\"\nNEW_TEST_PATH = \"preprocessed_test.csv\"\n\ntrain.to_csv(NEW_TRAIN_PATH, index = False)\ntest.to_csv(NEW_TEST_PATH, index = False)\n\ndel train\ndel test\ngc.collect()\n\n856"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#hf-dataset",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#hf-dataset",
    "title": "5. My best solution",
    "section": "",
    "text": "# Now We can use HF Datasets\n# We need to load our data, for this we use HF datasets\nfrom datasets import load_dataset\ndata_files = {\"train\": NEW_TRAIN_PATH,\n             \"test\": NEW_TEST_PATH}\ndataset = load_dataset(\"csv\", data_files = data_files, usecols = ['text', 'target'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n# Tokenization\ndataset = dataset.map(tokenize_dataset)\n\n\n\n\n\n\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\n# we transform the HF dataset into TF dataset\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = train_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True)\n\ntf_val_dataset = test_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False\n    )"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#model",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#model",
    "title": "5. My best solution",
    "section": "",
    "text": "Now we will use restore_best_weights from EarlyStopping\n\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(6e-6),metrics = ['accuracy'])\n\n#checkpoint_filepath = 'checkpoint'\n#checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nearly_stop_callback = EarlyStopping(patience = 5, restore_best_weights = True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n#                              patience=2, min_lr=1e-9)\n\nmodel.fit(tf_train_dataset,\n         validation_data = tf_val_dataset,\n          epochs = 20,\n          callbacks = [early_stop_callback]\n         )\n\n\n\n\n2022-12-12 14:19:24.079862: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 125018112 exceeds 10% of free system memory.\n2022-12-12 14:19:24.239512: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.254974: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.272507: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.294475: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\nAll model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nEpoch 1/20\n380/380 [==============================] - 164s 344ms/step - loss: 0.4792 - accuracy: 0.7821 - val_loss: 0.4102 - val_accuracy: 0.8286\nEpoch 2/20\n380/380 [==============================] - 123s 325ms/step - loss: 0.3694 - accuracy: 0.8528 - val_loss: 0.3909 - val_accuracy: 0.8339\nEpoch 3/20\n380/380 [==============================] - 122s 321ms/step - loss: 0.3145 - accuracy: 0.8832 - val_loss: 0.4103 - val_accuracy: 0.8365\nEpoch 4/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2622 - accuracy: 0.9035 - val_loss: 0.4222 - val_accuracy: 0.8391\nEpoch 5/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2201 - accuracy: 0.9211 - val_loss: 0.4777 - val_accuracy: 0.8201\nEpoch 6/20\n380/380 [==============================] - 122s 320ms/step - loss: 0.1804 - accuracy: 0.9327 - val_loss: 0.4962 - val_accuracy: 0.8293\nEpoch 7/20\n380/380 [==============================] - 123s 323ms/step - loss: 0.1509 - accuracy: 0.9413 - val_loss: 0.5522 - val_accuracy: 0.8201\n\n\n&lt;keras.callbacks.History at 0x7fefa3775050&gt;"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#predict-test",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#predict-test",
    "title": "5. My best solution",
    "section": "",
    "text": "pred_dataset = load_dataset(\"csv\", data_files = \"/kaggle/input/nlp-getting-started/test.csv\", usecols = ['text'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\npred_dataset = pred_dataset.map(tokenize_dataset)\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_pred_dataset = pred_dataset['train'].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False)\n\n\npreds = model.predict(tf_pred_dataset)\n\n\npreds = np.argmax(preds['logits'], axis = 1)\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_df[\"target\"] = preds\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html",
    "title": "4. BERT",
    "section": "",
    "text": "As we see in the previous notebook, RNN improve the results and give us some benefits. But they are expensive to train, mainly because they are not paralellizable! Now we are in the transformers era. Transformers are models that leverage the mechanism of self-attention.\nIn this specific case we will apply BERT. I will let you with some references that help me to understand what is happend behind scenes:\n\nhttp://nlp.seas.harvard.edu/annotated-transformer/\nhttp://jalammar.github.io/illustrated-transformer/\nhttps://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\nhttps://www.youtube.com/watch?v=xI0HHN5XKDo&t=2s&pp=ugMICgJlcxABGAE%3D\nhttps://www.youtube.com/watch?v=7kLi8u2dJz0\nhttps://www.youtube.com/watch?v=TQQlZhbC5ps&list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE&index=1\n\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#hf-pipelines---zero-shot-classification",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#hf-pipelines---zero-shot-classification",
    "title": "4. BERT",
    "section": "HF 🤗 Pipelines - Zero Shot Classification",
    "text": "HF 🤗 Pipelines - Zero Shot Classification\nAs you can see in the hf documentation. HuggingFace has pipelines, where we can develop a variety of nlp (and more) applications! (Text generation, classification, summarization, etc)\nNow we will do Zero shot classification. That means that we will predict classes that wasn’t observe during traning. It is a difficult task\n\nclassifier = pipeline(\"zero-shot-classification\",\n                      model=\"facebook/bart-large-mnli\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# predict\ncandidate_labels = ['disaster']\ntest_df['target'] = [x['scores'][0] for x in classifier(test_df.text.to_list(), candidate_labels)]\n\n\ntest_df[\"target\"] = tf.round(test_df.target)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"HF_ZS_Submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nMake sense because is a zero shot classification!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#fine-tune",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#fine-tune",
    "title": "4. BERT",
    "section": "Fine-Tune",
    "text": "Fine-Tune\nNow we will download a model from HuggingFace models and finetune with our data\n\n# Model name to fine tune\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n\nfrom transformers import AutoTokenizer\n\n# First we need to use the tokenizer (Is similar we did in the Tensorflow hub with the preprocessing layer)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"])\n\n\n\n\n\n\n\n\n\n\n\n# We need to load our data, for this we use HF datasets (read directly from the disk, so save us ram)\nfrom datasets import load_dataset\ndata_files = {\"train\": \"/kaggle/input/df-split/df_split/df_train.csv\",\n             \"test\":\"/kaggle/input/df-split/df_split/df_test.csv\"}\ndataset = load_dataset(\"csv\", data_files = data_files, usecols = ['text', 'target'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1860883215d5579d/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1860883215d5579d/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'target'],\n        num_rows: 6090\n    })\n    test: Dataset({\n        features: ['text', 'target'],\n        num_rows: 1523\n    })\n})\n\n\n\n# preprocess\ndataset = dataset.map(tokenize_dataset)\n\n\n\n\n\n\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\n# we transform the HF dataset into TF dataset to fine tune\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = train_dataset.to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True)\n\ntf_val_dataset = test_dataset.to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True\n    )"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#model-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#model-1",
    "title": "4. BERT",
    "section": "Model",
    "text": "Model\n\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(3e-5),metrics = ['accuracy'])\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nmodel.fit(tf_train_dataset,\n         validation_data = tf_val_dataset,\n          epochs = 20,\n          callbacks = [early_stop_callback]\n         )\n\n\n\n\n2022-12-12 03:54:13.603300: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\nSome layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_20']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nEpoch 1/20\n380/380 [==============================] - 44s 87ms/step - loss: 0.4815 - accuracy: 0.7913 - val_loss: 0.3999 - val_accuracy: 0.8270\nEpoch 2/20\n380/380 [==============================] - 31s 83ms/step - loss: 0.3179 - accuracy: 0.8750 - val_loss: 0.4339 - val_accuracy: 0.8164\nEpoch 3/20\n380/380 [==============================] - 31s 83ms/step - loss: 0.2035 - accuracy: 0.9238 - val_loss: 0.4884 - val_accuracy: 0.8053\nEpoch 4/20\n380/380 [==============================] - 31s 83ms/step - loss: 0.1147 - accuracy: 0.9572 - val_loss: 0.5929 - val_accuracy: 0.7967\nEpoch 5/20\n380/380 [==============================] - 32s 83ms/step - loss: 0.0829 - accuracy: 0.9729 - val_loss: 0.7295 - val_accuracy: 0.7717\nEpoch 6/20\n380/380 [==============================] - 31s 82ms/step - loss: 0.0579 - accuracy: 0.9760 - val_loss: 0.9569 - val_accuracy: 0.7855\n\n\n&lt;keras.callbacks.History at 0x7f06a863c950&gt;\n\n\n\nPredict Test\n\npred_dataset = load_dataset(\"csv\", data_files = \"/kaggle/input/nlp-getting-started/test.csv\", usecols = ['text'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n# preprocess\npred_dataset = pred_dataset.map(tokenize_dataset)\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_pred_dataset = pred_dataset['train'].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False)\n\n\npreds = model.predict(tf_pred_dataset)\n\n\npreds = np.argmax(preds['logits'], axis = 1)\n\n\ntest_df[\"target\"] = preds\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"DistilBERT_FT_Submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#observations-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#observations-1",
    "title": "4. BERT",
    "section": "Observations",
    "text": "Observations\nNot bad! Note that we don’t tune much this model, so probably we could improve it easily!\nThe important point is how easy is use hub models from huggingface and finetune. If you look the website, you can see that there are a lot of datasets and models made by de comunnity!\nIn the next notebook I will do my best combining all the stuff that we saw in this notebooks NLP series to get a better result. Also I will pre-process the data before feed it into the model"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html",
    "title": "2. Embeddings",
    "section": "",
    "text": "In the last notebook we use only the TextVectorization layer to represent text. I think it is not efficient because each position on each sentence will have a different number(index) depending on the token associated to it but the same weight! So we are sharing weights between words without capture some context.\nIt could be better if we encode each word as a vector(wich have magnitude and direction). So we could represent similar words with the magnitud and direction of the vectors! One way to do this is for example computing the (cosine similarity)[https://en.wikipedia.org/wiki/Cosine_similarity]\nWe will do that with Embeddings! This is a vector that represent in this case a token! (It could be represent a letter, sub-word, sentence, etc.)\nWe can use the (Tensorflow Embedding layer)[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding] for this. This will initialize the representations by random (see the parameter embedding_initializer). So the idea is fit these embeddings to the data and so learn to represent text.\nWe can also use pre-trained Embeddings like (Glove)[https://nlp.stanford.edu/projects/glove/] or (Word2Vec)[https://jalammar.github.io/illustrated-word2vec/]. This could be a better idea because we can fine-tune those.\nIn this notebook we will do both (Embedding And Pre-trained Embeddings).\n\n\n\nNLP-Embedding-NN.png\n\n\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\n\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 01:04:59.963536: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\n👁 Note that Embedding Layer transform indexes to vectors! So we still need TextVectorization Layer!\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 250 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 01:05:00.242787: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\nDue to we will use embedding to represent text, we can use a bigger output_sequence_length because we hope reduce the dimension of that with the embedding. For this now output_sequence_length=250\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nWe going to use an Embedding where the input will be a tensor with the lenght of the sequences. And we wants to use a representation embedding of dimension 64. Frequently this number is less than the sequence (because we expected less dimension without loss of information).\nNote that after the Embedding Layer there is a GlobalAveragePooling Layer. Thats because there is one vector embedding to each token. So we are adding one dimension (See the Arquitecture). To reduce this dimension I saw two techniques (maybe there are more): 1. Take the average in the sequence dimension (250 in this case) 2. Concat all the embeddings and then Flatten()\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(max_features, 64),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\nThe input dim of Embedding layer should be the vocabulary size, because the Embedding at the end is a big matrix of each word represented by a vector (max_features, output)\n\n\n\nimage.png\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding (Embedding)        (None, 250, 64)           640000    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 64)                0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                1040      \n_________________________________________________________________\ndropout (Dropout)            (None, 16)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 641,057\nTrainable params: 641,057\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n🔍 Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\nIn this case, I will use EarlyStopping Callback to avoid overfitting.\nClick Here to learn more about Early Stopping\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 3s 10ms/step - loss: 0.6815 - binary_accuracy: 0.5732 - val_loss: 0.6823 - val_binary_accuracy: 0.5588\nEpoch 2/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6720 - binary_accuracy: 0.5732 - val_loss: 0.6682 - val_binary_accuracy: 0.5588\nEpoch 3/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6303 - binary_accuracy: 0.5772 - val_loss: 0.5971 - val_binary_accuracy: 0.5929\nEpoch 4/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.5252 - binary_accuracy: 0.6987 - val_loss: 0.5087 - val_binary_accuracy: 0.7682\nEpoch 5/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.4301 - binary_accuracy: 0.7957 - val_loss: 0.4672 - val_binary_accuracy: 0.7840\nEpoch 6/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.3725 - binary_accuracy: 0.8350 - val_loss: 0.4521 - val_binary_accuracy: 0.7892\nEpoch 7/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.3281 - binary_accuracy: 0.8586 - val_loss: 0.4481 - val_binary_accuracy: 0.7905\nEpoch 8/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2898 - binary_accuracy: 0.8785 - val_loss: 0.4517 - val_binary_accuracy: 0.7997\nEpoch 9/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2584 - binary_accuracy: 0.8934 - val_loss: 0.4595 - val_binary_accuracy: 0.7978\nEpoch 10/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2286 - binary_accuracy: 0.9071 - val_loss: 0.4727 - val_binary_accuracy: 0.7978\nEpoch 11/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2058 - binary_accuracy: 0.9176 - val_loss: 0.4892 - val_binary_accuracy: 0.7971\nEpoch 12/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.1839 - binary_accuracy: 0.9261 - val_loss: 0.5107 - val_binary_accuracy: 0.7965\n\n\n🙉 It looks like the NN learn a lot more! Val Accuracy of 0.8\n\n\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[1.626427 ],\n       [0.6582693],\n       [5.033337 ],\n       ...,\n       [2.6492274],\n       [2.4858732],\n       [1.0287898]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.8356796 ],\n       [0.6588715 ],\n       [0.9935252 ],\n       ...,\n       [0.9339634 ],\n       [0.92314553],\n       [0.73668116]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n1\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‰ÛÒ SAFETY FASTE...\n1\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows × 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"Embedding_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data",
    "title": "2. Embeddings",
    "section": "",
    "text": "# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 01:04:59.963536: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\n👁 Note that Embedding Layer transform indexes to vectors! So we still need TextVectorization Layer!\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 250 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 01:05:00.242787: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\nDue to we will use embedding to represent text, we can use a bigger output_sequence_length because we hope reduce the dimension of that with the embedding. For this now output_sequence_length=250"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data-pipeline",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data-pipeline",
    "title": "2. Embeddings",
    "section": "",
    "text": "Now we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model",
    "title": "2. Embeddings",
    "section": "",
    "text": "We going to use an Embedding where the input will be a tensor with the lenght of the sequences. And we wants to use a representation embedding of dimension 64. Frequently this number is less than the sequence (because we expected less dimension without loss of information).\nNote that after the Embedding Layer there is a GlobalAveragePooling Layer. Thats because there is one vector embedding to each token. So we are adding one dimension (See the Arquitecture). To reduce this dimension I saw two techniques (maybe there are more): 1. Take the average in the sequence dimension (250 in this case) 2. Concat all the embeddings and then Flatten()\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(max_features, 64),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\nThe input dim of Embedding layer should be the vocabulary size, because the Embedding at the end is a big matrix of each word represented by a vector (max_features, output)\n\n\n\nimage.png\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding (Embedding)        (None, 250, 64)           640000    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 64)                0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                1040      \n_________________________________________________________________\ndropout (Dropout)            (None, 16)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 641,057\nTrainable params: 641,057\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n🔍 Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\nIn this case, I will use EarlyStopping Callback to avoid overfitting.\nClick Here to learn more about Early Stopping\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 3s 10ms/step - loss: 0.6815 - binary_accuracy: 0.5732 - val_loss: 0.6823 - val_binary_accuracy: 0.5588\nEpoch 2/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6720 - binary_accuracy: 0.5732 - val_loss: 0.6682 - val_binary_accuracy: 0.5588\nEpoch 3/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6303 - binary_accuracy: 0.5772 - val_loss: 0.5971 - val_binary_accuracy: 0.5929\nEpoch 4/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.5252 - binary_accuracy: 0.6987 - val_loss: 0.5087 - val_binary_accuracy: 0.7682\nEpoch 5/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.4301 - binary_accuracy: 0.7957 - val_loss: 0.4672 - val_binary_accuracy: 0.7840\nEpoch 6/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.3725 - binary_accuracy: 0.8350 - val_loss: 0.4521 - val_binary_accuracy: 0.7892\nEpoch 7/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.3281 - binary_accuracy: 0.8586 - val_loss: 0.4481 - val_binary_accuracy: 0.7905\nEpoch 8/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2898 - binary_accuracy: 0.8785 - val_loss: 0.4517 - val_binary_accuracy: 0.7997\nEpoch 9/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2584 - binary_accuracy: 0.8934 - val_loss: 0.4595 - val_binary_accuracy: 0.7978\nEpoch 10/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2286 - binary_accuracy: 0.9071 - val_loss: 0.4727 - val_binary_accuracy: 0.7978\nEpoch 11/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2058 - binary_accuracy: 0.9176 - val_loss: 0.4892 - val_binary_accuracy: 0.7971\nEpoch 12/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.1839 - binary_accuracy: 0.9261 - val_loss: 0.5107 - val_binary_accuracy: 0.7965\n\n\n🙉 It looks like the NN learn a lot more! Val Accuracy of 0.8"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test",
    "title": "2. Embeddings",
    "section": "",
    "text": "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[1.626427 ],\n       [0.6582693],\n       [5.033337 ],\n       ...,\n       [2.6492274],\n       [2.4858732],\n       [1.0287898]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.8356796 ],\n       [0.6588715 ],\n       [0.9935252 ],\n       ...,\n       [0.9339634 ],\n       [0.92314553],\n       [0.73668116]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n1\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‰ÛÒ SAFETY FASTE...\n1\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows × 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"Embedding_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#glove",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#glove",
    "title": "2. Embeddings",
    "section": "GloVe",
    "text": "GloVe\nIt is important to know how (and on what data) this embedding was trained. Here are some references:\n\nhttps://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010\nhttps://nlp.stanford.edu/projects/glove/\n\n\n# First we download the embedding or matrix !\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip\n\n--2022-12-12 01:05:29--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2022-12-12 01:05:29--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2022-12-12 01:05:30--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‘glove.6B.zip’\n\nglove.6B.zip        100%[===================&gt;] 822.24M  5.01MB/s    in 2m 39s  \n\n2022-12-12 01:08:10 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n\n\n\n👀👀👀👀👀👀👀👀👀\nWe will use the same Tensorflow Embedding layer. For this we need to construct an embedding matrix and initialize the layer with that matrix.\nTo do that we will:\n\nCreate the embedding index. i.e From the txt embedding file we will get the representation vector for each word! (token)\nWe will assign the pre-trained representation vector to each word(token) of our vocabulary. If the word is not in the pre-trained embedding, we will let a zeros vector\nIn the model we initialize the layer with our pre-trained embedding matrix\n\nSo we will get an embedding matrix of shape (voc_size, embedding_size). Note that each index corresponds to a specific word of our vocabulary. So we need to map index -&gt; embedding.\n\nimport os\n\n# Create a pre-trained embedding index\n\npath_to_glove_file = './glove.6B.50d.txt' # We can choose 300d,200d,100d,50d\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\nFound 400000 word vectors.\n\n\n\n# Word index of OUR vocabulary\nvoc = vectorization_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\n\n#word_index\n\n\n# We have to construct the embedding matrix with weigths from our own vocabulary\n\n# shape embedding matrix : (vocab_size, embedding_dim)\nnum_tokens = len(voc)\nembedding_dim = 50 # we download glove 100 dimension\nhits = []\nmisses = []\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits.append(word)\n    else:\n        misses.append(word)\nprint(\"Converted %d words (%d misses)\" % (len(hits), len(misses)))\n\nConverted 7692 words (2308 misses)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-1",
    "title": "2. Embeddings",
    "section": "Model",
    "text": "Model\nThe same architecture as above but now with the GloVe pre-trained embedding\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=True), # To Fine tune\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 250, 50)           500000    \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 50)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                816       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 500,833\nTrainable params: 500,833\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 3s 9ms/step - loss: 0.6637 - binary_accuracy: 0.5732 - val_loss: 0.6411 - val_binary_accuracy: 0.5588\nEpoch 2/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.5936 - binary_accuracy: 0.6099 - val_loss: 0.5518 - val_binary_accuracy: 0.6717\nEpoch 3/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.5024 - binary_accuracy: 0.7402 - val_loss: 0.4895 - val_binary_accuracy: 0.7879\nEpoch 4/100\n191/191 [==============================] - 1s 7ms/step - loss: 0.4393 - binary_accuracy: 0.7924 - val_loss: 0.4594 - val_binary_accuracy: 0.7984\nEpoch 5/100\n191/191 [==============================] - 1s 7ms/step - loss: 0.3949 - binary_accuracy: 0.8202 - val_loss: 0.4412 - val_binary_accuracy: 0.8050\nEpoch 6/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.3598 - binary_accuracy: 0.8415 - val_loss: 0.4314 - val_binary_accuracy: 0.8089\nEpoch 7/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.3287 - binary_accuracy: 0.8544 - val_loss: 0.4301 - val_binary_accuracy: 0.8116\nEpoch 8/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2976 - binary_accuracy: 0.8685 - val_loss: 0.4311 - val_binary_accuracy: 0.8063\nEpoch 9/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2732 - binary_accuracy: 0.8814 - val_loss: 0.4363 - val_binary_accuracy: 0.8070\nEpoch 10/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2517 - binary_accuracy: 0.8934 - val_loss: 0.4445 - val_binary_accuracy: 0.8076\nEpoch 11/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2306 - binary_accuracy: 0.9051 - val_loss: 0.4565 - val_binary_accuracy: 0.8070\nEpoch 12/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2083 - binary_accuracy: 0.9133 - val_loss: 0.4704 - val_binary_accuracy: 0.8050"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-1",
    "title": "2. Embeddings",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[1.5487207],\n       [1.0597007],\n       [3.9148426],\n       ...,\n       [1.958971 ],\n       [2.308964 ],\n       [1.0568069]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.8247289 ],\n       [0.74263334],\n       [0.98044634],\n       ...,\n       [0.8764216 ],\n       [0.9096167 ],\n       [0.74207985]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n1\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‰ÛÒ SAFETY FASTE...\n1\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows × 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\nsub.to_csv(\"GloVe_Embedding_submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\n🥰🥰🥰🥰🥰🥰\nYes! We improve the score! Although it is not so much better. I think that means that is not to difficult learn the embedding from scratch in this use case! Maybe due we have enough data? or because we select a not to huge embedding dimension? What do you think?\n\n🧠 Task: Try with an 300,200 or 100 d Glove Embedding!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-2",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-2",
    "title": "2. Embeddings",
    "section": "Model",
    "text": "Model\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=True\n    ),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 250, 400)          4000000   \n_________________________________________________________________\nglobal_average_pooling1d_2 ( (None, 400)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 16)                6416      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 4,006,433\nTrainable params: 4,006,433\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 9s 39ms/step - loss: 0.6561 - binary_accuracy: 0.5745 - val_loss: 0.6037 - val_binary_accuracy: 0.6179\nEpoch 2/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.5149 - binary_accuracy: 0.7233 - val_loss: 0.4822 - val_binary_accuracy: 0.7439\nEpoch 3/100\n191/191 [==============================] - 7s 39ms/step - loss: 0.3944 - binary_accuracy: 0.8220 - val_loss: 0.4769 - val_binary_accuracy: 0.7663\nEpoch 4/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.3212 - binary_accuracy: 0.8614 - val_loss: 0.4878 - val_binary_accuracy: 0.7827\nEpoch 5/100\n191/191 [==============================] - 7s 38ms/step - loss: 0.2681 - binary_accuracy: 0.8854 - val_loss: 0.5148 - val_binary_accuracy: 0.7846\nEpoch 6/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.2223 - binary_accuracy: 0.9080 - val_loss: 0.5104 - val_binary_accuracy: 0.7965\nEpoch 7/100\n191/191 [==============================] - 7s 39ms/step - loss: 0.1857 - binary_accuracy: 0.9268 - val_loss: 0.5446 - val_binary_accuracy: 0.7938\nEpoch 8/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.1563 - binary_accuracy: 0.9376 - val_loss: 0.5953 - val_binary_accuracy: 0.7859"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-2",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-2",
    "title": "2. Embeddings",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[-0.06506764],\n       [ 0.26650202],\n       [ 5.848435  ],\n       ...,\n       [ 1.5803046 ],\n       [ 1.5205853 ],\n       [ 0.33927795]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.48373884],\n       [0.56623393],\n       [0.9971239 ],\n       ...,\n       [0.8292477 ],\n       [0.82062465],\n       [0.58401513]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n0\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‰ÛÒ SAFETY FASTE...\n0\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows × 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n0\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows × 2 columns\n\n\n\n\n#sub.to_csv(\"TTW2V_Embedding_Submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nNice! As we excpected it, We improve the score a little bit!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Diegulio’s Blog 🦊",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIdentificando desastres en Twitter con NLP\n\n\n\n\n\n\n\nNLP\n\n\ncode\n\n\nKaggle\n\n\n\n\n\n\n\n\n\n\n\nDiegulio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! Soy Diego Machado, Ingeniero Civil Industrial con magíster en Ciencias de la Ingeniería. Soy un entusiasta en el mundo de la inteligencia artificial, me encuentro constantenemente aprendiendo y desarrollando nuevos proyectos. Tengo conocimientos en una gran variedad de aplicaciones de la IA, tales como Regresión, Clasificación, Time Series, NLP, CV y Audio."
  }
]