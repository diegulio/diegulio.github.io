[
  {
    "objectID": "posts/llm-recipe/main.html",
    "href": "posts/llm-recipe/main.html",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "",
    "text": "Note\n\n\n\nAntes de comenzar a leer esto, recuerda que yo estoy aprendiendo junto contigo. Si tienes alguna duda, sugerencia, correci√≥n o comentario, no dudes en escribirme a mi LinkedIn."
  },
  {
    "objectID": "posts/llm-recipe/main.html#prompt",
    "href": "posts/llm-recipe/main.html#prompt",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "Prompt:",
    "text": "Prompt:\nImaginemos queremos saber los ingredientes para cocinar una lasagna, entonces escribiremos algo del estilo:\n\n\n\n\n\n\nüë©üèº‚Äçüî¨\n\n\n\n¬øQue necesito para cocinar una Lasagna?\n\n\nEsta pregunta, que es la entrada de un LLM se le denomina prompt. Un t√©rmino bastante conocido hasta ahora, que de hecho se asocia a un rol, es Prompt Engineering. Podemos entender este t√©rmino como el ‚Äúarte‚Äù de escribir el mejor prompt para obtener la respuesta deseada.\n‚ÄúCuida la forma en la que pides las cosas‚Äù me dec√≠a mi mam√° cuando ni√±o al pedirle un favor a alguien. Las mam√°s siempre tienen la raz√≥n, y esta no es la excepci√≥n.\nImaginemos que la respuesta de la LLM es algo como:\n\n\n\n\n\n\nü§ñ\n\n\n\nPara cocinar una lasagna necesitas un horno, un cuchillo, una cocina y los ingredientes.\n\n\nNo es la respuesta que esperabamos! nosotros en realidad quer√≠amos saber los ingredientes, pero nos expresamos mal. Es por esto que com√∫nmente se suele iterar el prompt hasta conseguir la respuesta deseada, imaginemos que luego de iterar un poco llegamos al prompt final:\n\n\n\n\n\n\nüë©üèº‚Äçüî¨\n\n\n\n¬øCuales son los ingredientes que necesito para cocinar una lasagna?\n\n\nProbablemente con un prompt as√≠ obtengamos lo que buscamos. No hay un prompt √≥ptimo, pero si existen muchos prompt que nos conseguir√°n la respuesta que buscamos.\nM√°s adelante veremos algunas t√©cnicas de prompt engineering. Por ahora nos basta con saber que el prompt ser√° elemento importante de nuestra soluci√≥n."
  },
  {
    "objectID": "posts/llm-recipe/main.html#output",
    "href": "posts/llm-recipe/main.html#output",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "Output:",
    "text": "Output:\nAlgo que nos debemos cuestionar es: ¬øComo necesitamos el resultado? algunas de las opciones son: Una lista de ingredientes, un json, un DataFrame, etc.\nEn este caso lo que decid√≠ fue obtener un json, el cual luego convertir√≠a a un DataFrame. Los elementos que tendr√° la respuesta son:\n\nIngredient: Nombre del ingrediente\nQuantity: Cantidad necesaria\nOptional: ‚ÄúYes‚Äù si el ingrediente es opcional, ‚ÄúNo‚Äù si es obligatorio\nEstimated Price: Un precio estimado en d√≥lares del ingrediente (as√≠ podremos calcular alg√∫n valor aproximado de la receta)\nAvailable: Una simulaci√≥n de disponibilidad del ingrediente en el supermercado. ‚ÄúYes‚Äù si est√° disponible, ‚ÄúNo‚Äù si no lo est√°.\n\nAc√° tenemos un ejemplo:\n{\n  \"Spaguetthi With Meat\":\n  [\n    {\n      \"ingredient\": \"Spaguetti\",\n      \"optional\": \"No\",\n      \"quantity\": \"200g\",\n      \"estimated_price\": \"5.00\",\n      \"available\": \"No\"\n    },\n    {\n      \"ingredient\": \"Meat\",\n      \"optional\": \"No\",\n      \"quantity\": \"1kg\",\n      \"estimated_price\": \"10.00\",\n      \"available\": \"Yes\"\n    },\n    {\n      \"ingredient\": \"Pepper\",\n      \"optional\": \"Yes\",\n      \"quantity\": \"at ease\",\n      \"estimated_price\": \"1.00\",\n      \"available\": \"No\"\n    }\n\n  ]\n}\nUna pregunta importante es, ¬øC√≥mo lograremos que el LLM nos estructure el output como lo requerimos?\nSPOILER: Prompt Engineering"
  },
  {
    "objectID": "posts/llm-recipe/main.html#obtener-receta-mediante-llm",
    "href": "posts/llm-recipe/main.html#obtener-receta-mediante-llm",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "1. Obtener receta mediante LLM",
    "text": "1. Obtener receta mediante LLM\n\nPrompt Templates\nEn este punto vale la pena preguntarnos, c√≥mo esperamos que el usuario interact√∫e con nuestra aplicaci√≥n? Queremos que el usuario haga la pregunta completa? Ahora sabemos que esto no es una buena idea por varias razones:\n\nEl usuario podria ingresar incluso preguntas que no est√©n relacionadas con la aplicaci√≥n (comida)\nEl usuario puede preguntar de forma ineficiente\nObtener la estructura json que deseamos ser√≠a imposible\nEl usuario no sabe de Prompt Engineering\n\nLa idea es que el usuario s√≥lo ingrese el nombre de la comida, y por detr√°s nuestra aplicaci√≥n haga el resto. Para esto, Langchain cuenta con una herramienta llamada Prompt Templates, que como su nombre lo indica es una plantilla del prompt.\nEsto es, imaginemos nuestra plantilla es: ‚Äú¬øCuales son los ingredientes que necesito para cocinar {COMIDA}?‚Äù Entonces si la entrada del usuario es ‚ÄúLasagna‚Äù, el prompt quedar√° ‚Äú¬øCuales son los ingredientes que necesito para cocinar Lasagna?‚Äù.\n\n\nüë®üèæ‚Äçüíª¬†Code time!\nPara crear un template, primero debemos definir la estructura:\ntemplate_string = \"\"\"\nGive me a list of ingredients to cook {food}.\n\"\"\"\nNotemos que el input en este caso es ‚Äúfood‚Äù. Luego usamos Langchain\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = PromptTemplate.from_template(template_string)\nLuego simplemente obtenemos el prompt final para entregarle al modelo de la siguiente forma:\nuser_input = 'Lasagna'\nfinal_prompt = prompt.format(food=user_input)\nAs√≠ de simple! Ahora quiero que nos compliquemos un poco m√°s la vida. ChatGPT hizo unos cambios en su API, por lo que en Langchain ahora aparece un nuevo elemento llamado ChatPromptTemplate\nLa idea de este template, que si bien tambi√©n acepta entradas como las vistas anteriormente, ahora permite ingresar mensajes con roles. Existen tres tipos de roles: System, Human, AI. Bien brevemente te explico que deberian ser:\n\nSystem Message: Las instrucciones que se le quiere entregar al modelo\nHuman Message: Las entradas del usuario\nAI Message: Alguna respuesta por parte de el modelo\n\nAc√° te dejo un post del por qu√© de esta implementaci√≥n, que viene de la mano con ChatModels: https://blog.langchain.dev/chat-models/\nDebido a que s√≥lo el paso 1 utiliza input de usuario, s√≥lo lo haremos as√≠ en este paso:\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\n#¬†System template\nfirst_system_template_str = \"\"\"\n    You are a good chef, users need you to bring them recipes from given food.\n\"\"\"\nfirst_system_template = SystemMessagePromptTemplate.from_template(first_system_template_str)\n\n# Human Template\nfirst_human_template_str = \"{food}\"\nfirst_human_template = HumanMessagePromptTemplate.from_template(first_human_template_str)\n\n# First Prompt Template\nfirst_prompt = ChatPromptTemplate.from_messages([first_system_template, first_human_template])\n\n\nLLM Model\nAhora que ya tenemos una entrada para nuestro modelo, necesitamos llamarlo!\nPara poder hacer uso de un modelo de LLM (En este caso ChatOpenAI) primero debemos crearnos una cuenta en openai y crear una nueva API Key. Luego, debemos crear un archivo .env en la ra√≠z del proyecto con el siguiente contenido:\nOPENAI_API_KEY = \"&lt;tu api key&gt;\"\n\n\n\n\n\n\nWarning\n\n\n\nNo debes dejar que nadie vea tu API KEY, asi que no subas tu .env a ning√∫n lugar p√∫blico!\n\n\nSi no, puedes agregarlo directamente utilizando la libreria openai o Langchain.Luego de obtener y entregar tu API KEY, en Langchain basta con hacer algo como:\nfrom langchain.chat_models import ChatOpenAI\n\nchat = ChatOpenAI(temperature=0.0)\nprompt = \"Porfavor hazme la tesis!\"\n\nresult = chat(prompt)\nEn este caso, estaremos utilizando dos LLM, en donde la salida de una es la entrada de otra. Langchain ya tiene algo preparado para esto! se les llama Chains. En este caso, tenemos una cadena simple (Dos LLM secuenciales), pero existen otros tipos de arquitecturas mucho m√°s complejas ‚ò†Ô∏è\nPara poder crear nuestra primera Chain no es tanto m√°s complejo que el ejemplo anterior, s√≥lo debemos agregar unos pasos extras:\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models import ChatOpenAI\n\n# LLM Definition\nllm = ChatOpenAI(temperature=0)\n\n# Chain Step 1\nrecipe_chain = LLMChain(llm=llm, prompt=first_prompt, output_key=\"recipe\")\nEn el c√≥digo anterior estamos utilizando la LLM ChatOpenAI, y creando la primera parte de la cadena. El par√°metro output_key nos servir√° m√°s adelante para comunicarle a la segunda parte cual es el nombre del primer output."
  },
  {
    "objectID": "posts/llm-recipe/main.html#obtener-ingredientes-de-una-receta-mediante-llm",
    "href": "posts/llm-recipe/main.html#obtener-ingredientes-de-una-receta-mediante-llm",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "2. Obtener ingredientes de una receta mediante LLM",
    "text": "2. Obtener ingredientes de una receta mediante LLM\nYa sabemos que la salida del paso anterior ser√° una receta, a la cual deberemos extraer los ingredientes. Adem√°s, √©ste es el √∫ltimo paso, por lo que deberemos preocuparnos de que el output que salga del LLM sea adecuado y simple de ‚Äúparsear‚Äù.\nPara el output, lo que haremos ser√° aplicar un poco del ya conocido prompt engineering.\nAdem√°s, en este caso, ya que no existen tantos riesgos de prompt injection , es que usaremos templates sencillos (no haremos uso de los roles en los mensajes)\nsecond_step_template = \"\"\"I need you to bring me the ingredients contained in the following recipe: \\\nrecipe: {recipe}\n{format_instructions}\n{example_instructions}\"\"\"\nVemos que tenemos tres entradas:\n\nrecipe: Este input ser√° el resultado del paso anterior.\nformat_instructions: Ac√° le comunicaremos al LLM como queremos el formato del output.\nexample_instructions: Ac√° aplicamos un poco de lo que se llama few-shot , que es b√°sicamente darle un par de ejemplos al LLM para que entienda como esperamos el resultado.\n\n# Format Instructions\ncustom_format_instructions = \"\"\"\nThe output should be in a json format, formatted in the following schema:\n{\n  \"food\": List // List of ingredients\n  [\n    {\n      \"ingredient\": string // Name of one ingredient\n      \"quantity\": string  // Quantity of the ingredient \n      \"optional\": string  // Whether or not that ingredient is optional to cook the food. \"Yes\" if the ingredient is not indispensable to cook, \"No\" if is the ingredient is indispensable.\n      \"estimated_price\": string  // The ingredient's estimated price in dolars\n      \"available\": string // Random \"Yes\" or \"No\"\n    }\n  ]\n}\n\"\"\"\n    example_instructions = \"\"\"\nFollow the schema of this example:\n{\n  \"food\":\n  [\n    {\n      \"ingredient\": \"Spaguetti\",\n      \"optional\": \"No\",\n      \"quantity\": \"200g\",\n      \"estimated_price\": \"5.00\",\n      \"available\": \"No\"\n    },\n    {\n      \"ingredient\": \"Meat\",\n      \"optional\": \"No\",\n      \"quantity\": \"1kg\",\n      \"estimated_price\": \"10.00\",\n      \"available\": \"Yes\"\n    },\n    {\n      \"ingredient\": \"Pepper\",\n      \"optional\": \"Yes\",\n      \"quantity\": \"at ease\",\n      \"estimated_price\": \"1.00\",\n      \"available\": \"No\"\n    }\n\n  ]\n}\n\"\"\"\n\n\n\n\n\n\nWarning\n\n\n\n‚úãüèΩ Es importante que sepas que langchain cuenta con un m√≥dulo de Output parser que crea por detr√°s el format_instructions e incluso cuenta con funciones que transforman la salida del LLM(string) en el formato que deseabamos (json, lista, Pydantic, etc). La raz√≥n de porqu√© yo no ocup√© esto fue que no funciona para json nesteados. Pero me bas√© en sus instrucciones para crear custom_format_instructions\n\n\nLuego seguimos como lo vimos anteriormente:\nsecond_prompt = ChatPromptTemplate.from_template(second_step_template)\ningredient_chain = LLMChain(llm=llm, prompt=second_prompt, output_key=\"ingredients\")"
  },
  {
    "objectID": "posts/llm-recipe/main.html#cadena-final",
    "href": "posts/llm-recipe/main.html#cadena-final",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "3. Cadena Final",
    "text": "3. Cadena Final\nLuego necesitamos orquestar las cadenas que creamos anteriormente para obtener la cadena final. En Langchain se hace de la siguiente forma:\noverall_simple_chain = SequentialChain(chains=[recipe_chain, ingredient_chain], verbose=True,\n                                       input_variables=[\"food\", \"format_instructions\", \"example_instructions\"],\n                                       output_variables=[\"recipe\", \"ingredients\"])"
  },
  {
    "objectID": "posts/llm-recipe/main.html#output-parser",
    "href": "posts/llm-recipe/main.html#output-parser",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "4. Output Parser",
    "text": "4. Output Parser\nFinalmente, necesitamos parsear el resultado obtenido desde la LLM. Esto es string ‚Üí json.\nDebido a la forma en que le pedimos el resultado al LLM, es que se nos hace muy sencillo:\nimport json\nimport pandas as pd\n\nresult = overall_simple_chain(\n        {\n            \"food\": food, # Esto lo entrega el usuario\n            \"format_instructions\": custom_format_instructions, # Esto lo entregamos nosotros\n            \"example_instructions\": example_instructions, # Esto lo entregamos nosotros\n        }\n    )\n\n# String to json(dict en python)\ndict_response = json.loads(result['ingredients']) \n\n# Dict to df\noutput_df = pd.DataFrame(data = dict_response['food'])\nFinalmente, para la entrada ‚ÄúLasagna‚Äù, podemos obtener algo asi:\n\n\n\nOutput 1 as DataFrame"
  },
  {
    "objectID": "posts/back_to_sit/main.html#installation",
    "href": "posts/back_to_sit/main.html#installation",
    "title": "Back To Sit",
    "section": "Installation",
    "text": "Installation\nTo install back_to_sit library you can use pip:\npip install back-to-sit"
  },
  {
    "objectID": "posts/back_to_sit/main.html#usage",
    "href": "posts/back_to_sit/main.html#usage",
    "title": "Back To Sit",
    "section": "Usage",
    "text": "Usage\nUsing the Back to Sit library is easy. First, you need to create a Telegram bot and obtain an API token. You can follow the instructions on the Telegram website to create a bot and obtain an API token (or easily read the instructions below). Lastly, you need to obtain your chat id.\nHere I will show you how to get both:\n\nAPI Token ü§ñ\nFirst you need to create your own bot ü§ñ. This bot will be your messenger! üíå\nTo create it, follow the next steps:\n\nInstall Telegram in your smartphone\nGo to Chats ‚Üí Search Chats ‚Üí Type BotFather\nThere to create your bot you can simply follow the instructions. But it should be something like this:\n\n/newbot\n\nbot_name # You choose this\n\nexamplebot_bot # You choose this\n\nFinally, get the token API that BotFather bring to you. Nice! As simple as that you have your API token\n\n\n\nChat ID\nNow, you will create a ‚Äúuniversal‚Äù bot, it means that anyone could chat with him/her. So you need to correctly identify the chat between the bot and you! To do this follow the steps:\n\nSearch your bot. In telegram go to Chats ‚Üí Search Chats ‚Üí Type the username of your bot (Ex: examplebot_bot) and enter to the chat.\nWrite a dummy message to the bot. (For example, ‚ÄùHello world‚Äù)\nIn your navigator, enter to https://api.telegram.org/bot{TOKEN_API}/getUpdates. Replace {TOKEN_API} with your API token.\nHere you will see a payload with the messages, from the payload you should just get the ‚Äúid‚Äù number.\n\nPayload Example:\n{\"ok\":true,\"result\":[{\"update_id\":9421735642,\n\"message\":{\"message_id\":3,\"from\":{\"id\":6276864755,\"is_bot\":false,\"first_name\":\"Diego\",\"last_name\":\"Machado\",\"language_code\":\"es\"},\"chat\":{\"id\":6276864755,\"first_name\":\"Diego\",\"last_name\":\"Machado\",\"type\":\"private\"},\"date\":1682916041,\"text\":\"Hello World\"}}]}\nIn the example case, the chat_id is 6276864755\n\n\n\n\n\n\nNote\n\n\n\nüëÄ If you get something like {‚Äúok‚Äù:true,‚Äúresult‚Äù:[]}. Try to send more messages and reload the step 3.\n\n\n\nWell Done! üöÄüöÄüöÄüöÄüöÄ\nOnce you have the API token and the chat id, you are already done to use back to sit! You have two available options, use the back_to_sit()function or use the @back_to_sit_decorator(). We will navigate through them a little bit in the following sections:\n\n\nFunction mode üî®\nThis mode is useful when you are running tasks in your Jupyter notebook and don‚Äôt want to alter your function behavior.\nYou can use back_to_sit() function mode in your code like this:\n# Import Libraries\nfrom back_to_sit import back_to_sit\nimport time\n\n# Define necessary constants\nMESSAGE = \"Hey! wake up! your code is done!\"\nCHAT_ID = \"CHATIDNUMBER\"\nAPI_TOKEN = \"API:TOKEN\"\n\n# Start time is optional if you want backtosit to report the time consumed\nstart_time = time.time()\n\n# Some task code here ...\ntime.sleep(10)\n####\n\n# This line is executed after your task code is done! \nback_to_sit(MESSAGE, CHAT_ID, API_TOKEN, start_time, notebook = False)\n\n\n\nfunction mode\n\n\n\n\n\n\n\n\nNote\n\n\n\nüëÄ Important! If your are running it in Jupyter notebooks, you should give the input parameter notebook=True , otherwise you should do notebook=False\n\n\n\n\n\nDecorator mode ‚ú®\nThis mode is useful when you want to get the back to sit report every time you use the function! or maybe if don‚Äôt want to add an extra line of code!\nYou can use back-to-sit decorator mode in your code like this:\n# Import Libraries\nfrom back_to_sit import back_to_sit\nimport time\n\n# Define necessary constants\nMESSAGE = \"Hey! wake up! your code is done!\"\nCHAT_ID = \"CHATIDNUMBER\"\nAPI_TOKEN = \"API:TOKEN\"\n\n@back_to_sit_decorator(MESSAGE, CHAT_ID, API_TOKEN, notebook = False)\ndef sleep_ten_seconds():\n  time.sleep(10)\n  return None\nIn the decorator mode, the start time is configured automatically. So every time you execute the function you will get the back to sit report!\ngo_sleep = sleep_ten_seconds()\n\n\n\ndecorator mode\n\n\n\n\n\n\n\n\nNote\n\n\n\nüëÄ Important! If your are running it in Jupyter notebooks, you should give the input parameter notebook=True , otherwise you should do notebook=False"
  },
  {
    "objectID": "posts/back_to_sit/main.html#license",
    "href": "posts/back_to_sit/main.html#license",
    "title": "Back To Sit",
    "section": "License",
    "text": "License\nThe Back to Sit library is licensed under the MIT license. See the LICENSE file for more information."
  },
  {
    "objectID": "posts/birdclef2023/main.html#short-time-fourier-transform",
    "href": "posts/birdclef2023/main.html#short-time-fourier-transform",
    "title": "BirdClef 2023 Competition",
    "section": "Short Time Fourier Transform",
    "text": "Short Time Fourier Transform\nSabiendo ahora que lo que necesitamos es obtener espectogramas, ahora necesitamos saber como. Para tener un espectograma, necesitamos calcular las variaciones de frecuencias a trav√©s del tiempo, por lo que necesitamos calcular frecuencias. Una metodolog√≠a muy conocida en el √°rea de matem√°ticas que se utiliza para esto es la transformada de fourier. Lamentablemente existen unos problemas al intentar aplicarla a todo un audio, es por esto que se utiliza una t√©cnica llamada windowing, que lo que busca es dividir el audio en frames y aplicar la transformada de fourier a cada frame. Esta t√©cnica se conoce como Short-time Fourier Transform (STFT)\nVeamos como calcularlo:\nLa transformada de Fourier de tiempo corto (Short Time Fourier Transform o STFT) es una t√©cnica ampliamente utilizada en el procesamiento de audio y se utiliza com√∫nmente en aplicaciones de aprendizaje autom√°tico. Perm√≠teme explicarte c√≥mo se calcula y los conceptos clave asociados.\nLa STFT es una forma de analizar se√±ales de audio en dominio de frecuencia a lo largo del tiempo. Permite descomponer una se√±al de audio en sus componentes de frecuencia en intervalos de tiempo peque√±os y consecutivos. Esto es √∫til para capturar informaci√≥n detallada sobre c√≥mo cambian las frecuencias a lo largo del tiempo en una se√±al de audio.\nA continuaci√≥n, describir√© los conceptos clave involucrados en el c√°lculo de la STFT:\n\nVentaneo (Windowing): El ventaneo es una t√©cnica utilizada para evitar artefactos en la transformada de Fourier al considerar solo una secci√≥n finita de la se√±al a la vez. La se√±al de audio se multiplica por una funci√≥n de ventana, que es b√°sicamente una funci√≥n que se desvanece hacia los extremos. Al multiplicar la se√±al por la ventana, se reduce la cantidad de energ√≠a ‚Äúderramada‚Äù en las frecuencias adyacentes, lo que mejora la resoluci√≥n en frecuencia. Las funciones de ventana comunes incluyen la ventana rectangular, la ventana Hamming, la ventana Blackman, entre otras.\nTama√±o de ventana (Window Size o Frame Size): El tama√±o de ventana se refiere al n√∫mero de muestras de la se√±al de audio que se consideran para calcular la transformada de Fourier en un momento dado. Espec√≠ficamente, la se√±al se divide en segmentos superpuestos de tama√±o de ventana y se aplica la funci√≥n de ventana a cada segmento. Un tama√±o de ventana m√°s grande proporciona una mejor resoluci√≥n en frecuencia, pero reduce la resoluci√≥n en el tiempo, mientras que un tama√±o de ventana m√°s peque√±o tiene el efecto contrario.\nSalto de ventana (Hop Size): El salto de ventana, tambi√©n conocido como desplazamiento o hop size, se refiere a la cantidad de muestras que se desplaza la ventana despu√©s de calcular la transformada de Fourier para un segmento. Un salto de ventana m√°s peque√±o resulta en una mayor superposici√≥n entre los segmentos adyacentes y, por lo tanto, proporciona una mejor resoluci√≥n en el tiempo. Sin embargo, tambi√©n implica un mayor costo computacional. Un salto de ventana m√°s grande reduce la superposici√≥n y aumenta la resoluci√≥n en frecuencia.\n\nEl proceso general para calcular la STFT se realiza de la siguiente manera:\n\nDivide la se√±al de audio en segmentos superpuestos de tama√±o de ventana.\nAplica una funci√≥n de ventana a cada segmento para reducir el efecto de los extremos.\nCalcula la transformada de Fourier de cada segmento.\nRepite los pasos anteriores desplazando la ventana por el salto de ventana hasta que se haya cubierto toda la se√±al de audio.\n\nEl resultado de la STFT es una matriz 2D que representa la magnitud o la fase de las componentes de frecuencia a lo largo del tiempo. Esta matriz se puede utilizar posteriormente en t√©cnicas de aprendizaje autom√°tico, como redes neuronales, para extraer caracter√≠sticas o para realizar tareas espec√≠ficas, como clasificaci√≥n de audio, detecci√≥n de eventos,entre otras.\nEn resumen, la STFT es una herramienta poderosa en el procesamiento de audio y el aprendizaje autom√°tico. Permite analizar las caracter√≠sticas de frecuencia que cambian en el tiempo en una se√±al de audio y se calcula mediante el ventaneo de la se√±al, el c√°lculo de la transformada de Fourier en cada segmento y el desplazamiento de la ventana a lo largo de la se√±al.\n\n# Seteamos el tama√±o del frame (en cantidad de samples) y el tama√±o del HOP\nFRAME_SIZE = 1024\nHOP_SIZE = FRAME_SIZE//2\n\n\n# Utilizamos librosa para aplicar stft\ns_audio = librosa.stft(audio_data, n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n\n\ns_audio.shape #(frequency bins, frames) # Complex numbers\n\n(513, 216)\n\n\n\nFRAME_SIZE//2 + 1, (n_samples + FRAME_SIZE)//HOP_SIZE + 1\n\n(513, 218)\n\n\nLa t√©cnica STFT nos entrega n√∫meros complejos, para poder entenderlos e ingresarlos a un modelo de machine learning, calculamos su magnitud\n\n# We get the magnitude \nm_audio = np.abs(s_audio)**2\n\n¬øComo observamos de otra forma un audio?\nUn espectrograma es una representaci√≥n visual que muestra c√≥mo var√≠a la energ√≠a espectral de una se√±al a lo largo del tiempo. Muestra c√≥mo se distribuyen las diferentes frecuencias en una se√±al en funci√≥n del tiempo. En un espectrograma convencional, la amplitud de las frecuencias se representa mediante colores o tonos de grises, donde los colores m√°s brillantes o m√°s claros indican una mayor amplitud y los colores m√°s oscuros indican una menor amplitud.\nCon esto, ahora podemos observar los siguientes espectogramas:\n\n# Plot Spectogram\ndef plot_spectrogram(Y, sr, hop_length, y_axis=\"linear\"):\n    \"\"\"\n    Plot a spectrogram of an audio signal.\n\n    Parameters:\n    - Y (ndarray): 2D array representing the spectrogram.\n    - sr (int): Sampling rate of the audio signal.\n    - hop_length (int): Number of samples between successive frames.\n    - y_axis (str, optional): Scale of the frequency axis. Default is \"linear\".\n\n    Returns:\n    None\n\n    This function plots a spectrogram of the audio signal using the provided parameters. The spectrogram represents\n    the distribution of frequencies over time.\n\n    Example usage:\n    &gt;&gt;&gt; plot_spectrogram(Y, sr=22050, hop_length=512, y_axis=\"log\")\n\n    \"\"\"\n    plt.figure(figsize=(25, 10))\n    librosa.display.specshow(Y, \n                             sr=sr, \n                             hop_length=hop_length, \n                             x_axis=\"time\", \n                             y_axis=y_axis)\n    plt.colorbar(format=\"%+2.f\")\n\n\nplot_spectrogram(m_audio, sampling_rate, HOP_SIZE)\n\n\n\n\nPodemos notar que las frecuencias son muy bajas, para poder observarlo de mejor forma, calculamos el logaritmo, lo que equivale a transformar el sonido a decibeles (algo que los humanos podemos entender)."
  },
  {
    "objectID": "posts/birdclef2023/main.html#log-amplitud-spectogram",
    "href": "posts/birdclef2023/main.html#log-amplitud-spectogram",
    "title": "BirdClef 2023 Competition",
    "section": "Log-Amplitud Spectogram",
    "text": "Log-Amplitud Spectogram\nPor otro lado, un espectrograma de amplitud logar√≠tmica (Log-Amplitude Spectrogram) aplica una transformaci√≥n logar√≠tmica a la amplitud de las frecuencias antes de su representaci√≥n visual. En lugar de mostrar la amplitud directamente, se muestra el logaritmo de la amplitud. Esto se hace para mejorar la interpretaci√≥n visual y resaltar mejor las caracter√≠sticas de baja amplitud en la se√±al.\nExisten varias razones por las que se utiliza un espectrograma de amplitud logar√≠tmica en lugar de un espectrograma normal:\nEscala perceptual: La audici√≥n humana no percibe la amplitud de manera lineal, sino de forma logar√≠tmica. Al aplicar la transformaci√≥n logar√≠tmica, el espectrograma de amplitud logar√≠tmica se ajusta m√°s a c√≥mo percibimos el sonido, lo que facilita su interpretaci√≥n.\nRango din√°mico: El uso de una escala logar√≠tmica comprime el rango din√°mico del espectrograma. Esto significa que las diferencias de amplitud m√°s peque√±as se vuelven m√°s visibles y f√°ciles de distinguir, lo que permite apreciar mejor las caracter√≠sticas de baja amplitud en la se√±al.\nReducci√≥n de ruido: Al aplicar una transformaci√≥n logar√≠tmica, las partes m√°s ruidosas de la se√±al se aten√∫an, lo que ayuda a reducir la presencia visual del ruido y mejora la legibilidad del espectrograma.\n\n# To a better visualization, we obtain logs (transform to decibels)\n\nY_log_scale = librosa.power_to_db(m_audio)\nplot_spectrogram(Y_log_scale, sampling_rate, HOP_SIZE)"
  },
  {
    "objectID": "posts/birdclef2023/main.html#log-frequency-spectogram",
    "href": "posts/birdclef2023/main.html#log-frequency-spectogram",
    "title": "BirdClef 2023 Competition",
    "section": "Log-Frequency Spectogram",
    "text": "Log-Frequency Spectogram\n\n# Now we also put the log in the Y-axis (frequency)\nplot_spectrogram(Y_log_scale, sampling_rate, HOP_SIZE, y_axis=\"log\")\n\n\n\n\n¬ø Como se obtienen los espectogramas?\nEl c√°lculo de un espectrograma implica varios pasos:\n\nPreprocesamiento de la se√±al: Si la se√±al de audio no est√° en el dominio del tiempo discreto, se debe convertir a ese formato. Adem√°s, es posible que desees aplicar t√©cnicas de preprocesamiento adicionales, como normalizaci√≥n o eliminaci√≥n de ruido, para mejorar la calidad del espectrograma.\nDivisi√≥n de la se√±al en segmentos: La se√±al de audio se divide en segmentos superpuestos de longitud fija. La longitud de estos segmentos se denomina ‚Äúventana‚Äù y es un par√°metro importante en el c√°lculo del espectrograma. La elecci√≥n de la longitud de la ventana depende de la resoluci√≥n temporal y frecuencial deseada. Una ventana t√≠picamente utilizada es la ventana de Hamming.\nAplicaci√≥n de la funci√≥n de ventana: A cada segmento de la se√±al se le aplica una funci√≥n de ventana, como la ventana de Hamming mencionada anteriormente. La funci√≥n de ventana reduce las discontinuidades en los extremos de los segmentos y ayuda a evitar el sangrado espectral (spectral leakage), que puede introducir artefactos no deseados en el espectrograma.\nC√°lculo de la Transformada de Fourier de tiempo corto (STFT): Para cada segmento de la se√±al, se calcula la Transformada de Fourier de tiempo corto (STFT). La STFT se obtiene aplicando la Transformada de Fourier a cada segmento de la se√±al despu√©s de aplicar la funci√≥n de ventana. La STFT proporciona informaci√≥n sobre la distribuci√≥n de frecuencias en cada segmento de tiempo.\nC√°lculo del espectrograma: El espectrograma se crea a partir de la magnitud cuadrada de los valores obtenidos en la STFT. La magnitud cuadrada se calcula para resaltar las amplitudes y eliminar la informaci√≥n de fase. Luego, se aplica una escala de colores o una escala de grises para representar la amplitud de cada frecuencia en funci√≥n del tiempo. Los valores m√°s altos de amplitud se suelen representar con colores m√°s brillantes o tonos de grises m√°s claros, mientras que los valores m√°s bajos se representan con colores m√°s oscuros o tonos de grises m√°s oscuros.\n\nEs importante tener en cuenta que los par√°metros utilizados en el c√°lculo del espectrograma, como el tama√±o de la ventana y el solapamiento de los segmentos, pueden afectar la resoluci√≥n temporal y frecuencial del espectrograma resultante. Por lo tanto, es posible que debas ajustar estos par√°metros seg√∫n las necesidades espec√≠ficas de tu aplicaci√≥n."
  },
  {
    "objectID": "posts/birdclef2023/main.html#train-val-split",
    "href": "posts/birdclef2023/main.html#train-val-split",
    "title": "BirdClef 2023 Competition",
    "section": "Train, Val SPlit",
    "text": "Train, Val SPlit\nExiste un concepto en Machine Learning muy importante llamado ‚Äúoverfitting‚Äù, que b√°sicamente es cuando los modelos comienzan a aprenderse de memoria los datos en vez de buscar patrones que puedan funcionar bien para predecir a futuro. Es por esto que una de las acciones que se toman para evitar el overfitting es un ‚Äúsplit‚Äù de la data. Esto es, simplemente dividimos la data en dos partes (o en 3), una parte llamada de entrenamiento y otra llamada de prueba. As√≠ entrenamos nuestro modelo en la data de entrenamiento y luego calculamos m√©tricas en la parte de prueba para as√≠ tener con mayor certeza una aproximaci√≥n de como nuestro modelo lo har√° para datos que no fueron vistos a la hora de entrenar. El splitting es mucho m√°s complicado que esto, pero con esto se tiene una idea.\nYa que dividimos los audios, tenemos que asegurarnos que todos los splits de un mismo audio esten o en train o en test, para no generar leakages.\n\nOriginal Metadata\n\noriginal_metadata\n\n\n\n\n\n\n\n\nprimary_label\nsecondary_labels\ntype\nlatitude\nlongitude\nscientific_name\ncommon_name\nauthor\nlicense\nrating\nurl\nfilename\n\n\n\n\n0\nabethr1\n[]\n['song']\n4.3906\n38.2788\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nRolf A. de By\nCreative Commons Attribution-NonCommercial-Sha...\n4.0\nhttps://www.xeno-canto.org/128013\nabethr1/XC128013.ogg\n\n\n1\nabethr1\n[]\n['call']\n-2.9524\n38.2921\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nJames Bradley\nCreative Commons Attribution-NonCommercial-Sha...\n3.5\nhttps://www.xeno-canto.org/363501\nabethr1/XC363501.ogg\n\n\n2\nabethr1\n[]\n['song']\n-2.9524\n38.2921\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nJames Bradley\nCreative Commons Attribution-NonCommercial-Sha...\n3.5\nhttps://www.xeno-canto.org/363502\nabethr1/XC363502.ogg\n\n\n3\nabethr1\n[]\n['song']\n-2.9524\n38.2921\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nJames Bradley\nCreative Commons Attribution-NonCommercial-Sha...\n5.0\nhttps://www.xeno-canto.org/363503\nabethr1/XC363503.ogg\n\n\n4\nabethr1\n[]\n['call', 'song']\n-2.9524\n38.2921\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nJames Bradley\nCreative Commons Attribution-NonCommercial-Sha...\n4.5\nhttps://www.xeno-canto.org/363504\nabethr1/XC363504.ogg\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16936\nyewgre1\n[]\n['']\n-1.2502\n29.7971\nEurillas latirostris\nYellow-whiskered Greenbul\nAndr√°s Schmidt\nCreative Commons Attribution-NonCommercial-Sha...\n3.0\nhttps://xeno-canto.org/703472\nyewgre1/XC703472.ogg\n\n\n16937\nyewgre1\n[]\n['']\n-1.2489\n29.7923\nEurillas latirostris\nYellow-whiskered Greenbul\nAndr√°s Schmidt\nCreative Commons Attribution-NonCommercial-Sha...\n4.0\nhttps://xeno-canto.org/703485\nyewgre1/XC703485.ogg\n\n\n16938\nyewgre1\n[]\n['']\n-1.2433\n29.7844\nEurillas latirostris\nYellow-whiskered Greenbul\nAndr√°s Schmidt\nCreative Commons Attribution-NonCommercial-Sha...\n4.0\nhttps://xeno-canto.org/704433\nyewgre1/XC704433.ogg\n\n\n16939\nyewgre1\n[]\n['']\n0.0452\n36.3699\nEurillas latirostris\nYellow-whiskered Greenbul\nLars Lachmann\nCreative Commons Attribution-NonCommercial-Sha...\n4.0\nhttps://xeno-canto.org/752974\nyewgre1/XC752974.ogg\n\n\n16940\nyewgre1\n[]\n['']\n-0.3986\n37.3087\nEurillas latirostris\nYellow-whiskered Greenbul\nLars Lachmann\nCreative Commons Attribution-NonCommercial-Sha...\n4.5\nhttps://xeno-canto.org/753190\nyewgre1/XC753190.ogg\n\n\n\n\n16941 rows √ó 12 columns\n\n\n\n\n# Contamos los audios para cada pajaro\nm_count = original_metadata.primary_label.value_counts()\none_class = list(m_count[m_count &lt;= 1].index)\n# Aseguramos aquellos pajaros que si o si estar√°n en el entrenamiento (s√≥lo tienen un dato)\nto_train = original_metadata[original_metadata.primary_label.isin(one_class)].copy()\nmetadata = original_metadata[~original_metadata.primary_label.isin(one_class)].copy()\n\n\n# Hacemos el split\nfrom sklearn.model_selection import train_test_split\nmetadata_train, metadata_test = train_test_split(metadata, train_size = CFG.TRAIN_SIZE, random_state=CFG.seed, stratify = metadata['primary_label'])\n\n\n# Le agregamos aquellos pajaros que deben ir en el entrenamiento\nmetadata_train = pd.concat([metadata_train, to_train], axis = 0)\n\n\n# Guardamos los datos\nmetadata_train.to_csv('train_metadata.csv', index = False)\nmetadata_test.to_csv('val_metadata.csv', index = False)"
  },
  {
    "objectID": "posts/birdclef2023/main.html#melspectograms-efficientnet",
    "href": "posts/birdclef2023/main.html#melspectograms-efficientnet",
    "title": "BirdClef 2023 Competition",
    "section": "MelSpectograms + EfficientNet",
    "text": "MelSpectograms + EfficientNet\nComo mencionamos anteriormente, la metodolog√≠a m√°s comun para clasificar audios es tratar el problema como un de visi√≥n. En donde la im√°gen que queremos procesar est√° dada por un espectograma. En este caso utilizaremos una variaci√≥n llamada MelSpectogram.\nEl modelo clasificador, ser√° el modelo ya pre-entrenado llamado EfficientNet. El motivo de esto es que seg√∫n comentarios de otros competidores, este modelo funciona bien en este tipo de data (Audios de p√°jaros). Existen distintos repositorios en donde podemos encontrar estos modelos pre-entrenados (Tensorflow Hub, Pytorch Hub, timm, etc.). En este caso usar√© timm ¬øPor qu√©? Simplemente porque hab√≠a o√≠do de el y quice probarlo.\n\nTimm\nEl framework de aprendizaje autom√°tico timm (Transfer Image Models) es una biblioteca de c√≥digo abierto que proporciona una amplia colecci√≥n de modelos preentrenados y herramientas para la visi√≥n por computadora\n\n\nEfficientNet\nEfficientNet es un modelo de red neuronal convolucional desarrollado por Google que destaca por su eficiencia y rendimiento sobresaliente en la clasificaci√≥n de im√°genes. Utiliza un enfoque de escalado compuesto, ajustando la profundidad, el ancho y la resoluci√≥n de la imagen de entrada de manera √≥ptima. Esto le permite lograr un equilibrio entre el tama√±o de la red y el rendimiento, superando a modelos anteriores y siendo altamente eficiente en el uso de recursos computacionales y memoria.\n\n\nMelSpectogram\nUn mel spectrograma, o melspectrograma, es una representaci√≥n visual que muestra c√≥mo var√≠a la energ√≠a espectral de una se√±al de audio en funci√≥n del tiempo y de las bandas de frecuencia mel. El melspectrograma es una versi√≥n modificada del espectrograma convencional que utiliza una escala de frecuencia perceptualmente m√°s relevante, conocida como escala mel.\nLa escala mel es una escala no lineal que se basa en la percepci√≥n humana de las frecuencias de sonido. Esta escala asigna m√°s resoluci√≥n a las frecuencias m√°s bajas, donde nuestro sistema auditivo es m√°s sensible, y menos resoluci√≥n a las frecuencias m√°s altas.\nEl proceso de c√°lculo de un melspectrograma implica los siguientes pasos:\n\nPreprocesamiento de la se√±al: Al igual que con el c√°lculo de un espectrograma, se pueden aplicar t√©cnicas de preprocesamiento a la se√±al de audio, como conversi√≥n al dominio del tiempo discreto y normalizaci√≥n.\nDivisi√≥n de la se√±al en segmentos: La se√±al de audio se divide en segmentos superpuestos de longitud fija, al igual que en el c√°lculo del espectrograma.\nAplicaci√≥n de la funci√≥n de ventana: A cada segmento de la se√±al se le aplica una funci√≥n de ventana, como la ventana de Hamming, para reducir las discontinuidades en los extremos de los segmentos y evitar el sangrado espectral.\nC√°lculo de la Transformada de Fourier de tiempo corto (STFT): Se calcula la STFT para cada segmento de la se√±al despu√©s de aplicar la funci√≥n de ventana. La STFT proporciona informaci√≥n sobre la distribuci√≥n de frecuencias en cada segmento de tiempo.\nConversi√≥n a la escala mel: Los coeficientes de frecuencia obtenidos de la STFT se convierten a la escala mel utilizando una transformaci√≥n no lineal. Esta transformaci√≥n se basa en filtros mel, que se superponen en el dominio de la frecuencia para representar las bandas de frecuencia mel.\nC√°lculo del melspectrograma: El melspectrograma se crea a partir de la magnitud cuadrada de los valores obtenidos en el paso anterior. Luego, se aplica una escala de colores o una escala de grises para representar la amplitud de cada banda de frecuencia mel en funci√≥n del tiempo.\n\nEn resumen, un melspectrograma es una representaci√≥n visual que muestra c√≥mo var√≠a la energ√≠a espectral de una se√±al de audio en funci√≥n del tiempo y de las bandas de frecuencia mel. Se calcula mediante la conversi√≥n de los coeficientes de frecuencia obtenidos de la STFT a la escala mel y representa la amplitud de cada banda de frecuencia mel en funci√≥n del tiempo utilizando una escala de colores o de grises.\n\ndef get_sample():\n    audio_samples = glob.glob('/kaggle/input/birdclef-2023/train_audio/*/*.ogg')\n    random_audio = np.random.choice(audio_samples)\n    bird_class = random_audio.split('/')[-2]\n    #signal, sampling_rate = librosa.load(random_audio)\n    signal, sampling_rate  = torchaudio.load(random_audio)\n    return signal, sampling_rate\n\ndef plot_waveform(signal, sr):\n    # Plot the waveform\n    plt.figure(figsize=(15, 5))\n    lid.waveshow(signal.numpy(), sr=sr)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Amplitude')\n    plt.show()\n    \ndef display_audio(signal, sr):\n    return Audio(signal, rate = sr)\n\ndef get_mel_spectogram(signal, sr):\n    mel_t = T.MelSpectrogram(sample_rate = sr, n_fft = CFG.FRAME_SIZE,\n                            hop_length = CFG.HOP_SIZE, n_mels = CFG.N_MELS)\n    mel = mel_t(signal).squeeze(0)\n    return mel\n\n# Plot Spectogram\ndef plot_spectrogram(mel, sr, hop_length, scaled = False):\n    if not scaled:\n        # Convert to dB scale (log-scale)\n        mel = librosa.power_to_db(mel, ref=np.max)\n        \n    # Plot the mel spectrogram\n    plt.figure(figsize=(10, 4))\n    librosa.display.specshow(mel, x_axis='time',\n                             y_axis='mel', sr=sr, hop_length=CFG.HOP_SIZE)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Mel spectrogram')\n    plt.tight_layout()\n    plt.show()\n\n    \n    \n\n\nsample, sr = get_sample()\n\n\nplot_waveform(sample, sr)\n\n\n\n\n\nmel = get_mel_spectogram(sample, sr)\n\n\nplot_spectrogram(mel, sr, CFG.HOP_SIZE, scaled = False)"
  },
  {
    "objectID": "posts/birdclef2023/main.html#data-augmentation",
    "href": "posts/birdclef2023/main.html#data-augmentation",
    "title": "BirdClef 2023 Competition",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nOtro concepto importante que ser√≠a bueno conocer, esta vez en DeepLearning, es Data Augmentation. Como su nombre lo dice, este busca aumentar los datos de forma sint√©tinca.\n¬øPor qu√©? En ML siempre es bueno tener m√°s datos, podemos ver los datos como experiencia, a mayor experiencia, mayor conocimiento tenemos.\n¬øC√≥mo? En visi√≥n existen distintos tipos de Data Augmentation, por ejemplo, tomar algunos de los datos que ya tenemos y girarlos aleatoriamente, o quiz√° cortalos, o quiz√°s hacerles un zoom, etc. En este caso, las t√©cnicas tradicionales de Data Augmentation en visi√≥n no son √∫tiles porque no tienen sentido o no son interpretables ¬ø Que significa que haga un zoom a un espectograma? O que lo gire 90 grados?\nEn este caso aplicaremos dos tipos de Data Augmentation:\n\nSignal Data Augmentation: Buscamos aplicarle ciertos efectos al audio en si\nSpectogram Augmentation: Utilizaremos algunas t√©cnicas que si tengan sentido cuando trabajamos con Audio.\n\n\nMixUp\nM√°s adelante veremos otra t√©cnica llamada MixUp.\n\nclass ComposeTransform:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, audio_data):\n        for t in self.transforms:\n            audio_data = t(audio_data)\n        return audio_data\n\n\n\n\nSignal Augmentations\n\n\nGaussian Noise:\nGaussian Noise en el procesamiento de audio es un tipo de ruido aleatorio que se utiliza para simular ruidos ambientales y evaluaciones de calidad del sonido. Se genera agregando valores aleatorios extra√≠dos de una distribuci√≥n normal a la se√±al de audio original. Adem√°s, se utiliza como t√©cnica de regularizaci√≥n en el entrenamiento de modelos de aprendizaje autom√°tico. Su aplicaci√≥n implica agregar variabilidad y evaluar el impacto del ruido en la se√±al de audio.\n\nimport random\n\n\nclass GaussianNoise:\n    def __init__(self, mean = 0, sigma = 'auto', p = 0.5):\n        self.mean = mean\n        self.sigma = sigma\n        self.p = p\n        \n    def __call__(self, audio):\n        random_f = torch.rand([]).item()\n        \n        if self.sigma == 'auto':\n            sigma = audio.std()\n        else:\n            sigma = self.sigma\n    \n        if self.p &lt; random_f:\n\n            std_dev = random.uniform(0, sigma)\n            noise = torch.randn_like(audio) * std_dev + self.mean\n\n            # Add the noise to the audio signal\n            audio = audio + noise\n    \n        return audio\n        \n\n\ngaussian_noise = GaussianNoise()\n\n\nnoise_sample = gaussian_noise(sample)\n\n\nplot_waveform(noise_sample, sr)\n\n\n\n\n\ndisplay_audio(sample, sr)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndisplay_audio(noise_sample, sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nsignal_augmentations = ComposeTransform([\n    GaussianNoise()\n])\n\n\n\nSpectogram Augmentations\n\n\nFrequency Masking\nFrequencyMasking es una t√©cnica utilizada en el procesamiento de audio para eliminar o enmascarar informaci√≥n de frecuencia espec√≠fica en una se√±al de sonido. Funciona al ‚Äúmascarar‚Äù o bloquear un rango de frecuencias en el espectro de audio. Esto se logra aplicando un filtro que aten√∫a o elimina selectivamente las componentes de frecuencia dentro del rango especificado. Al enmascarar ciertas frecuencias, se pueden ocultar caracter√≠sticas no deseadas o mejorar la detecci√≥n de otras caracter√≠sticas importantes en la se√±al de audio. Esta t√©cnica es com√∫nmente utilizada en tareas de reconocimiento de voz, separaci√≥n de fuentes de audio y reducci√≥n de ruido para mejorar la calidad y el rendimiento de los algoritmos de procesamiento de audio.\n\n\nTimeMasking\nTimeMasking es una t√©cnica utilizada en el procesamiento de audio para enmascarar o bloquear segmentos de tiempo espec√≠ficos en una se√±al de sonido. Consiste en ocultar o atenuar temporalmente ciertas partes de la se√±al de audio, creando una especie de ‚Äúm√°scara‚Äù en el dominio del tiempo. Al aplicar TimeMasking, se bloquea temporalmente la informaci√≥n de sonido dentro de un intervalo de tiempo determinado. Esta t√©cnica es √∫til para ocultar partes no deseadas de la se√±al, mejorar la privacidad de la informaci√≥n o enfocar la atenci√≥n en partes espec√≠ficas del audio. Es ampliamente utilizado en tareas como el procesamiento de voz, reconocimiento de habla y an√°lisis de se√±ales de audio en general.\n\n\nData Augmentation\nB√°sicamente puedes imaginar un masking de forma horizontal y vertical si lo vemos en un espectograma\n\nspec_augmentations = nn.Sequential(\n    T.FrequencyMasking(CFG.FREQ_MASK_PARAM),\n    T.TimeMasking(time_mask_param = CFG.TIME_MASK_PARAM, p = CFG.P_TIME_MASK)\n)\n\n\naugm_spec = spec_augmentations(mel[None, ...])\n\n\nplot_spectrogram(augm_spec.squeeze(0), sr, CFG.HOP_SIZE, scaled = False)"
  },
  {
    "objectID": "posts/birdclef2023/main.html#load-data",
    "href": "posts/birdclef2023/main.html#load-data",
    "title": "BirdClef 2023 Competition",
    "section": "Load Data",
    "text": "Load Data\nEn este caso de uso, haremos uso de Pytorch Lighting, que es un framework basado en Pytorch. En cualquier framework que decidamos utilizar se debe crear un script para cargar los datos. En el caso de Pytorch creamos una clase que debe tener ciertos requisitos:\n\n__len__\n__getitem__\n\nEn este caso, incorpor√© la creaci√≥n de MelSpectograms, para crear cada instancia entonces realizo: 1. Obtener el path del audio correspondiente 2. Leer el audio utilizando torchaudio 3. Resample: la idea es que todos los sonidos tengan la misma ‚Äúresoluci√≥n‚Äù, por esto resampleamos cada audio 4. Mix Down: Dejamos el audio con un s√≥lo canal: stereo -&gt; mono 5. Cut: Ya que existen audios que tienen diferentes largos, cortamos 5 segundos aleatorios de cada uno. En caso de ser validaci√≥n, tomamos los primeros 5 segundos del audio, esto debido a que la data de validaci√≥n debe ser la misma en cada epoch. 6. Signal Augmentations: Aplicamos Data Augmentation para se√±ales 7. MelSpectograms: Obtenemos los MelSpectogramas 8. Pasamos la escala a decibeles, lo hace m√°s ‚Äúentendible‚Äù 9. Spectograms Augmentations: Aplicamos Data Augmentation para spectogramas 10. Para crear imagenes con 3 canales, repetimos el spectograma 11. Target Transform: ya que el target debe ser num√©rico, hacemos esta transformaci√≥n.\n\nclass BirdDataset(Dataset):\n     \"\"\"\n    Dataset class for bird audio classification.\n\n    Parameters:\n    - metadata_path (str): Path to the metadata CSV file.\n    - audio_path (str): Path to the directory containing the audio files.\n    - sr (int, optional): Sampling rate of the audio files. Default is CFG.SAMPLE_RATE.\n    - framesize (int, optional): Size of the audio frames. Default is CFG.FRAME_SIZE.\n    - hop (int, optional): Number of samples between successive frames. Default is CFG.HOP_SIZE.\n    - n_mels (int, optional): Number of mel bands in the mel spectrogram. Default is CFG.N_MELS.\n    - n_samples (int, optional): Number of samples to be used from each audio file. Default is CFG.N_SAMPLES.\n    - signal_augmentations (callable, optional): Function or transform to apply signal augmentations. Default is None.\n    - spec_augmentations (callable, optional): Function or transform to apply spectrogram augmentations. Default is None.\n    - train (bool, optional): Whether the dataset is for training or not. Default is True.\n\n    Methods:\n    - __len__(): Returns the number of samples in the dataset.\n    - __getitem__(idx): Retrieves a sample from the dataset.\n\n    \"\"\"\n    def __init__(self, metadata_path, audio_path, sr = CFG.SAMPLE_RATE,\n                 framesize = CFG.FRAME_SIZE, hop = CFG.HOP_SIZE,\n                 n_mels = CFG.N_MELS, n_samples = CFG.N_SAMPLES, \n                 signal_augmentations = None, spec_augmentations = None, train = True):\n        \"\"\"\n        Initialize the BirdDataset.\n\n        Loads the metadata CSV file, sets the audio paths and parameters, and defines augmentations.\n\n        \"\"\"\n        \n        self.metadata = pd.read_csv(metadata_path)\n        self.audio_path = audio_path\n        self.framesize = framesize\n        self.hop = hop\n        self.sr = sr\n        self.n_mels = n_mels\n        self.num_samples = n_samples\n        self.signal_augmentations = signal_augmentations\n        self.spec_augmentations = spec_augmentations\n        self.train = train\n        \n    \n        \n    \n    def __len__(self):\n        \"\"\"\n        Return the number of samples in the dataset.\n\n        Returns:\n        - int: Number of samples.\n\n        \"\"\"\n        return len(self.metadata)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieve a sample from the dataset.\n\n        Parameters:\n        - idx (int): Index of the sample to retrieve.\n\n        Returns:\n        - tuple: A tuple containing the mel spectrogram and the target label.\n\n        \"\"\"\n        # 1. Get image path and label\n        path, label = self.metadata.loc[idx, [\"filename\", \"primary_label\"]]\n        full_path = os.path.join(self.audio_path, path)\n        \n        # 2. Load audio\n        waveform, sample_rate = torchaudio.load(full_path)\n        \n        \n        \n        # 3. & 4. preprocess\n        waveform = self._resample(waveform, sample_rate)\n        waveform = self._mix_down(waveform)\n        \n        #5. Cut or take first\n        if not(CFG.SPLIT_AUDIO):\n            if self.train:\n                waveform = self._random_cut(waveform) # Get random split if &gt; duration\n            else:\n                waveform = self._take_first(waveform) # Get the first split (Validation should not be random)\n\n            waveform = self._pad(waveform)\n        \n        #6. Signal Augmentations\n        if self.signal_augmentations != None:\n            waveform = self.signal_augmentations(waveform)\n            \n\n        \n        #7. MelSpectogram\n        mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate = self.sr, n_fft = self.framesize,\n                                                             hop_length = self.hop, n_mels = self.n_mels)\n        mels = mel_transform(waveform)\n        \n        #8. To decibels\n        to_db = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n        mels = to_db(mels)\n        \n        #9. Spectogram Augmentations\n        if self.spec_augmentations != None:\n            mels = self.spec_augmentations(mels)\n            \n        # ride off the batch\n        mels = mels.squeeze(0)\n    \n        #10. Stack (Add channels in based in efffnet requirements)\n        if CFG.CHANNELS==3:\n            mels = torch.stack([mels,mels, mels], dim=0)\n        else:\n            mels = mels[None, :, :]\n        \n       \n        \n        # 11. Label transform (OHE)\n        num_label = CFG.name2label[label]\n        target = F.one_hot(torch.tensor(num_label), num_classes = CFG.N_CLASSES)\n        target = target.type(torch.FloatTensor)\n        \n        return mels, target\n    \n    \n    #### Helper Functions #####\n    \n    def _random_cut(self, signal):\n        \"\"\"\n        Randomly cut a segment from the given signal.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n\n        Returns:\n        - Tensor: Cut segment of the signal.\n\n        \"\"\"\n        if signal.shape[1] &gt; self.num_samples:\n            #signal = signal[:, :self.num_samples]\n            diff_len = signal.shape[1] - self.num_samples \n            idx = int(torch.rand([]).item()*diff_len)\n            signal = signal[:, idx: (idx + self.num_samples)]\n        return signal\n    \n    \n    def _take_first(self, signal):\n        \"\"\"\n        Take the first segment from the given signal.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n\n        Returns:\n        - Tensor: First segment of the signal.\n\n        \"\"\"\n        if signal.shape[1] &gt; self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n            \n    \n    def _resample(self, signal, sr):\n        \"\"\"\n        Resample the signal to the target sampling rate.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n        - sr (int): Original sampling rate of the signal.\n\n        Returns:\n        - Tensor: Resampled signal.\n\n        \"\"\"\n        if sr != self.sr:\n            resampler = torchaudio.transforms.Resample(sr, self.sr)\n            signal = resampler(signal)\n        return signal\n    \n    def _pad(self, signal):\n        \"\"\"\n        Pad the signal with zeros if its length is shorter than the desired number of samples.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n\n        Returns:\n        - Tensor: Padded signal.\n\n        \"\"\"\n        length_signal = signal.shape[1]\n        if length_signal &lt; self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n    \n    def _mix_down(self, signal):\n        \"\"\"\n        Mix down a multi-channel signal to a single channel.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n\n        Returns:\n        - Tensor: Mixed down signal.\n\n        \"\"\"\n        if signal.shape[0] &gt; 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n    \n            \n    \n    \n\n        \n        \n        \n\n\nDataloader\nFinalmente, creamos los datasets y el dataloader.\nLos DataLoaders en PyTorch son clases que facilitan la carga eficiente de conjuntos de datos para entrenar modelos de aprendizaje autom√°tico. Proporcionan funciones como la divisi√≥n en lotes, la mezcla de datos y la aplicaci√≥n de transformaciones, lo que simplifica el proceso de entrenamiento. Los DataLoaders permiten cargar y procesar los datos en paralelo, lo que acelera el entrenamiento y garantiza que los datos se utilicen de manera eficiente. Adem√°s, ofrecen una interfaz sencilla para iterar sobre los datos durante el entrenamiento del modelo. En resumen, los DataLoaders son una herramienta clave en PyTorch para cargar y preparar los datos de manera eficiente antes de entrenar un modelo de aprendizaje autom√°tico.\n\nif CFG.SPLIT_AUDIO:\n    train_dataset = BirdDataset('train_metadata.csv', CFG.CUT_AUDIOS_PATH, train = True, signal_augmentations = signal_augmentations, spec_augmentations= spec_augmentations)\n    val_dataset = BirdDataset('val_metadata.csv', CFG.CUT_AUDIOS_PATH, train = False)\nelse:\n    train_dataset = BirdDataset('train_metadata.csv', CFG.AUDIOS_PATH, train = True, signal_augmentations = signal_augmentations, spec_augmentations= spec_augmentations)\n    val_dataset = BirdDataset('val_metadata.csv', CFG.AUDIOS_PATH, train = False)\n\n\ndl_train = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE , shuffle=True, num_workers = 2)\ndl_val = DataLoader(val_dataset, batch_size=CFG.BATCH_SIZE , shuffle=False, num_workers = 2)\n\n\n\nShow Data\nVeamos como es la data generada\n\ndef show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n    fig = plt.figure(figsize=(12, 6))    \n    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n    for index, img_index in enumerate(img_index):  # list first 9 images\n        img, lb = img_ds[img_index]    \n        #img = img[0][0]\n        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n        if isinstance(img, torch.Tensor):\n            img = img.detach().numpy()\n        if isinstance(img, np.ndarray):\n            img = img.transpose(1, 2, 0)\n            ax.imshow(img)        \n        title = f\"Spec {CFG.label2name[lb.argmax().item()]}\"\n        ax.set_title(title)\n\n\nshow_batch(train_dataset, 8, 2, 4)\n\n\nshow_batch(val_dataset, 8, 2, 4)\n\n\n# Aunque son similares, vemos que no son los mismos! Entonces cada vez que dataloader extrea\n# El item, le entrega un sample distinto!"
  },
  {
    "objectID": "posts/birdclef2023/main.html#modeling",
    "href": "posts/birdclef2023/main.html#modeling",
    "title": "BirdClef 2023 Competition",
    "section": "Modeling !",
    "text": "Modeling !\nPara la fase de modelado, recordar que utilizamos Pytorch Lighting\n\nimport timm\nimport pytorch_lightning as pl\n\n\nOptimizer\nNormalmente, podemos utilizar los optimizadores que Pytorch trae por defecto, en este caso utilizamos Adam pero agregandole un schedular al learning rate.\n\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau, OneCycleLR\n\ndef get_optimizer(lr, params):\n    model_optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, params), \n            lr=lr,\n            weight_decay=CFG.WEIGHT_DECAY\n        )\n    interval = \"epoch\"\n    \n    lr_scheduler = CosineAnnealingWarmRestarts(\n                            model_optimizer, \n                            T_0=CFG.EPOCHS, \n                            T_mult=1, \n                            eta_min=1e-6, \n                            last_epoch=-1\n                        )\n\n    return {\n        \"optimizer\": model_optimizer, \n        \"lr_scheduler\": {\n            \"scheduler\": lr_scheduler,\n            \"interval\": interval,\n            \"monitor\": \"val_loss\",\n            \"frequency\": 1\n        }\n    }\n\n\n\nMetrics\nNormalmente las m√©tricas utilizadas en Kaggle son particulares, este caso no es distinto, es por esto que utilizamos funciones creadas por algunos participantes para poder ingresar en los distintos frameworks que utilizamos.\n\nimport sklearn.metrics\n\ndef padded_cmap(solution, submission, padding_factor=5):\n    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n    new_rows = []\n    for i in range(padding_factor):\n        new_rows.append([1 for i in range(len(solution.columns))])\n    new_rows = pd.DataFrame(new_rows)\n    new_rows.columns = solution.columns\n    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n    score = sklearn.metrics.average_precision_score(\n        padded_solution.values,\n        padded_submission.values,\n        average='macro',\n    )\n    return score\n\ndef map_score(solution, submission):\n    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n    score = sklearn.metrics.average_precision_score(\n        solution.values,\n        submission.values,\n        average='micro',\n    )\n    return score\n\n\n\nMix Up\nAprendiendo del desarrollo de otros Kagglers, encontr√© una t√©cnica bastante utilizada llamada Mixup. MixUp es una t√©cnica utilizada en el procesamiento de audio que combina dos o m√°s se√±ales de audio para crear una nueva se√±al. Consiste en mezclar las formas de onda de diferentes se√±ales con ponderaciones espec√≠ficas. El proceso implica tomar una combinaci√≥n lineal de las formas de onda originales y sus correspondientes etiquetas o caracter√≠sticas asociadas. El objetivo principal de MixUp es aumentar la diversidad y la cantidad de datos de entrenamiento al crear nuevas instancias de se√±ales de audio mediante la interpolaci√≥n entre muestras existentes. Esta t√©cnica promueve la generalizaci√≥n del modelo al exponerlo a diferentes combinaciones de se√±ales, mejorando as√≠ su capacidad para reconocer y clasificar diferentes patrones en los datos de audio. MixUp es particularmente √∫til en tareas de clasificaci√≥n de audio, como el reconocimiento de g√©nero o emociones, y puede ayudar a mejorar el rendimiento y la robustez de los modelos de aprendizaje autom√°tico en el procesamiento de audio.\n\n# Copied and edited from https://www.kaggle.com/code/riadalmadani/fastai-effb0-base-model-birdclef2023\ndef mixup(data, targets, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    new_data = data * lam + shuffled_data * (1 - lam)\n    new_targets = [targets, shuffled_targets, lam]\n    return new_data, new_targets\n\n\n# Copied and edited from https://www.kaggle.com/code/riadalmadani/fastai-effb0-base-model-birdclef2023\ndef mixup_criterion(criterion, preds, targets):\n    targets1, targets2, lam = targets[0], targets[1], targets[2]\n    #criterion = nn.CrossEntropyLoss()\n    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n\n\n\nModel\nPara el caso del modelo, es un poco similar a la creaci√≥n del Datasets, necesitamos tener algunos m√©todos particulares, en este caso tenemos:\n\nForward\nConfigure Optimizers\nTraining Step\nValidation Step\nValidation Epoch End\n\n\nclass BirdModel(pl.LightningModule):\n    def __init__(self, model_name = CFG.MODEL_NAME, num_classes=CFG.N_CLASSES):\n        \"\"\"\n        BirdModel class for audio classification using a pretrained model.\n\n        Args:\n        - model_name (str): Name of the pretrained model architecture.\n        - num_classes (int): Number of output classes.\n\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.model = timm.create_model(model_name, pretrained = True, num_classes = CFG.N_CLASSES, in_chans = CFG.CHANNELS)\n        self.loss_function = nn.CrossEntropyLoss()\n        self.batch_norm = nn.BatchNorm2d(num_features=CFG.CHANNELS)\n        \n        #self.backbone = timm.create_model(model_name, pretrained=True)\n        #self.in_features = self.backbone.classifier.in_features\n        #self.backbone.classifier = nn.Sequential(\n        #        nn.Linear(self.in_features, num_classes)\n        #        )\n        \n        #self.loss_function = nn.BCEWithLogitsLoss() \n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n        - x (torch.Tensor): Input tensor.\n\n        Returns:\n        - torch.Tensor: Output logits.\n\n        \"\"\"\n        x = self.batch_norm(x)\n        logits = self.model(x)\n        return logits\n    \n    def configure_optimizers(self):\n        \"\"\"\n        Configure the optimizer for training.\n\n        Returns:\n        - torch.optim.Optimizer: Optimizer object.\n\n        \"\"\"\n        return get_optimizer(lr=CFG.LR, params=self.parameters())\n        #optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer \n    \n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Training step.\n\n        Args:\n        - batch (tuple): Input batch (image, target).\n        - batch_idx (int): Batch index.\n\n        Returns:\n        - torch.Tensor: Loss value.\n\n        \"\"\"\n        image, target = batch     \n        \n        r_float = torch.rand([]).item()\n\n        \n        if CFG.MIXUP and CFG.P_MIXUP &lt; r_float:\n            image, targets = mixup(image, target, CFG.MIXUP_LAMBDA)\n            y_pred = self.forward(image)\n            loss = mixup_criterion(self.loss_function, y_pred, targets)\n        else:\n            y_pred = self.forward(image)\n            target = torch.argmax(target, dim=1)\n            loss = self.loss_function(y_pred,target)\n\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n        return loss  \n    \n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Validation step.\n\n        Args:\n        - batch (tuple): Input batch (image, target).\n        - batch_idx (int): Batch index.\n\n        Returns:\n        - dict: Dictionary containing validation loss, logits, and targets.\n\n        \"\"\"\n        image, target = batch    \n        return_target = target\n        target = torch.argmax(target, dim=1)\n        y_pred = self.forward(image)\n        loss = self.loss_function(y_pred,target)\n\n        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n        return {\"val_loss\": loss, \"logits\": y_pred, \"targets\": return_target}\n    \n    def validation_epoch_end(self, outputs):\n        \"\"\"\n        Validation epoch end.\n\n        Args:\n        - outputs (list): List of dictionaries containing validation loss, logits, and targets.\n\n        Returns:\n        - dict: Dictionary containing validation loss and C-MAP score.\n\n        \"\"\"\n        \n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        output_val = torch.cat([x['logits'] for x in outputs],dim=0).sigmoid().cpu().detach().numpy()\n        target_val = torch.cat([x['targets'] for x in outputs],dim=0).cpu().detach().numpy()\n        \n        # print(output_val.shape)\n        val_df = pd.DataFrame(target_val, columns = list(CFG.label2name.values()))\n        pred_df = pd.DataFrame(output_val, columns = list(CFG.label2name.values()))\n        \n        avg_score = padded_cmap(val_df, pred_df, padding_factor = 5)\n        ap_score = sklearn.metrics.label_ranking_average_precision_score(target_val,output_val)\n        \n#         competition_metrics(output_val,target_val)\n        print(f'epoch {self.current_epoch} validation loss {avg_loss}')\n        print(f'epoch {self.current_epoch} validation C-MAP score pad 5 {avg_score}')\n        print(f'epoch {self.current_epoch} validation AP score {ap_score}')\n        \n\n        self.log('val_map_score', avg_score)\n        self.log('val_ap_score', ap_score)\n        \n        return {'val_loss': avg_loss,'val_cmap':avg_score}\n        \n\n\n\nWANDB Logging\nPara el monitoreo del entrenamiento utilzamos Weight And Biases. Es importante se√±alar que esta herramienta sirve para mucho m√°s que el monitoreo del entrenamiento.\nWeight and Bias (W&B) es un framework utilizado para el seguimiento y la visualizaci√≥n de experimentos de aprendizaje autom√°tico. Proporciona herramientas para registrar y monitorear m√©tricas, hiperpar√°metros, gr√°ficos y visualizaciones en tiempo real durante el entrenamiento y evaluaci√≥n de modelos. W&B permite el registro de experimentos de manera f√°cil e integrada con diferentes bibliotecas y frameworks de aprendizaje autom√°tico. Adem√°s, ofrece caracter√≠sticas como la comparaci√≥n de experimentos, la colaboraci√≥n en equipo y la reproducci√≥n de resultados anteriores, lo que lo convierte en una herramienta valiosa para el desarrollo y la iteraci√≥n eficiente en proyectos de aprendizaje autom√°tico.\n\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\n\nPara utilizarlo debes ingresar tu KEY de WandB. En este caso la almacenamos en los secrets de Kaggle, pero puedes hacerlo de la forma que quieras.\n\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nsecret_label = \"WANDB_KEY\"\nkey = UserSecretsClient().get_secret(secret_label)\nwandb.login(key = key)\n\n\n\nTraining\nAhora s√≥lo queda entrenar!\nLa funci√≥n run_training() configura y ejecuta el proceso de entrenamiento para un modelo BirdModel. Comienza imprimiendo un mensaje que indica que se est√° ejecutando el entrenamiento. Luego, crea un objeto WandbLogger para registrar y visualizar el proceso de entrenamiento en la plataforma Weights and Biases.\nA continuaci√≥n, inicializa un objeto BirdModel, que es el modelo a ser entrenado.\nLa funci√≥n tambi√©n configura dos callbacks: EarlyStopping y ModelCheckpoint. El callback EarlyStopping monitorea la p√©rdida de validaci√≥n y detiene el entrenamiento si no hay mejoras durante un cierto n√∫mero de √©pocas, mientras que el callback ModelCheckpoint guarda el modelo con la menor p√©rdida de validaci√≥n.\nDespu√©s de configurar los callbacks necesarios, la funci√≥n crea un objeto Trainer del framework PyTorch Lightning. El Trainer se encarga de administrar el proceso de entrenamiento. Especifica el n√∫mero de √©pocas, el logger, los callbacks y otras configuraciones, como el uso de una GPU para aceleraci√≥n.\nFinalmente, se llama al m√©todo trainer.fit() para iniciar el proceso de entrenamiento real, utilizando audio_model como el modelo a entrenar y dl_train y dl_val como los dataloaders de entrenamiento y validaci√≥n, respectivamente. Una vez que se completa el entrenamiento, se libera la memoria llamando a gc.collect() y torch.cuda.empty_cache().\nEn resumen, esta funci√≥n encapsula el proceso de entrenamiento para un modelo BirdModel, incluyendo la inicializaci√≥n del modelo, el registro de datos, los callbacks y el bucle de entrenamiento utilizando el Trainer de PyTorch Lightning.\n\nimport gc\n\ndef run_training():\n    print(f\"Running training...\")\n    \n    wandb_logger = WandbLogger(project=\"birdclef23\", entity = 'diegulio', name = CFG.EXP_NAME)\n    \n    \n    audio_model = BirdModel()\n\n    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=3, verbose= True, mode=\"min\")\n    checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n                                          dirpath= \"/kaggle/working/exp1/\",\n                                      save_top_k=1,\n                                      save_last= True,\n                                      save_weights_only=True,\n                                      filename= f'./{CFG.MODEL_NAME}_loss',\n                                      verbose= True,\n                                      mode='min')\n    \n    callbacks_to_use = [checkpoint_callback,early_stop_callback]\n\n\n    trainer = pl.Trainer(\n        val_check_interval=CFG.VAL_CHECK_INTERVAL,\n        #deterministic=True,\n        max_epochs=CFG.EPOCHS,\n        logger=wandb_logger,\n        #auto_lr_find=True,    \n        callbacks=callbacks_to_use,\n        #precision=16, \n        accelerator=\"gpu\",\n        #num_nodes = 2,\n        strategy = 'ddp_notebook'\n        \n    )\n\n    print(\"Running trainer.fit\")\n    trainer.fit(audio_model, train_dataloaders = dl_train, val_dataloaders = dl_val)                \n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n\nrun_training()"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/main.html",
    "href": "posts/kaggle_nlp_disaster/main.html",
    "title": "Identificando desastres en Twitter con NLP",
    "section": "",
    "text": "Goal\nI‚Äôm learning NLP. So to do that I decided pass trough diverse NLP models, study the teory and code them! I think that it is a good method to learn Machine Learning things. So, Disclaimer: All the content on the notebooks is what I understood from diverse references (I will put the links), somethings could be wrong, If you find any mistake please let me know, so I can learn of it. Also, if you have some doubt, it will be a pleasure to me to answer it (as long as I have the answer).\nAt the end, I achieve a score of 0.843 in the LB. Is beatifull to see how you are improving the solutions step by step!\nSo, this will be the embedding Notebook, I will put the link to each specific notebook here (So it will be more readable).\nMethodologies & Notebooks:\n\n\n\nModel Notebook\nScore\n\n\n\n\nSimple Neural Network ( View on kaggle)\n0.56\n\n\nEmbeddings ( View on kaggle)\n0.797\n\n\nRecurrent Neural Networks ( View on kaggle)\n0.809\n\n\nBERT & HuggingFace ( View on kaggle)\n0.824\n\n\nMyBestSolution ( View on kaggle)\n0.843\n\n\n\n\n\nEDA\nHere I will do some preprocessing and split the data. I will use that data to each notebook!\nI think there is a lot of notebooks with a beatifull EDA, So I won‚Äôt take to much around this.\n\nLibraries\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\nData\n\ntrain_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntrain_df.head()\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n1\nNaN\nNaN\nOur Deeds are the Reason of this #earthquake M...\n1\n\n\n1\n4\nNaN\nNaN\nForest fire near La Ronge Sask. Canada\n1\n\n\n2\n5\nNaN\nNaN\nAll residents asked to 'shelter in place' are ...\n1\n\n\n3\n6\nNaN\nNaN\n13,000 people receive #wildfires evacuation or...\n1\n\n\n4\n7\nNaN\nNaN\nJust got sent this photo from Ruby #Alaska as ...\n1\n\n\n\n\n\n\n\n\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntest_df.head()\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n\n\n\n\n\n\n\n\n\nEDA\n\n# Target Proportion\nsns.countplot(data=train_df, x = \"target\")\n\n&lt;AxesSubplot:xlabel='target', ylabel='count'&gt;\n\n\n\n\n\nI think it is balanced!\n\n# Random example of disaster tweet\ntrain_df[train_df.target == 1].sample(1).text.values[0]\n\n'California Bush fires please evacuate affected areas ASAP when california govts advised you to do so http://t.co/ubVEVUuAch'\n\n\n\n# Random example of NO disaster tweet\ntrain_df[train_df.target == 0].sample(1).text.values[0]\n\n'Someone split a mudslide w me when I get off work'\n\n\n\n\nPre-processing\nI will do some preprocessing with Tensorflow!\n\n# Input Tensor Data\ntext = tf.data.Dataset.from_tensor_slices(train_df.text)\ntext\n\n2022-12-11 15:20:58.687517: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: (), types: tf.string&gt;\n\n\nNote that Im reading data from memory! If it would huge data I would be in troubles!\nOne advantage of initialize a Tensorflow dataset is that I will be able to create a data pipeline (batch, fetch, shuffle, etc.)\n\n# some samples\nlist(text.take(2).as_numpy_iterator())\n\n[b'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n b'Forest fire near La Ronge Sask. Canada']\n\n\nWe need to know that models don‚Äôt understand text by itself! Just numbers! For this, we vectorize the sentences. I don‚Äôt plan to use a model now, I would like to observe wich words are more present by target! (Also I don‚Äôt want to consider stopwords)\nI will use the tensorflow layer: Text Vectorization. from behind, it apply lowercase and delete punctuation. I also wants to remove stopwords, so I will build a custom standarization that do: 1. lowercase 2. strip punctuation 3. remove stop words!\nClick here if you don‚Äôt know what are stop words\n\n#### COUNT WORDS BY TARGET\n\ndef custom_standardization(inputs):\n    \"\"\"\n    Apply: lowercase, remove punctuation and stopwords\n    \"\"\"\n    PUNCTUATION = r'[!\"#$%&()\\*\\+,-\\./:;&lt;=&gt;?@\\[\\\\\\]^_`{|}~\\']'\n    lowercase = tf.strings.lower(inputs) # lowercase\n    strip = tf.strings.regex_replace(lowercase, PUNCTUATION, '') # strip punctuation\n    stopwrd = tf.strings.regex_replace(strip, r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*', '')\n    return stopwrd\n    \n\n# model to apply vectorize_layer with custom standardization\nvectorize_layer = tf.keras.layers.TextVectorization(output_mode = 'multi_hot', standardize = custom_standardization)\nvectorize_layer.adapt(text)\n\n# model to vectorize\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.Input(shape=(1,), dtype=tf.string))\nmodel.add(vectorize_layer)\n\n# make counter\ntrain_count = model.predict(text.batch(batch_size = len(text))) # predict to count \ntoken_counts = pd.DataFrame(columns = vectorize_layer.get_vocabulary(), data = train_count) # df with tokens and count\ntrain_df.rename(columns = {\"target\":\"disaster_target\"}, inplace = True) # rename target because there is a word target in data\ncount_df = pd.concat([train_df, token_counts], axis = 1) #concat\ngroup_count = count_df.iloc[:,4:].groupby(\"disaster_target\", as_index = False).sum() # count token for each target\nmelt_count = pd.melt(group_count, id_vars=[\"disaster_target\"], value_name = \"count\") # each token to row\nmelt_count.sort_values(by=[\"count\"], ascending = False).head(30)\n\n2022-12-11 15:20:58.997911: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: FutureWarning: This dataframe has a column name that matches the 'value_name' column name of the resulting Dataframe. In the future this will raise an error, please set the 'value_name' parameter of DataFrame.melt to a unique name.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\n\n\n\n\n\n\ndisaster_target\nvariable\ncount\n\n\n\n\n2\n0\nlike\n239.0\n\n\n4\n0\nim\n221.0\n\n\n6\n0\namp\n174.0\n\n\n12\n0\nnew\n163.0\n\n\n9\n1\nfire\n162.0\n\n\n10\n0\nget\n158.0\n\n\n22\n0\ndont\n136.0\n\n\n21\n1\nnews\n130.0\n\n\n18\n0\none\n122.0\n\n\n15\n1\nvia\n121.0\n\n\n42\n0\nbody\n110.0\n\n\n51\n1\ncalifornia\n108.0\n\n\n53\n1\nsuicide\n104.0\n\n\n17\n1\npeople\n101.0\n\n\n37\n1\npolice\n97.0\n\n\n35\n1\ndisaster\n96.0\n\n\n14\n0\nvia\n96.0\n\n\n7\n1\namp\n95.0\n\n\n38\n0\nwould\n93.0\n\n\n95\n1\nkilled\n90.0\n\n\n24\n0\nvideo\n90.0\n\n\n16\n0\npeople\n90.0\n\n\n3\n1\nlike\n88.0\n\n\n117\n1\nhiroshima\n84.0\n\n\n87\n1\nfires\n82.0\n\n\n62\n0\nknow\n82.0\n\n\n28\n0\n2\n81.0\n\n\n104\n0\nfull\n81.0\n\n\n84\n0\nlove\n81.0\n\n\n58\n0\ntime\n80.0\n\n\n\n\n\n\n\nAfter I build it I realized that it is not necessary to instantiate a model to use layers! üòÖ\n\n\n\nSplit Data\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(train_df[[col for col in train_df.columns if col != 'disaster_target']], train_df.disaster_target, test_size=0.2, random_state=13)\n\n\n# To csv (In some notebooks I will use this data)\npd.concat([X_train, y_train], axis = 1).to_csv('df_train.csv',index = False)\npd.concat([X_test, y_test], axis = 1).to_csv('df_test.csv',index = False)\n\nThis dataset is here: https://www.kaggle.com/datasets/diegomachado/df-split\n\n# Delete it from memory\ndel train_df, test_df, X_train, X_test, y_train, y_test\ngc.collect()\n\n671"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html",
    "title": "1. Simple Neural Network",
    "section": "",
    "text": "In this notebook I will use a simple Neural Network Arquitecture. We need to remember that we only can feed numbers to NN !\nThe Arquitecture will look like this:\n\n\n\nNLP-NN.png\n\n\nRemember that is belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\n\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-11 14:55:06.219408: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nInstantiate TextVectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 50 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-11 14:55:06.451771: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\n\n# Testing the Vectorization layer\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return vectorization_layer(text)\n\n# retrieve a batch (of 1 review and label) from the dataset\ntext_batch, label_batch = next(iter(train_ds))\nfirst_review, first_label = text_batch, label_batch\nprint(\"Review\", first_review)\nprint(\"Label\", first_label)\nprint(\"Vectorized review\", vectorize_text(first_review))\n\nReview tf.Tensor(b'Riot Kit Bah - part of the new concept Gear coming for Autumn/Winter\\n#menswear #fashion #urbanfashion\\xc2\\x89\\xc3\\x9b_ https://t.co/cCwzDTFbUS', shape=(), dtype=string)\nLabel tf.Tensor(0, shape=(), dtype=int64)\nVectorized review tf.Tensor(\n[[ 403 1278    1  572    6    2   44 3700    1  250   10    1    1  968\n  6559    1    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0]], shape=(1, 50), dtype=int64)\n\n\nüîç We see that the length of the vector is output_sequence_length = 250, and if the statement is not that long, it is paded with 0. We also note that no integer is greater than 10000, since we set this as our maximum vocabulary of tokens (vocabulary_size()). We also see that there are some integers = 1. This in the vocabulary corresponds to UNK, that is, unknown words (tokens), this is because our vocabulary is limited (TextVectorization chooses the 10000 most frequent ones).\nAn improvement can be to analyze the length of the dataset sentences and choose an output_sequence_lenght based on this (to avoid to much pad)\n\nvectorization_layer.vocabulary_size()\n\n10000\n\n\n\n# To see the vocabulary\nvectorization_layer.get_vocabulary()[:10]\n\n['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nWe will build a simple Sequential Model composed by two Dense Layers. I dont take to much time tunning it.\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string), # Input Layer\n    vectorization_layer, # Vectorization Layer\n    layers.Dense(64),\n    #layers.Dropout(0.1),\n    layers.Dense(32),\n    #layers.Dropout(0.1),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 50)                0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                3264      \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 5,377\nTrainable params: 5,377\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\nüîç Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\n\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs)\n\nEpoch 1/100\n191/191 [==============================] - 2s 4ms/step - loss: 76.1297 - binary_accuracy: 0.5123 - val_loss: 30.4719 - val_binary_accuracy: 0.5207\nEpoch 2/100\n191/191 [==============================] - 1s 3ms/step - loss: 37.2703 - binary_accuracy: 0.5167 - val_loss: 22.0519 - val_binary_accuracy: 0.5194\nEpoch 3/100\n191/191 [==============================] - 1s 3ms/step - loss: 34.7663 - binary_accuracy: 0.5118 - val_loss: 27.2791 - val_binary_accuracy: 0.5141\nEpoch 4/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.8605 - binary_accuracy: 0.5135 - val_loss: 26.1287 - val_binary_accuracy: 0.5253\nEpoch 5/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.4162 - binary_accuracy: 0.5123 - val_loss: 24.3497 - val_binary_accuracy: 0.5108\nEpoch 6/100\n191/191 [==============================] - 1s 3ms/step - loss: 26.5699 - binary_accuracy: 0.5084 - val_loss: 17.0479 - val_binary_accuracy: 0.5148\nEpoch 7/100\n191/191 [==============================] - 1s 3ms/step - loss: 24.1579 - binary_accuracy: 0.5136 - val_loss: 24.3595 - val_binary_accuracy: 0.5351\nEpoch 8/100\n191/191 [==============================] - 1s 3ms/step - loss: 22.3645 - binary_accuracy: 0.5110 - val_loss: 18.3034 - val_binary_accuracy: 0.5108\nEpoch 9/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.4826 - binary_accuracy: 0.5059 - val_loss: 17.5856 - val_binary_accuracy: 0.5161\nEpoch 10/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.1403 - binary_accuracy: 0.5156 - val_loss: 15.4736 - val_binary_accuracy: 0.5299\nEpoch 11/100\n191/191 [==============================] - 1s 3ms/step - loss: 16.6029 - binary_accuracy: 0.5156 - val_loss: 13.5105 - val_binary_accuracy: 0.5220\nEpoch 12/100\n191/191 [==============================] - 0s 3ms/step - loss: 16.8898 - binary_accuracy: 0.5166 - val_loss: 13.1816 - val_binary_accuracy: 0.5253\nEpoch 13/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.9538 - binary_accuracy: 0.5151 - val_loss: 14.4587 - val_binary_accuracy: 0.5391\nEpoch 14/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.0781 - binary_accuracy: 0.5085 - val_loss: 11.7277 - val_binary_accuracy: 0.5312\nEpoch 15/100\n191/191 [==============================] - 1s 3ms/step - loss: 13.2963 - binary_accuracy: 0.5072 - val_loss: 8.4516 - val_binary_accuracy: 0.5167\nEpoch 16/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.8151 - binary_accuracy: 0.5125 - val_loss: 13.2906 - val_binary_accuracy: 0.5351\nEpoch 17/100\n191/191 [==============================] - 1s 3ms/step - loss: 12.2239 - binary_accuracy: 0.5136 - val_loss: 10.8708 - val_binary_accuracy: 0.5378\nEpoch 18/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.7989 - binary_accuracy: 0.5046 - val_loss: 8.7974 - val_binary_accuracy: 0.5299\nEpoch 19/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5223 - binary_accuracy: 0.5103 - val_loss: 9.4426 - val_binary_accuracy: 0.5253\nEpoch 20/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5585 - binary_accuracy: 0.5122 - val_loss: 9.8687 - val_binary_accuracy: 0.5246\nEpoch 21/100\n191/191 [==============================] - 1s 3ms/step - loss: 9.2616 - binary_accuracy: 0.5167 - val_loss: 11.9928 - val_binary_accuracy: 0.5253\nEpoch 22/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.1322 - binary_accuracy: 0.5213 - val_loss: 7.8273 - val_binary_accuracy: 0.4990\nEpoch 23/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.4561 - binary_accuracy: 0.5200 - val_loss: 9.2781 - val_binary_accuracy: 0.5174\nEpoch 24/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.0963 - binary_accuracy: 0.5166 - val_loss: 5.4576 - val_binary_accuracy: 0.5167\nEpoch 25/100\n191/191 [==============================] - 1s 3ms/step - loss: 7.1907 - binary_accuracy: 0.5122 - val_loss: 8.8754 - val_binary_accuracy: 0.5003\nEpoch 26/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.9971 - binary_accuracy: 0.5190 - val_loss: 7.2822 - val_binary_accuracy: 0.5095\nEpoch 27/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.5726 - binary_accuracy: 0.5105 - val_loss: 5.3474 - val_binary_accuracy: 0.4879\nEpoch 28/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.0869 - binary_accuracy: 0.5243 - val_loss: 5.1194 - val_binary_accuracy: 0.5174\nEpoch 29/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.7851 - binary_accuracy: 0.5125 - val_loss: 6.9937 - val_binary_accuracy: 0.5076\nEpoch 30/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.1723 - binary_accuracy: 0.5148 - val_loss: 5.7589 - val_binary_accuracy: 0.5062\nEpoch 31/100\n191/191 [==============================] - 1s 4ms/step - loss: 5.1466 - binary_accuracy: 0.5125 - val_loss: 3.9673 - val_binary_accuracy: 0.5181\nEpoch 32/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.2948 - binary_accuracy: 0.5186 - val_loss: 5.4504 - val_binary_accuracy: 0.5089\nEpoch 33/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.4029 - binary_accuracy: 0.5095 - val_loss: 2.8821 - val_binary_accuracy: 0.5128\nEpoch 34/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.0808 - binary_accuracy: 0.5099 - val_loss: 3.8832 - val_binary_accuracy: 0.5161\nEpoch 35/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7501 - binary_accuracy: 0.5174 - val_loss: 3.9605 - val_binary_accuracy: 0.5135\nEpoch 36/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7382 - binary_accuracy: 0.5102 - val_loss: 3.6448 - val_binary_accuracy: 0.5115\nEpoch 37/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.1055 - binary_accuracy: 0.5245 - val_loss: 3.9398 - val_binary_accuracy: 0.5056\nEpoch 38/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.9880 - binary_accuracy: 0.5151 - val_loss: 2.8102 - val_binary_accuracy: 0.5082\nEpoch 39/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.7847 - binary_accuracy: 0.5215 - val_loss: 2.1289 - val_binary_accuracy: 0.5187\nEpoch 40/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.8415 - binary_accuracy: 0.5241 - val_loss: 2.8795 - val_binary_accuracy: 0.5115\nEpoch 41/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.5144 - binary_accuracy: 0.5223 - val_loss: 2.5149 - val_binary_accuracy: 0.5213\nEpoch 42/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0964 - binary_accuracy: 0.5287 - val_loss: 2.0305 - val_binary_accuracy: 0.5207\nEpoch 43/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.1527 - binary_accuracy: 0.5158 - val_loss: 1.8447 - val_binary_accuracy: 0.5095\nEpoch 44/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0041 - binary_accuracy: 0.5296 - val_loss: 1.7671 - val_binary_accuracy: 0.5095\nEpoch 45/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.7718 - binary_accuracy: 0.5232 - val_loss: 2.0167 - val_binary_accuracy: 0.5121\nEpoch 46/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.8157 - binary_accuracy: 0.5159 - val_loss: 1.7764 - val_binary_accuracy: 0.5227\nEpoch 47/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6907 - binary_accuracy: 0.5281 - val_loss: 1.3804 - val_binary_accuracy: 0.5233\nEpoch 48/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.5388 - binary_accuracy: 0.5233 - val_loss: 1.4022 - val_binary_accuracy: 0.5213\nEpoch 49/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6031 - binary_accuracy: 0.5259 - val_loss: 1.3513 - val_binary_accuracy: 0.5253\nEpoch 50/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.3865 - binary_accuracy: 0.5271 - val_loss: 1.1486 - val_binary_accuracy: 0.5246\nEpoch 51/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2101 - binary_accuracy: 0.5269 - val_loss: 1.0696 - val_binary_accuracy: 0.5227\nEpoch 52/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2028 - binary_accuracy: 0.5368 - val_loss: 1.1062 - val_binary_accuracy: 0.5292\nEpoch 53/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1968 - binary_accuracy: 0.5332 - val_loss: 0.9421 - val_binary_accuracy: 0.5318\nEpoch 54/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1506 - binary_accuracy: 0.5340 - val_loss: 1.0106 - val_binary_accuracy: 0.5338\nEpoch 55/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2173 - binary_accuracy: 0.5281 - val_loss: 0.9825 - val_binary_accuracy: 0.5378\nEpoch 56/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1079 - binary_accuracy: 0.5296 - val_loss: 0.8303 - val_binary_accuracy: 0.5515\nEpoch 57/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9980 - binary_accuracy: 0.5378 - val_loss: 0.8406 - val_binary_accuracy: 0.5456\nEpoch 58/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1121 - binary_accuracy: 0.5363 - val_loss: 1.8118 - val_binary_accuracy: 0.5548\nEpoch 59/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0127 - binary_accuracy: 0.5388 - val_loss: 0.9639 - val_binary_accuracy: 0.5483\nEpoch 60/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9924 - binary_accuracy: 0.5374 - val_loss: 0.8626 - val_binary_accuracy: 0.5483\nEpoch 61/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8680 - binary_accuracy: 0.5399 - val_loss: 0.8083 - val_binary_accuracy: 0.5542\nEpoch 62/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5440 - val_loss: 1.2377 - val_binary_accuracy: 0.5581\nEpoch 63/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9144 - binary_accuracy: 0.5396 - val_loss: 0.7873 - val_binary_accuracy: 0.5522\nEpoch 64/100\n191/191 [==============================] - 0s 3ms/step - loss: 0.9946 - binary_accuracy: 0.5343 - val_loss: 0.8474 - val_binary_accuracy: 0.5463\nEpoch 65/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9548 - binary_accuracy: 0.5365 - val_loss: 0.7671 - val_binary_accuracy: 0.5568\nEpoch 66/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9172 - binary_accuracy: 0.5432 - val_loss: 1.1670 - val_binary_accuracy: 0.5535\nEpoch 67/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9938 - binary_accuracy: 0.5473 - val_loss: 1.7050 - val_binary_accuracy: 0.5575\nEpoch 68/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8954 - binary_accuracy: 0.5394 - val_loss: 0.7633 - val_binary_accuracy: 0.5483\nEpoch 69/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8815 - binary_accuracy: 0.5460 - val_loss: 0.9965 - val_binary_accuracy: 0.5561\nEpoch 70/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5438 - val_loss: 1.0431 - val_binary_accuracy: 0.5509\nEpoch 71/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8825 - binary_accuracy: 0.5483 - val_loss: 1.0019 - val_binary_accuracy: 0.5601\nEpoch 72/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9897 - binary_accuracy: 0.5479 - val_loss: 1.3509 - val_binary_accuracy: 0.5502\nEpoch 73/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9775 - binary_accuracy: 0.5427 - val_loss: 0.9652 - val_binary_accuracy: 0.5581\nEpoch 74/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0205 - binary_accuracy: 0.5445 - val_loss: 1.2212 - val_binary_accuracy: 0.5614\nEpoch 75/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0616 - binary_accuracy: 0.5432 - val_loss: 1.1886 - val_binary_accuracy: 0.5575\nEpoch 76/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9314 - binary_accuracy: 0.5468 - val_loss: 1.3687 - val_binary_accuracy: 0.5548\nEpoch 77/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8521 - binary_accuracy: 0.5470 - val_loss: 0.7695 - val_binary_accuracy: 0.5548\nEpoch 78/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8151 - binary_accuracy: 0.5517 - val_loss: 0.9690 - val_binary_accuracy: 0.5509\nEpoch 79/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9553 - binary_accuracy: 0.5453 - val_loss: 0.8552 - val_binary_accuracy: 0.5575\nEpoch 80/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8296 - binary_accuracy: 0.5512 - val_loss: 0.7398 - val_binary_accuracy: 0.5555\nEpoch 81/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8471 - binary_accuracy: 0.5548 - val_loss: 0.7572 - val_binary_accuracy: 0.5529\nEpoch 82/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8430 - binary_accuracy: 0.5456 - val_loss: 0.7894 - val_binary_accuracy: 0.5575\nEpoch 83/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8508 - binary_accuracy: 0.5471 - val_loss: 0.9579 - val_binary_accuracy: 0.5476\nEpoch 84/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8445 - binary_accuracy: 0.5537 - val_loss: 0.8346 - val_binary_accuracy: 0.5581\nEpoch 85/100\n191/191 [==============================] - 1s 4ms/step - loss: 0.8542 - binary_accuracy: 0.5511 - val_loss: 0.7923 - val_binary_accuracy: 0.5542\nEpoch 86/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9084 - binary_accuracy: 0.5481 - val_loss: 0.9421 - val_binary_accuracy: 0.5575\nEpoch 87/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9367 - binary_accuracy: 0.5463 - val_loss: 0.9959 - val_binary_accuracy: 0.5450\nEpoch 88/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9629 - binary_accuracy: 0.5498 - val_loss: 1.1058 - val_binary_accuracy: 0.5607\nEpoch 89/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9141 - binary_accuracy: 0.5365 - val_loss: 0.7433 - val_binary_accuracy: 0.5561\nEpoch 90/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8145 - binary_accuracy: 0.5478 - val_loss: 0.7720 - val_binary_accuracy: 0.5529\nEpoch 91/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.7985 - binary_accuracy: 0.5560 - val_loss: 0.7784 - val_binary_accuracy: 0.5581\nEpoch 92/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8700 - binary_accuracy: 0.5468 - val_loss: 0.8171 - val_binary_accuracy: 0.5594\nEpoch 93/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8868 - binary_accuracy: 0.5502 - val_loss: 0.9612 - val_binary_accuracy: 0.5594\nEpoch 94/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8591 - binary_accuracy: 0.5493 - val_loss: 0.8276 - val_binary_accuracy: 0.5581\nEpoch 95/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8683 - binary_accuracy: 0.5435 - val_loss: 0.8003 - val_binary_accuracy: 0.5581\nEpoch 96/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8546 - binary_accuracy: 0.5415 - val_loss: 0.7477 - val_binary_accuracy: 0.5555\nEpoch 97/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9054 - binary_accuracy: 0.5484 - val_loss: 1.2473 - val_binary_accuracy: 0.5575\nEpoch 98/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8245 - binary_accuracy: 0.5479 - val_loss: 0.7795 - val_binary_accuracy: 0.5515\nEpoch 99/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8624 - binary_accuracy: 0.5435 - val_loss: 0.9183 - val_binary_accuracy: 0.5529\nEpoch 100/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8200 - binary_accuracy: 0.5470 - val_loss: 0.7893 - val_binary_accuracy: 0.5509\n\n\n\n\n\nNow we will predict the test to do the submission\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[-0.83098686],\n       [-0.71497   ],\n       [-0.4031287 ],\n       ...,\n       [-0.3488861 ],\n       [-0.4957207 ],\n       [-1.3229517 ]], dtype=float32)\n\n\nüìù Note that those are the logits!\nTo get a prediction we will compute the sigmoid function and round it to 1 or 0! (Thats because they are 2 classes, if there would be multi class classification then we would need Softmax Function)\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds)) # We use simple round function\ntest_df[\"target\"] = test_df[\"target\"].astype(int) # Submission needs int prediction\n\n\ntest_df.target.value_counts()\n\n0    3023\n1     240\nName: target, dtype: int64\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n0\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n9\n0\n\n\n4\n11\n0\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n0\n\n\n3260\n10868\n0\n\n\n3261\n10874\n0\n\n\n3262\n10875\n0\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\n#sub.to_csv(\"NN_submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#libraries",
    "href": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#libraries",
    "title": "1. Simple Neural Network",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#data",
    "href": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#data",
    "title": "1. Simple Neural Network",
    "section": "",
    "text": "# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-11 14:55:06.219408: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nInstantiate TextVectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 50 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-11 14:55:06.451771: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\n\n# Testing the Vectorization layer\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return vectorization_layer(text)\n\n# retrieve a batch (of 1 review and label) from the dataset\ntext_batch, label_batch = next(iter(train_ds))\nfirst_review, first_label = text_batch, label_batch\nprint(\"Review\", first_review)\nprint(\"Label\", first_label)\nprint(\"Vectorized review\", vectorize_text(first_review))\n\nReview tf.Tensor(b'Riot Kit Bah - part of the new concept Gear coming for Autumn/Winter\\n#menswear #fashion #urbanfashion\\xc2\\x89\\xc3\\x9b_ https://t.co/cCwzDTFbUS', shape=(), dtype=string)\nLabel tf.Tensor(0, shape=(), dtype=int64)\nVectorized review tf.Tensor(\n[[ 403 1278    1  572    6    2   44 3700    1  250   10    1    1  968\n  6559    1    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0]], shape=(1, 50), dtype=int64)\n\n\nüîç We see that the length of the vector is output_sequence_length = 250, and if the statement is not that long, it is paded with 0. We also note that no integer is greater than 10000, since we set this as our maximum vocabulary of tokens (vocabulary_size()). We also see that there are some integers = 1. This in the vocabulary corresponds to UNK, that is, unknown words (tokens), this is because our vocabulary is limited (TextVectorization chooses the 10000 most frequent ones).\nAn improvement can be to analyze the length of the dataset sentences and choose an output_sequence_lenght based on this (to avoid to much pad)\n\nvectorization_layer.vocabulary_size()\n\n10000\n\n\n\n# To see the vocabulary\nvectorization_layer.get_vocabulary()[:10]\n\n['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nWe will build a simple Sequential Model composed by two Dense Layers. I dont take to much time tunning it.\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string), # Input Layer\n    vectorization_layer, # Vectorization Layer\n    layers.Dense(64),\n    #layers.Dropout(0.1),\n    layers.Dense(32),\n    #layers.Dropout(0.1),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 50)                0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                3264      \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 5,377\nTrainable params: 5,377\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\nüîç Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\n\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs)\n\nEpoch 1/100\n191/191 [==============================] - 2s 4ms/step - loss: 76.1297 - binary_accuracy: 0.5123 - val_loss: 30.4719 - val_binary_accuracy: 0.5207\nEpoch 2/100\n191/191 [==============================] - 1s 3ms/step - loss: 37.2703 - binary_accuracy: 0.5167 - val_loss: 22.0519 - val_binary_accuracy: 0.5194\nEpoch 3/100\n191/191 [==============================] - 1s 3ms/step - loss: 34.7663 - binary_accuracy: 0.5118 - val_loss: 27.2791 - val_binary_accuracy: 0.5141\nEpoch 4/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.8605 - binary_accuracy: 0.5135 - val_loss: 26.1287 - val_binary_accuracy: 0.5253\nEpoch 5/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.4162 - binary_accuracy: 0.5123 - val_loss: 24.3497 - val_binary_accuracy: 0.5108\nEpoch 6/100\n191/191 [==============================] - 1s 3ms/step - loss: 26.5699 - binary_accuracy: 0.5084 - val_loss: 17.0479 - val_binary_accuracy: 0.5148\nEpoch 7/100\n191/191 [==============================] - 1s 3ms/step - loss: 24.1579 - binary_accuracy: 0.5136 - val_loss: 24.3595 - val_binary_accuracy: 0.5351\nEpoch 8/100\n191/191 [==============================] - 1s 3ms/step - loss: 22.3645 - binary_accuracy: 0.5110 - val_loss: 18.3034 - val_binary_accuracy: 0.5108\nEpoch 9/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.4826 - binary_accuracy: 0.5059 - val_loss: 17.5856 - val_binary_accuracy: 0.5161\nEpoch 10/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.1403 - binary_accuracy: 0.5156 - val_loss: 15.4736 - val_binary_accuracy: 0.5299\nEpoch 11/100\n191/191 [==============================] - 1s 3ms/step - loss: 16.6029 - binary_accuracy: 0.5156 - val_loss: 13.5105 - val_binary_accuracy: 0.5220\nEpoch 12/100\n191/191 [==============================] - 0s 3ms/step - loss: 16.8898 - binary_accuracy: 0.5166 - val_loss: 13.1816 - val_binary_accuracy: 0.5253\nEpoch 13/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.9538 - binary_accuracy: 0.5151 - val_loss: 14.4587 - val_binary_accuracy: 0.5391\nEpoch 14/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.0781 - binary_accuracy: 0.5085 - val_loss: 11.7277 - val_binary_accuracy: 0.5312\nEpoch 15/100\n191/191 [==============================] - 1s 3ms/step - loss: 13.2963 - binary_accuracy: 0.5072 - val_loss: 8.4516 - val_binary_accuracy: 0.5167\nEpoch 16/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.8151 - binary_accuracy: 0.5125 - val_loss: 13.2906 - val_binary_accuracy: 0.5351\nEpoch 17/100\n191/191 [==============================] - 1s 3ms/step - loss: 12.2239 - binary_accuracy: 0.5136 - val_loss: 10.8708 - val_binary_accuracy: 0.5378\nEpoch 18/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.7989 - binary_accuracy: 0.5046 - val_loss: 8.7974 - val_binary_accuracy: 0.5299\nEpoch 19/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5223 - binary_accuracy: 0.5103 - val_loss: 9.4426 - val_binary_accuracy: 0.5253\nEpoch 20/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5585 - binary_accuracy: 0.5122 - val_loss: 9.8687 - val_binary_accuracy: 0.5246\nEpoch 21/100\n191/191 [==============================] - 1s 3ms/step - loss: 9.2616 - binary_accuracy: 0.5167 - val_loss: 11.9928 - val_binary_accuracy: 0.5253\nEpoch 22/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.1322 - binary_accuracy: 0.5213 - val_loss: 7.8273 - val_binary_accuracy: 0.4990\nEpoch 23/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.4561 - binary_accuracy: 0.5200 - val_loss: 9.2781 - val_binary_accuracy: 0.5174\nEpoch 24/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.0963 - binary_accuracy: 0.5166 - val_loss: 5.4576 - val_binary_accuracy: 0.5167\nEpoch 25/100\n191/191 [==============================] - 1s 3ms/step - loss: 7.1907 - binary_accuracy: 0.5122 - val_loss: 8.8754 - val_binary_accuracy: 0.5003\nEpoch 26/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.9971 - binary_accuracy: 0.5190 - val_loss: 7.2822 - val_binary_accuracy: 0.5095\nEpoch 27/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.5726 - binary_accuracy: 0.5105 - val_loss: 5.3474 - val_binary_accuracy: 0.4879\nEpoch 28/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.0869 - binary_accuracy: 0.5243 - val_loss: 5.1194 - val_binary_accuracy: 0.5174\nEpoch 29/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.7851 - binary_accuracy: 0.5125 - val_loss: 6.9937 - val_binary_accuracy: 0.5076\nEpoch 30/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.1723 - binary_accuracy: 0.5148 - val_loss: 5.7589 - val_binary_accuracy: 0.5062\nEpoch 31/100\n191/191 [==============================] - 1s 4ms/step - loss: 5.1466 - binary_accuracy: 0.5125 - val_loss: 3.9673 - val_binary_accuracy: 0.5181\nEpoch 32/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.2948 - binary_accuracy: 0.5186 - val_loss: 5.4504 - val_binary_accuracy: 0.5089\nEpoch 33/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.4029 - binary_accuracy: 0.5095 - val_loss: 2.8821 - val_binary_accuracy: 0.5128\nEpoch 34/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.0808 - binary_accuracy: 0.5099 - val_loss: 3.8832 - val_binary_accuracy: 0.5161\nEpoch 35/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7501 - binary_accuracy: 0.5174 - val_loss: 3.9605 - val_binary_accuracy: 0.5135\nEpoch 36/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7382 - binary_accuracy: 0.5102 - val_loss: 3.6448 - val_binary_accuracy: 0.5115\nEpoch 37/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.1055 - binary_accuracy: 0.5245 - val_loss: 3.9398 - val_binary_accuracy: 0.5056\nEpoch 38/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.9880 - binary_accuracy: 0.5151 - val_loss: 2.8102 - val_binary_accuracy: 0.5082\nEpoch 39/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.7847 - binary_accuracy: 0.5215 - val_loss: 2.1289 - val_binary_accuracy: 0.5187\nEpoch 40/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.8415 - binary_accuracy: 0.5241 - val_loss: 2.8795 - val_binary_accuracy: 0.5115\nEpoch 41/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.5144 - binary_accuracy: 0.5223 - val_loss: 2.5149 - val_binary_accuracy: 0.5213\nEpoch 42/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0964 - binary_accuracy: 0.5287 - val_loss: 2.0305 - val_binary_accuracy: 0.5207\nEpoch 43/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.1527 - binary_accuracy: 0.5158 - val_loss: 1.8447 - val_binary_accuracy: 0.5095\nEpoch 44/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0041 - binary_accuracy: 0.5296 - val_loss: 1.7671 - val_binary_accuracy: 0.5095\nEpoch 45/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.7718 - binary_accuracy: 0.5232 - val_loss: 2.0167 - val_binary_accuracy: 0.5121\nEpoch 46/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.8157 - binary_accuracy: 0.5159 - val_loss: 1.7764 - val_binary_accuracy: 0.5227\nEpoch 47/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6907 - binary_accuracy: 0.5281 - val_loss: 1.3804 - val_binary_accuracy: 0.5233\nEpoch 48/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.5388 - binary_accuracy: 0.5233 - val_loss: 1.4022 - val_binary_accuracy: 0.5213\nEpoch 49/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6031 - binary_accuracy: 0.5259 - val_loss: 1.3513 - val_binary_accuracy: 0.5253\nEpoch 50/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.3865 - binary_accuracy: 0.5271 - val_loss: 1.1486 - val_binary_accuracy: 0.5246\nEpoch 51/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2101 - binary_accuracy: 0.5269 - val_loss: 1.0696 - val_binary_accuracy: 0.5227\nEpoch 52/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2028 - binary_accuracy: 0.5368 - val_loss: 1.1062 - val_binary_accuracy: 0.5292\nEpoch 53/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1968 - binary_accuracy: 0.5332 - val_loss: 0.9421 - val_binary_accuracy: 0.5318\nEpoch 54/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1506 - binary_accuracy: 0.5340 - val_loss: 1.0106 - val_binary_accuracy: 0.5338\nEpoch 55/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2173 - binary_accuracy: 0.5281 - val_loss: 0.9825 - val_binary_accuracy: 0.5378\nEpoch 56/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1079 - binary_accuracy: 0.5296 - val_loss: 0.8303 - val_binary_accuracy: 0.5515\nEpoch 57/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9980 - binary_accuracy: 0.5378 - val_loss: 0.8406 - val_binary_accuracy: 0.5456\nEpoch 58/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1121 - binary_accuracy: 0.5363 - val_loss: 1.8118 - val_binary_accuracy: 0.5548\nEpoch 59/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0127 - binary_accuracy: 0.5388 - val_loss: 0.9639 - val_binary_accuracy: 0.5483\nEpoch 60/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9924 - binary_accuracy: 0.5374 - val_loss: 0.8626 - val_binary_accuracy: 0.5483\nEpoch 61/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8680 - binary_accuracy: 0.5399 - val_loss: 0.8083 - val_binary_accuracy: 0.5542\nEpoch 62/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5440 - val_loss: 1.2377 - val_binary_accuracy: 0.5581\nEpoch 63/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9144 - binary_accuracy: 0.5396 - val_loss: 0.7873 - val_binary_accuracy: 0.5522\nEpoch 64/100\n191/191 [==============================] - 0s 3ms/step - loss: 0.9946 - binary_accuracy: 0.5343 - val_loss: 0.8474 - val_binary_accuracy: 0.5463\nEpoch 65/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9548 - binary_accuracy: 0.5365 - val_loss: 0.7671 - val_binary_accuracy: 0.5568\nEpoch 66/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9172 - binary_accuracy: 0.5432 - val_loss: 1.1670 - val_binary_accuracy: 0.5535\nEpoch 67/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9938 - binary_accuracy: 0.5473 - val_loss: 1.7050 - val_binary_accuracy: 0.5575\nEpoch 68/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8954 - binary_accuracy: 0.5394 - val_loss: 0.7633 - val_binary_accuracy: 0.5483\nEpoch 69/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8815 - binary_accuracy: 0.5460 - val_loss: 0.9965 - val_binary_accuracy: 0.5561\nEpoch 70/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5438 - val_loss: 1.0431 - val_binary_accuracy: 0.5509\nEpoch 71/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8825 - binary_accuracy: 0.5483 - val_loss: 1.0019 - val_binary_accuracy: 0.5601\nEpoch 72/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9897 - binary_accuracy: 0.5479 - val_loss: 1.3509 - val_binary_accuracy: 0.5502\nEpoch 73/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9775 - binary_accuracy: 0.5427 - val_loss: 0.9652 - val_binary_accuracy: 0.5581\nEpoch 74/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0205 - binary_accuracy: 0.5445 - val_loss: 1.2212 - val_binary_accuracy: 0.5614\nEpoch 75/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0616 - binary_accuracy: 0.5432 - val_loss: 1.1886 - val_binary_accuracy: 0.5575\nEpoch 76/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9314 - binary_accuracy: 0.5468 - val_loss: 1.3687 - val_binary_accuracy: 0.5548\nEpoch 77/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8521 - binary_accuracy: 0.5470 - val_loss: 0.7695 - val_binary_accuracy: 0.5548\nEpoch 78/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8151 - binary_accuracy: 0.5517 - val_loss: 0.9690 - val_binary_accuracy: 0.5509\nEpoch 79/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9553 - binary_accuracy: 0.5453 - val_loss: 0.8552 - val_binary_accuracy: 0.5575\nEpoch 80/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8296 - binary_accuracy: 0.5512 - val_loss: 0.7398 - val_binary_accuracy: 0.5555\nEpoch 81/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8471 - binary_accuracy: 0.5548 - val_loss: 0.7572 - val_binary_accuracy: 0.5529\nEpoch 82/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8430 - binary_accuracy: 0.5456 - val_loss: 0.7894 - val_binary_accuracy: 0.5575\nEpoch 83/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8508 - binary_accuracy: 0.5471 - val_loss: 0.9579 - val_binary_accuracy: 0.5476\nEpoch 84/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8445 - binary_accuracy: 0.5537 - val_loss: 0.8346 - val_binary_accuracy: 0.5581\nEpoch 85/100\n191/191 [==============================] - 1s 4ms/step - loss: 0.8542 - binary_accuracy: 0.5511 - val_loss: 0.7923 - val_binary_accuracy: 0.5542\nEpoch 86/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9084 - binary_accuracy: 0.5481 - val_loss: 0.9421 - val_binary_accuracy: 0.5575\nEpoch 87/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9367 - binary_accuracy: 0.5463 - val_loss: 0.9959 - val_binary_accuracy: 0.5450\nEpoch 88/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9629 - binary_accuracy: 0.5498 - val_loss: 1.1058 - val_binary_accuracy: 0.5607\nEpoch 89/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9141 - binary_accuracy: 0.5365 - val_loss: 0.7433 - val_binary_accuracy: 0.5561\nEpoch 90/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8145 - binary_accuracy: 0.5478 - val_loss: 0.7720 - val_binary_accuracy: 0.5529\nEpoch 91/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.7985 - binary_accuracy: 0.5560 - val_loss: 0.7784 - val_binary_accuracy: 0.5581\nEpoch 92/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8700 - binary_accuracy: 0.5468 - val_loss: 0.8171 - val_binary_accuracy: 0.5594\nEpoch 93/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8868 - binary_accuracy: 0.5502 - val_loss: 0.9612 - val_binary_accuracy: 0.5594\nEpoch 94/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8591 - binary_accuracy: 0.5493 - val_loss: 0.8276 - val_binary_accuracy: 0.5581\nEpoch 95/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8683 - binary_accuracy: 0.5435 - val_loss: 0.8003 - val_binary_accuracy: 0.5581\nEpoch 96/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8546 - binary_accuracy: 0.5415 - val_loss: 0.7477 - val_binary_accuracy: 0.5555\nEpoch 97/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9054 - binary_accuracy: 0.5484 - val_loss: 1.2473 - val_binary_accuracy: 0.5575\nEpoch 98/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8245 - binary_accuracy: 0.5479 - val_loss: 0.7795 - val_binary_accuracy: 0.5515\nEpoch 99/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8624 - binary_accuracy: 0.5435 - val_loss: 0.9183 - val_binary_accuracy: 0.5529\nEpoch 100/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8200 - binary_accuracy: 0.5470 - val_loss: 0.7893 - val_binary_accuracy: 0.5509\n\n\n\n\n\nNow we will predict the test to do the submission\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[-0.83098686],\n       [-0.71497   ],\n       [-0.4031287 ],\n       ...,\n       [-0.3488861 ],\n       [-0.4957207 ],\n       [-1.3229517 ]], dtype=float32)\n\n\nüìù Note that those are the logits!\nTo get a prediction we will compute the sigmoid function and round it to 1 or 0! (Thats because they are 2 classes, if there would be multi class classification then we would need Softmax Function)\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds)) # We use simple round function\ntest_df[\"target\"] = test_df[\"target\"].astype(int) # Submission needs int prediction\n\n\ntest_df.target.value_counts()\n\n0    3023\n1     240\nName: target, dtype: int64\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n0\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n9\n0\n\n\n4\n11\n0\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n0\n\n\n3260\n10868\n0\n\n\n3261\n10874\n0\n\n\n3262\n10875\n0\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\n#sub.to_csv(\"NN_submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "As we see in the previous notebooks. Embeddings solve the problem of have a good representation of text! But we still having some other problems:\nThe input of the Dense layers could vary in size! There are sequences of differents size! (Remember that we solve it with padding and truncation) Due to sequences could be large, there is a lot of computational costs! Layers are not sharing information! (They have different weighths) So we are not taking into account the order of the words, the context, or the words around To take care of this points, we will apply Recurrent Neural Networks in this notebook! üí•üí•üí•üí•\nI recommend spend time in understand what is happen inside RNN. For this I see a lot of videos and read some blogs. I will let you some of them here:\n\nhttps://www.youtube.com/watch?v=Y2wfIKQyd1I\nhttps://www.tensorflow.org/text/tutorials/text_classification_rnn\nhttps://www.youtube.com/watch?v=AsNTP8Kwu80\n\n\n\n\nNLP-Embedding-RNN.png\n\n\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\n\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 02:13:04.769598: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 40 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 02:13:04.955961: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nAs Twitter Embedding is the best so far. We will continue use it!\nIn this case we will use the SimpleRNN Tensorflow Layer. Furthermore, we also going to use the Bidirectional Layer. This is basically to apply two RNN, one that process the sequence from left to right, and another that process the sequence from right to left. It makes sense because we will know all the sequence input at the moment we want to predict. Also, I prove with and without Bidirectional, and with Bidirectional improve a lot wit respect without it. I think that in timeseries is not a good idea because we don‚Äôt know the future at the moment we want predict.\nClick here to read about Bidirectional RNN\nüëÄ Also, note that now we will define explicitly the activation functions in the NN. Also we will apply the sigmoid function at the end. So know the loss functions should has: from_logits=False or let it by default. (I just do it because I want to prove both ways)\n\n# GloVe Twitter Embedding\nwv = KeyedVectors.load_word2vec_format('../input/twitter-word2vecs-wordvecs-from-godin/word2vec_twitter_tokens.bin', \n                                       binary=True,\n                                       unicode_errors='ignore')\n\n\n# Build embedding matrix \n\nvoc = vectorization_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\n# We have to construct the embedding matrix with weigths from our own vocabulary\n# shape embedding matrix : (vocab_size, embedding_dim)\nnum_tokens = len(voc)\nembedding_dim = 400 # we download glove 100 dimension\nhits = []\nmisses = []\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    if word in wv:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_vector = wv[word]\n        embedding_matrix[i] = embedding_vector\n        hits.append(word)\n    else:\n        misses.append(word)\nprint(\"Converted %d words (%d misses)\" % (len(hits), len(misses)))\n\nConverted 7873 words (2127 misses)\n\n\n\n# Model \nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False\n    ),\n    layers.SpatialDropout1D(0.3),\n    layers.Bidirectional(layers.SimpleRNN(64, dropout = 0.2, recurrent_dropout = 0.2)),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(16, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 40)                0         \n_________________________________________________________________\nembedding (Embedding)        (None, 40, 400)           4000000   \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 40, 400)           0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128)               59520     \n_________________________________________________________________\ndense (Dense)                (None, 32)                4128      \n_________________________________________________________________\ndropout (Dropout)            (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                528       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 4,064,193\nTrainable params: 64,193\nNon-trainable params: 4,000,000\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback]\n)\n\nEpoch 1/100\n191/191 [==============================] - 16s 70ms/step - loss: 0.6465 - binary_accuracy: 0.6245 - val_loss: 0.5218 - val_binary_accuracy: 0.7610\nEpoch 2/100\n191/191 [==============================] - 13s 67ms/step - loss: 0.5322 - binary_accuracy: 0.7484 - val_loss: 0.4914 - val_binary_accuracy: 0.7722\nEpoch 3/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.5156 - binary_accuracy: 0.7675 - val_loss: 0.4749 - val_binary_accuracy: 0.7846\nEpoch 4/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.5046 - binary_accuracy: 0.7768 - val_loss: 0.4774 - val_binary_accuracy: 0.7912\nEpoch 5/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4972 - binary_accuracy: 0.7811 - val_loss: 0.4751 - val_binary_accuracy: 0.7827\nEpoch 6/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4820 - binary_accuracy: 0.7806 - val_loss: 0.4750 - val_binary_accuracy: 0.7774\nEpoch 7/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4786 - binary_accuracy: 0.7836 - val_loss: 0.4745 - val_binary_accuracy: 0.7859\nEpoch 8/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4677 - binary_accuracy: 0.7870 - val_loss: 0.4880 - val_binary_accuracy: 0.7748\nEpoch 9/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4674 - binary_accuracy: 0.7885 - val_loss: 0.4692 - val_binary_accuracy: 0.7859\nEpoch 10/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4619 - binary_accuracy: 0.7941 - val_loss: 0.4784 - val_binary_accuracy: 0.7761\nEpoch 11/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4604 - binary_accuracy: 0.7957 - val_loss: 0.4947 - val_binary_accuracy: 0.7715\nEpoch 12/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4576 - binary_accuracy: 0.7995 - val_loss: 0.4750 - val_binary_accuracy: 0.7827\nEpoch 13/100\n191/191 [==============================] - 13s 68ms/step - loss: 0.4503 - binary_accuracy: 0.7972 - val_loss: 0.4668 - val_binary_accuracy: 0.7925\nEpoch 14/100\n191/191 [==============================] - 13s 71ms/step - loss: 0.4568 - binary_accuracy: 0.7979 - val_loss: 0.4693 - val_binary_accuracy: 0.7794\nEpoch 15/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4446 - binary_accuracy: 0.8015 - val_loss: 0.4820 - val_binary_accuracy: 0.7807\nEpoch 16/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4411 - binary_accuracy: 0.8048 - val_loss: 0.4815 - val_binary_accuracy: 0.7676\nEpoch 17/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4406 - binary_accuracy: 0.8061 - val_loss: 0.4686 - val_binary_accuracy: 0.7840\nEpoch 18/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4396 - binary_accuracy: 0.8003 - val_loss: 0.4630 - val_binary_accuracy: 0.7912\nEpoch 19/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4364 - binary_accuracy: 0.8080 - val_loss: 0.4975 - val_binary_accuracy: 0.7761\nEpoch 20/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4346 - binary_accuracy: 0.8025 - val_loss: 0.4834 - val_binary_accuracy: 0.7820\nEpoch 21/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4325 - binary_accuracy: 0.8026 - val_loss: 0.4844 - val_binary_accuracy: 0.7741\nEpoch 22/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4780 - binary_accuracy: 0.7767 - val_loss: 0.4947 - val_binary_accuracy: 0.7702\nEpoch 23/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4338 - binary_accuracy: 0.8046 - val_loss: 0.4666 - val_binary_accuracy: 0.7873\n\n\n\n\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"RNN_submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\n\n\n\nRNN has some benefits. But also some disadvantages (See the references). One of them, maybe the most named is the Exploding-Vanishing gradient problem!\n\nTo improve our solution, maybe we could try adding more RNN layers. Note that for this we need to set up return_sequences = True in the previous LSTM layers. Thats because by default the output of a RNN or LSTM layer is the last hidden state, but to feed to another LSTM we need a sequence (and this sequence is given thanks to return_sequences = True). And of course we could try hyperparameter optimization\n\nSo there are other arquitectures that take care of it. We will explore those now:\n\nLSTM\nGRU"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#libraries",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#libraries",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 02:13:04.769598: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 40 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 02:13:04.955961: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data-pipeline",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data-pipeline",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "Now we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#model",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#model",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "As Twitter Embedding is the best so far. We will continue use it!\nIn this case we will use the SimpleRNN Tensorflow Layer. Furthermore, we also going to use the Bidirectional Layer. This is basically to apply two RNN, one that process the sequence from left to right, and another that process the sequence from right to left. It makes sense because we will know all the sequence input at the moment we want to predict. Also, I prove with and without Bidirectional, and with Bidirectional improve a lot wit respect without it. I think that in timeseries is not a good idea because we don‚Äôt know the future at the moment we want predict.\nClick here to read about Bidirectional RNN\nüëÄ Also, note that now we will define explicitly the activation functions in the NN. Also we will apply the sigmoid function at the end. So know the loss functions should has: from_logits=False or let it by default. (I just do it because I want to prove both ways)\n\n# GloVe Twitter Embedding\nwv = KeyedVectors.load_word2vec_format('../input/twitter-word2vecs-wordvecs-from-godin/word2vec_twitter_tokens.bin', \n                                       binary=True,\n                                       unicode_errors='ignore')\n\n\n# Build embedding matrix \n\nvoc = vectorization_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\n# We have to construct the embedding matrix with weigths from our own vocabulary\n# shape embedding matrix : (vocab_size, embedding_dim)\nnum_tokens = len(voc)\nembedding_dim = 400 # we download glove 100 dimension\nhits = []\nmisses = []\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    if word in wv:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_vector = wv[word]\n        embedding_matrix[i] = embedding_vector\n        hits.append(word)\n    else:\n        misses.append(word)\nprint(\"Converted %d words (%d misses)\" % (len(hits), len(misses)))\n\nConverted 7873 words (2127 misses)\n\n\n\n# Model \nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False\n    ),\n    layers.SpatialDropout1D(0.3),\n    layers.Bidirectional(layers.SimpleRNN(64, dropout = 0.2, recurrent_dropout = 0.2)),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(16, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 40)                0         \n_________________________________________________________________\nembedding (Embedding)        (None, 40, 400)           4000000   \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 40, 400)           0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128)               59520     \n_________________________________________________________________\ndense (Dense)                (None, 32)                4128      \n_________________________________________________________________\ndropout (Dropout)            (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                528       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 4,064,193\nTrainable params: 64,193\nNon-trainable params: 4,000,000\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback]\n)\n\nEpoch 1/100\n191/191 [==============================] - 16s 70ms/step - loss: 0.6465 - binary_accuracy: 0.6245 - val_loss: 0.5218 - val_binary_accuracy: 0.7610\nEpoch 2/100\n191/191 [==============================] - 13s 67ms/step - loss: 0.5322 - binary_accuracy: 0.7484 - val_loss: 0.4914 - val_binary_accuracy: 0.7722\nEpoch 3/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.5156 - binary_accuracy: 0.7675 - val_loss: 0.4749 - val_binary_accuracy: 0.7846\nEpoch 4/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.5046 - binary_accuracy: 0.7768 - val_loss: 0.4774 - val_binary_accuracy: 0.7912\nEpoch 5/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4972 - binary_accuracy: 0.7811 - val_loss: 0.4751 - val_binary_accuracy: 0.7827\nEpoch 6/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4820 - binary_accuracy: 0.7806 - val_loss: 0.4750 - val_binary_accuracy: 0.7774\nEpoch 7/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4786 - binary_accuracy: 0.7836 - val_loss: 0.4745 - val_binary_accuracy: 0.7859\nEpoch 8/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4677 - binary_accuracy: 0.7870 - val_loss: 0.4880 - val_binary_accuracy: 0.7748\nEpoch 9/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4674 - binary_accuracy: 0.7885 - val_loss: 0.4692 - val_binary_accuracy: 0.7859\nEpoch 10/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4619 - binary_accuracy: 0.7941 - val_loss: 0.4784 - val_binary_accuracy: 0.7761\nEpoch 11/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4604 - binary_accuracy: 0.7957 - val_loss: 0.4947 - val_binary_accuracy: 0.7715\nEpoch 12/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4576 - binary_accuracy: 0.7995 - val_loss: 0.4750 - val_binary_accuracy: 0.7827\nEpoch 13/100\n191/191 [==============================] - 13s 68ms/step - loss: 0.4503 - binary_accuracy: 0.7972 - val_loss: 0.4668 - val_binary_accuracy: 0.7925\nEpoch 14/100\n191/191 [==============================] - 13s 71ms/step - loss: 0.4568 - binary_accuracy: 0.7979 - val_loss: 0.4693 - val_binary_accuracy: 0.7794\nEpoch 15/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4446 - binary_accuracy: 0.8015 - val_loss: 0.4820 - val_binary_accuracy: 0.7807\nEpoch 16/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4411 - binary_accuracy: 0.8048 - val_loss: 0.4815 - val_binary_accuracy: 0.7676\nEpoch 17/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4406 - binary_accuracy: 0.8061 - val_loss: 0.4686 - val_binary_accuracy: 0.7840\nEpoch 18/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4396 - binary_accuracy: 0.8003 - val_loss: 0.4630 - val_binary_accuracy: 0.7912\nEpoch 19/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4364 - binary_accuracy: 0.8080 - val_loss: 0.4975 - val_binary_accuracy: 0.7761\nEpoch 20/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4346 - binary_accuracy: 0.8025 - val_loss: 0.4834 - val_binary_accuracy: 0.7820\nEpoch 21/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4325 - binary_accuracy: 0.8026 - val_loss: 0.4844 - val_binary_accuracy: 0.7741\nEpoch 22/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4780 - binary_accuracy: 0.7767 - val_loss: 0.4947 - val_binary_accuracy: 0.7702\nEpoch 23/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4338 - binary_accuracy: 0.8046 - val_loss: 0.4666 - val_binary_accuracy: 0.7873"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"RNN_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "RNN has some benefits. But also some disadvantages (See the references). One of them, maybe the most named is the Exploding-Vanishing gradient problem!\n\nTo improve our solution, maybe we could try adding more RNN layers. Note that for this we need to set up return_sequences = True in the previous LSTM layers. Thats because by default the output of a RNN or LSTM layer is the last hidden state, but to feed to another LSTM we need a sequence (and this sequence is given thanks to return_sequences = True). And of course we could try hyperparameter optimization\n\nSo there are other arquitectures that take care of it. We will explore those now:\n\nLSTM\nGRU"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-1",
    "title": "3. Recurrent Neural Networks",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds.shape\n\n(3263, 1)\n\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"LSTM_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations-1",
    "title": "3. Recurrent Neural Networks",
    "section": "Observations:",
    "text": "Observations:\nNice, we find our better solution so far!\n\nTo improve our solution, maybe we could try adding more LSTM. Note that for this we need to set up return_sequences = True in the previous LSTM layers. Thats because by default the output of a RNN or LSTM layer is the last hidden state, but to feed to another LSTM we need a sequence (and this sequence is given thanks to return_sequences = True). And of course we could try hyperparameter optimization\n\nNow we will try GRU Recurrent Neural Network"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-2",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-2",
    "title": "3. Recurrent Neural Networks",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\n#sub.to_csv(\"GRU_submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nA little better than LSTM!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html",
    "title": "5. My best solution",
    "section": "",
    "text": "In this notebook I will use all the knowledge that we acquiring with the previous notebooks!\nIn this solution I will be using:\n\nTransformers\nHuggingFace\nPreprocessing\nTensorflow\nEarlyStopping\n\nand more..\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\n\n# A dependency of the preprocessing for BERT inputs\n!pip install -q -U \"tensorflow-text==2.8.*\"\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;3,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;2.10,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow&lt;3,&gt;=2.9.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow&lt;2.7.0,&gt;=2.6.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.28.0 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard&gt;=2.9.1, but you have tensorboard 2.8.0 which is incompatible.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\n\n\n\n# Try with a large model\nmodel_name = \"bert-large-uncased\"\n\n\nfrom transformers import AutoTokenizer\n\n# properly tokenization\ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\ntokenizer_max_length = 161\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"], truncation=True, padding=True, max_length=tokenizer_max_length)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI will try a preprocessing that I find here: (I lost the notebook! so sorry, please if someone find it let me know in the comments!)\n\n# Some preprocess \ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\n\n def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\nimport spacy\nimport re\nnlp = spacy.load('en_core_web_sm')\ndef preprocessing(text):\n    text = text.replace('#','')\n    text = decontracted(text)\n    text = re.sub('\\S*@\\S*\\s?','',text)\n    text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n    #token=[]\n    #result=''\n    #text = re.sub('[^A-z]', ' ',text.lower())\n\n    #text = nlp(text)\n    #for t in text:\n    #    if not t.is_stop and len(t)&gt;2:  \n    #        token.append(t.lemma_)\n    #result = ' '.join([i for i in token])\n\n    return text.strip()\n\n\ntrain.text = train.text.apply(lambda x : preprocessing(x)).astype(str)\ntest.text = test.text.apply(lambda x : preprocessing(x)).astype(str)\n\n\n# Save processed data to disk\nNEW_TRAIN_PATH = \"preprocessed_train.csv\"\nNEW_TEST_PATH = \"preprocessed_test.csv\"\n\ntrain.to_csv(NEW_TRAIN_PATH, index = False)\ntest.to_csv(NEW_TEST_PATH, index = False)\n\ndel train\ndel test\ngc.collect()\n\n856\n\n\n\n\n\n\n# Now We can use HF Datasets\n# We need to load our data, for this we use HF datasets\nfrom datasets import load_dataset\ndata_files = {\"train\": NEW_TRAIN_PATH,\n             \"test\": NEW_TEST_PATH}\ndataset = load_dataset(\"csv\", data_files = data_files, usecols = ['text', 'target'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n# Tokenization\ndataset = dataset.map(tokenize_dataset)\n\n\n\n\n\n\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\n# we transform the HF dataset into TF dataset\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = train_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True)\n\ntf_val_dataset = test_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False\n    )\n\n\n\n\nNow we will use restore_best_weights from EarlyStopping\n\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(6e-6),metrics = ['accuracy'])\n\n#checkpoint_filepath = 'checkpoint'\n#checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nearly_stop_callback = EarlyStopping(patience = 5, restore_best_weights = True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n#                              patience=2, min_lr=1e-9)\n\nmodel.fit(tf_train_dataset,\n         validation_data = tf_val_dataset,\n          epochs = 20,\n          callbacks = [early_stop_callback]\n         )\n\n\n\n\n2022-12-12 14:19:24.079862: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 125018112 exceeds 10% of free system memory.\n2022-12-12 14:19:24.239512: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.254974: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.272507: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.294475: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\nAll model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nEpoch 1/20\n380/380 [==============================] - 164s 344ms/step - loss: 0.4792 - accuracy: 0.7821 - val_loss: 0.4102 - val_accuracy: 0.8286\nEpoch 2/20\n380/380 [==============================] - 123s 325ms/step - loss: 0.3694 - accuracy: 0.8528 - val_loss: 0.3909 - val_accuracy: 0.8339\nEpoch 3/20\n380/380 [==============================] - 122s 321ms/step - loss: 0.3145 - accuracy: 0.8832 - val_loss: 0.4103 - val_accuracy: 0.8365\nEpoch 4/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2622 - accuracy: 0.9035 - val_loss: 0.4222 - val_accuracy: 0.8391\nEpoch 5/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2201 - accuracy: 0.9211 - val_loss: 0.4777 - val_accuracy: 0.8201\nEpoch 6/20\n380/380 [==============================] - 122s 320ms/step - loss: 0.1804 - accuracy: 0.9327 - val_loss: 0.4962 - val_accuracy: 0.8293\nEpoch 7/20\n380/380 [==============================] - 123s 323ms/step - loss: 0.1509 - accuracy: 0.9413 - val_loss: 0.5522 - val_accuracy: 0.8201\n\n\n&lt;keras.callbacks.History at 0x7fefa3775050&gt;\n\n\n\n\n\n\npred_dataset = load_dataset(\"csv\", data_files = \"/kaggle/input/nlp-getting-started/test.csv\", usecols = ['text'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\npred_dataset = pred_dataset.map(tokenize_dataset)\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_pred_dataset = pred_dataset['train'].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False)\n\n\npreds = model.predict(tf_pred_dataset)\n\n\npreds = np.argmax(preds['logits'], axis = 1)\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_df[\"target\"] = preds\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#libraries",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#libraries",
    "title": "5. My best solution",
    "section": "",
    "text": "# A dependency of the preprocessing for BERT inputs\n!pip install -q -U \"tensorflow-text==2.8.*\"\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;3,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;2.10,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow&lt;3,&gt;=2.9.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow&lt;2.7.0,&gt;=2.6.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.28.0 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard&gt;=2.9.1, but you have tensorboard 2.8.0 which is incompatible.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#huggingface-model",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#huggingface-model",
    "title": "5. My best solution",
    "section": "",
    "text": "# Try with a large model\nmodel_name = \"bert-large-uncased\"\n\n\nfrom transformers import AutoTokenizer\n\n# properly tokenization\ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\ntokenizer_max_length = 161\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"], truncation=True, padding=True, max_length=tokenizer_max_length)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#data-pre-processing",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#data-pre-processing",
    "title": "5. My best solution",
    "section": "",
    "text": "I will try a preprocessing that I find here: (I lost the notebook! so sorry, please if someone find it let me know in the comments!)\n\n# Some preprocess \ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\n\n def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\nimport spacy\nimport re\nnlp = spacy.load('en_core_web_sm')\ndef preprocessing(text):\n    text = text.replace('#','')\n    text = decontracted(text)\n    text = re.sub('\\S*@\\S*\\s?','',text)\n    text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n    #token=[]\n    #result=''\n    #text = re.sub('[^A-z]', ' ',text.lower())\n\n    #text = nlp(text)\n    #for t in text:\n    #    if not t.is_stop and len(t)&gt;2:  \n    #        token.append(t.lemma_)\n    #result = ' '.join([i for i in token])\n\n    return text.strip()\n\n\ntrain.text = train.text.apply(lambda x : preprocessing(x)).astype(str)\ntest.text = test.text.apply(lambda x : preprocessing(x)).astype(str)\n\n\n# Save processed data to disk\nNEW_TRAIN_PATH = \"preprocessed_train.csv\"\nNEW_TEST_PATH = \"preprocessed_test.csv\"\n\ntrain.to_csv(NEW_TRAIN_PATH, index = False)\ntest.to_csv(NEW_TEST_PATH, index = False)\n\ndel train\ndel test\ngc.collect()\n\n856"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#hf-dataset",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#hf-dataset",
    "title": "5. My best solution",
    "section": "",
    "text": "# Now We can use HF Datasets\n# We need to load our data, for this we use HF datasets\nfrom datasets import load_dataset\ndata_files = {\"train\": NEW_TRAIN_PATH,\n             \"test\": NEW_TEST_PATH}\ndataset = load_dataset(\"csv\", data_files = data_files, usecols = ['text', 'target'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n# Tokenization\ndataset = dataset.map(tokenize_dataset)\n\n\n\n\n\n\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\n# we transform the HF dataset into TF dataset\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = train_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True)\n\ntf_val_dataset = test_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False\n    )"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#model",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#model",
    "title": "5. My best solution",
    "section": "",
    "text": "Now we will use restore_best_weights from EarlyStopping\n\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(6e-6),metrics = ['accuracy'])\n\n#checkpoint_filepath = 'checkpoint'\n#checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nearly_stop_callback = EarlyStopping(patience = 5, restore_best_weights = True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n#                              patience=2, min_lr=1e-9)\n\nmodel.fit(tf_train_dataset,\n         validation_data = tf_val_dataset,\n          epochs = 20,\n          callbacks = [early_stop_callback]\n         )\n\n\n\n\n2022-12-12 14:19:24.079862: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 125018112 exceeds 10% of free system memory.\n2022-12-12 14:19:24.239512: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.254974: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.272507: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.294475: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\nAll model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nEpoch 1/20\n380/380 [==============================] - 164s 344ms/step - loss: 0.4792 - accuracy: 0.7821 - val_loss: 0.4102 - val_accuracy: 0.8286\nEpoch 2/20\n380/380 [==============================] - 123s 325ms/step - loss: 0.3694 - accuracy: 0.8528 - val_loss: 0.3909 - val_accuracy: 0.8339\nEpoch 3/20\n380/380 [==============================] - 122s 321ms/step - loss: 0.3145 - accuracy: 0.8832 - val_loss: 0.4103 - val_accuracy: 0.8365\nEpoch 4/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2622 - accuracy: 0.9035 - val_loss: 0.4222 - val_accuracy: 0.8391\nEpoch 5/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2201 - accuracy: 0.9211 - val_loss: 0.4777 - val_accuracy: 0.8201\nEpoch 6/20\n380/380 [==============================] - 122s 320ms/step - loss: 0.1804 - accuracy: 0.9327 - val_loss: 0.4962 - val_accuracy: 0.8293\nEpoch 7/20\n380/380 [==============================] - 123s 323ms/step - loss: 0.1509 - accuracy: 0.9413 - val_loss: 0.5522 - val_accuracy: 0.8201\n\n\n&lt;keras.callbacks.History at 0x7fefa3775050&gt;"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#predict-test",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#predict-test",
    "title": "5. My best solution",
    "section": "",
    "text": "pred_dataset = load_dataset(\"csv\", data_files = \"/kaggle/input/nlp-getting-started/test.csv\", usecols = ['text'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\npred_dataset = pred_dataset.map(tokenize_dataset)\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_pred_dataset = pred_dataset['train'].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False)\n\n\npreds = model.predict(tf_pred_dataset)\n\n\npreds = np.argmax(preds['logits'], axis = 1)\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_df[\"target\"] = preds\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html",
    "title": "4. BERT",
    "section": "",
    "text": "As we see in the previous notebook, RNN improve the results and give us some benefits. But they are expensive to train, mainly because they are not paralellizable! Now we are in the transformers era. Transformers are models that leverage the mechanism of self-attention.\nIn this specific case we will apply BERT. I will let you with some references that help me to understand what is happend behind scenes:\n\nhttp://nlp.seas.harvard.edu/annotated-transformer/\nhttp://jalammar.github.io/illustrated-transformer/\nhttps://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\nhttps://www.youtube.com/watch?v=xI0HHN5XKDo&t=2s&pp=ugMICgJlcxABGAE%3D\nhttps://www.youtube.com/watch?v=7kLi8u2dJz0\nhttps://www.youtube.com/watch?v=TQQlZhbC5ps&list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE&index=1\n\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#hf-pipelines---zero-shot-classification",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#hf-pipelines---zero-shot-classification",
    "title": "4. BERT",
    "section": "HF ü§ó Pipelines - Zero Shot Classification",
    "text": "HF ü§ó Pipelines - Zero Shot Classification\nAs you can see in the hf documentation. HuggingFace has pipelines, where we can develop a variety of nlp (and more) applications! (Text generation, classification, summarization, etc)\nNow we will do Zero shot classification. That means that we will predict classes that wasn‚Äôt observe during traning. It is a difficult task\n\nclassifier = pipeline(\"zero-shot-classification\",\n                      model=\"facebook/bart-large-mnli\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# predict\ncandidate_labels = ['disaster']\ntest_df['target'] = [x['scores'][0] for x in classifier(test_df.text.to_list(), candidate_labels)]\n\n\ntest_df[\"target\"] = tf.round(test_df.target)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"HF_ZS_Submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nMake sense because is a zero shot classification!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#fine-tune",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#fine-tune",
    "title": "4. BERT",
    "section": "Fine-Tune",
    "text": "Fine-Tune\nNow we will download a model from HuggingFace models and finetune with our data\n\n# Model name to fine tune\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n\nfrom transformers import AutoTokenizer\n\n# First we need to use the tokenizer (Is similar we did in the Tensorflow hub with the preprocessing layer)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"])\n\n\n\n\n\n\n\n\n\n\n\n# We need to load our data, for this we use HF datasets (read directly from the disk, so save us ram)\nfrom datasets import load_dataset\ndata_files = {\"train\": \"/kaggle/input/df-split/df_split/df_train.csv\",\n             \"test\":\"/kaggle/input/df-split/df_split/df_test.csv\"}\ndataset = load_dataset(\"csv\", data_files = data_files, usecols = ['text', 'target'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1860883215d5579d/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1860883215d5579d/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'target'],\n        num_rows: 6090\n    })\n    test: Dataset({\n        features: ['text', 'target'],\n        num_rows: 1523\n    })\n})\n\n\n\n# preprocess\ndataset = dataset.map(tokenize_dataset)\n\n\n\n\n\n\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\n# we transform the HF dataset into TF dataset to fine tune\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = train_dataset.to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True)\n\ntf_val_dataset = test_dataset.to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True\n    )"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#model-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#model-1",
    "title": "4. BERT",
    "section": "Model",
    "text": "Model\n\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(3e-5),metrics = ['accuracy'])\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nmodel.fit(tf_train_dataset,\n         validation_data = tf_val_dataset,\n          epochs = 20,\n          callbacks = [early_stop_callback]\n         )\n\n\n\n\n2022-12-12 03:54:13.603300: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\nSome layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_20']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nEpoch 1/20\n380/380 [==============================] - 44s 87ms/step - loss: 0.4815 - accuracy: 0.7913 - val_loss: 0.3999 - val_accuracy: 0.8270\nEpoch 2/20\n380/380 [==============================] - 31s 83ms/step - loss: 0.3179 - accuracy: 0.8750 - val_loss: 0.4339 - val_accuracy: 0.8164\nEpoch 3/20\n380/380 [==============================] - 31s 83ms/step - loss: 0.2035 - accuracy: 0.9238 - val_loss: 0.4884 - val_accuracy: 0.8053\nEpoch 4/20\n380/380 [==============================] - 31s 83ms/step - loss: 0.1147 - accuracy: 0.9572 - val_loss: 0.5929 - val_accuracy: 0.7967\nEpoch 5/20\n380/380 [==============================] - 32s 83ms/step - loss: 0.0829 - accuracy: 0.9729 - val_loss: 0.7295 - val_accuracy: 0.7717\nEpoch 6/20\n380/380 [==============================] - 31s 82ms/step - loss: 0.0579 - accuracy: 0.9760 - val_loss: 0.9569 - val_accuracy: 0.7855\n\n\n&lt;keras.callbacks.History at 0x7f06a863c950&gt;\n\n\n\nPredict Test\n\npred_dataset = load_dataset(\"csv\", data_files = \"/kaggle/input/nlp-getting-started/test.csv\", usecols = ['text'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n# preprocess\npred_dataset = pred_dataset.map(tokenize_dataset)\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_pred_dataset = pred_dataset['train'].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False)\n\n\npreds = model.predict(tf_pred_dataset)\n\n\npreds = np.argmax(preds['logits'], axis = 1)\n\n\ntest_df[\"target\"] = preds\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"DistilBERT_FT_Submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#observations-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#observations-1",
    "title": "4. BERT",
    "section": "Observations",
    "text": "Observations\nNot bad! Note that we don‚Äôt tune much this model, so probably we could improve it easily!\nThe important point is how easy is use hub models from huggingface and finetune. If you look the website, you can see that there are a lot of datasets and models made by de comunnity!\nIn the next notebook I will do my best combining all the stuff that we saw in this notebooks NLP series to get a better result. Also I will pre-process the data before feed it into the model"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html",
    "title": "2. Embeddings",
    "section": "",
    "text": "In the last notebook we use only the TextVectorization layer to represent text. I think it is not efficient because each position on each sentence will have a different number(index) depending on the token associated to it but the same weight! So we are sharing weights between words without capture some context.\nIt could be better if we encode each word as a vector(wich have magnitude and direction). So we could represent similar words with the magnitud and direction of the vectors! One way to do this is for example computing the (cosine similarity)[https://en.wikipedia.org/wiki/Cosine_similarity]\nWe will do that with Embeddings! This is a vector that represent in this case a token! (It could be represent a letter, sub-word, sentence, etc.)\nWe can use the (Tensorflow Embedding layer)[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding] for this. This will initialize the representations by random (see the parameter embedding_initializer). So the idea is fit these embeddings to the data and so learn to represent text.\nWe can also use pre-trained Embeddings like (Glove)[https://nlp.stanford.edu/projects/glove/] or (Word2Vec)[https://jalammar.github.io/illustrated-word2vec/]. This could be a better idea because we can fine-tune those.\nIn this notebook we will do both (Embedding And Pre-trained Embeddings).\n\n\n\nNLP-Embedding-NN.png\n\n\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\n\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 01:04:59.963536: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nüëÅ Note that Embedding Layer transform indexes to vectors! So we still need TextVectorization Layer!\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 250 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 01:05:00.242787: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\nDue to we will use embedding to represent text, we can use a bigger output_sequence_length because we hope reduce the dimension of that with the embedding. For this now output_sequence_length=250\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nWe going to use an Embedding where the input will be a tensor with the lenght of the sequences. And we wants to use a representation embedding of dimension 64. Frequently this number is less than the sequence (because we expected less dimension without loss of information).\nNote that after the Embedding Layer there is a GlobalAveragePooling Layer. Thats because there is one vector embedding to each token. So we are adding one dimension (See the Arquitecture). To reduce this dimension I saw two techniques (maybe there are more): 1. Take the average in the sequence dimension (250 in this case) 2. Concat all the embeddings and then Flatten()\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(max_features, 64),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\nThe input dim of Embedding layer should be the vocabulary size, because the Embedding at the end is a big matrix of each word represented by a vector (max_features, output)\n\n\n\nimage.png\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding (Embedding)        (None, 250, 64)           640000    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 64)                0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                1040      \n_________________________________________________________________\ndropout (Dropout)            (None, 16)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 641,057\nTrainable params: 641,057\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\nüîç Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\nIn this case, I will use EarlyStopping Callback to avoid overfitting.\nClick Here to learn more about Early Stopping\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 3s 10ms/step - loss: 0.6815 - binary_accuracy: 0.5732 - val_loss: 0.6823 - val_binary_accuracy: 0.5588\nEpoch 2/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6720 - binary_accuracy: 0.5732 - val_loss: 0.6682 - val_binary_accuracy: 0.5588\nEpoch 3/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6303 - binary_accuracy: 0.5772 - val_loss: 0.5971 - val_binary_accuracy: 0.5929\nEpoch 4/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.5252 - binary_accuracy: 0.6987 - val_loss: 0.5087 - val_binary_accuracy: 0.7682\nEpoch 5/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.4301 - binary_accuracy: 0.7957 - val_loss: 0.4672 - val_binary_accuracy: 0.7840\nEpoch 6/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.3725 - binary_accuracy: 0.8350 - val_loss: 0.4521 - val_binary_accuracy: 0.7892\nEpoch 7/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.3281 - binary_accuracy: 0.8586 - val_loss: 0.4481 - val_binary_accuracy: 0.7905\nEpoch 8/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2898 - binary_accuracy: 0.8785 - val_loss: 0.4517 - val_binary_accuracy: 0.7997\nEpoch 9/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2584 - binary_accuracy: 0.8934 - val_loss: 0.4595 - val_binary_accuracy: 0.7978\nEpoch 10/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2286 - binary_accuracy: 0.9071 - val_loss: 0.4727 - val_binary_accuracy: 0.7978\nEpoch 11/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2058 - binary_accuracy: 0.9176 - val_loss: 0.4892 - val_binary_accuracy: 0.7971\nEpoch 12/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.1839 - binary_accuracy: 0.9261 - val_loss: 0.5107 - val_binary_accuracy: 0.7965\n\n\nüôâ It looks like the NN learn a lot more! Val Accuracy of 0.8\n\n\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[1.626427 ],\n       [0.6582693],\n       [5.033337 ],\n       ...,\n       [2.6492274],\n       [2.4858732],\n       [1.0287898]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.8356796 ],\n       [0.6588715 ],\n       [0.9935252 ],\n       ...,\n       [0.9339634 ],\n       [0.92314553],\n       [0.73668116]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n1\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‚Ä∞√õ√í SAFETY FASTE...\n1\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows √ó 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"Embedding_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data",
    "title": "2. Embeddings",
    "section": "",
    "text": "# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 01:04:59.963536: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nüëÅ Note that Embedding Layer transform indexes to vectors! So we still need TextVectorization Layer!\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 250 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 01:05:00.242787: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\nDue to we will use embedding to represent text, we can use a bigger output_sequence_length because we hope reduce the dimension of that with the embedding. For this now output_sequence_length=250"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data-pipeline",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data-pipeline",
    "title": "2. Embeddings",
    "section": "",
    "text": "Now we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model",
    "title": "2. Embeddings",
    "section": "",
    "text": "We going to use an Embedding where the input will be a tensor with the lenght of the sequences. And we wants to use a representation embedding of dimension 64. Frequently this number is less than the sequence (because we expected less dimension without loss of information).\nNote that after the Embedding Layer there is a GlobalAveragePooling Layer. Thats because there is one vector embedding to each token. So we are adding one dimension (See the Arquitecture). To reduce this dimension I saw two techniques (maybe there are more): 1. Take the average in the sequence dimension (250 in this case) 2. Concat all the embeddings and then Flatten()\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(max_features, 64),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\nThe input dim of Embedding layer should be the vocabulary size, because the Embedding at the end is a big matrix of each word represented by a vector (max_features, output)\n\n\n\nimage.png\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding (Embedding)        (None, 250, 64)           640000    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 64)                0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                1040      \n_________________________________________________________________\ndropout (Dropout)            (None, 16)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 641,057\nTrainable params: 641,057\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\nüîç Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\nIn this case, I will use EarlyStopping Callback to avoid overfitting.\nClick Here to learn more about Early Stopping\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 3s 10ms/step - loss: 0.6815 - binary_accuracy: 0.5732 - val_loss: 0.6823 - val_binary_accuracy: 0.5588\nEpoch 2/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6720 - binary_accuracy: 0.5732 - val_loss: 0.6682 - val_binary_accuracy: 0.5588\nEpoch 3/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6303 - binary_accuracy: 0.5772 - val_loss: 0.5971 - val_binary_accuracy: 0.5929\nEpoch 4/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.5252 - binary_accuracy: 0.6987 - val_loss: 0.5087 - val_binary_accuracy: 0.7682\nEpoch 5/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.4301 - binary_accuracy: 0.7957 - val_loss: 0.4672 - val_binary_accuracy: 0.7840\nEpoch 6/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.3725 - binary_accuracy: 0.8350 - val_loss: 0.4521 - val_binary_accuracy: 0.7892\nEpoch 7/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.3281 - binary_accuracy: 0.8586 - val_loss: 0.4481 - val_binary_accuracy: 0.7905\nEpoch 8/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2898 - binary_accuracy: 0.8785 - val_loss: 0.4517 - val_binary_accuracy: 0.7997\nEpoch 9/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2584 - binary_accuracy: 0.8934 - val_loss: 0.4595 - val_binary_accuracy: 0.7978\nEpoch 10/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2286 - binary_accuracy: 0.9071 - val_loss: 0.4727 - val_binary_accuracy: 0.7978\nEpoch 11/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2058 - binary_accuracy: 0.9176 - val_loss: 0.4892 - val_binary_accuracy: 0.7971\nEpoch 12/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.1839 - binary_accuracy: 0.9261 - val_loss: 0.5107 - val_binary_accuracy: 0.7965\n\n\nüôâ It looks like the NN learn a lot more! Val Accuracy of 0.8"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test",
    "title": "2. Embeddings",
    "section": "",
    "text": "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[1.626427 ],\n       [0.6582693],\n       [5.033337 ],\n       ...,\n       [2.6492274],\n       [2.4858732],\n       [1.0287898]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.8356796 ],\n       [0.6588715 ],\n       [0.9935252 ],\n       ...,\n       [0.9339634 ],\n       [0.92314553],\n       [0.73668116]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n1\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‚Ä∞√õ√í SAFETY FASTE...\n1\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows √ó 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"Embedding_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#glove",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#glove",
    "title": "2. Embeddings",
    "section": "GloVe",
    "text": "GloVe\nIt is important to know how (and on what data) this embedding was trained. Here are some references:\n\nhttps://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010\nhttps://nlp.stanford.edu/projects/glove/\n\n\n# First we download the embedding or matrix !\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip\n\n--2022-12-12 01:05:29--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2022-12-12 01:05:29--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2022-12-12 01:05:30--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‚Äòglove.6B.zip‚Äô\n\nglove.6B.zip        100%[===================&gt;] 822.24M  5.01MB/s    in 2m 39s  \n\n2022-12-12 01:08:10 (5.17 MB/s) - ‚Äòglove.6B.zip‚Äô saved [862182613/862182613]\n\n\n\nüëÄüëÄüëÄüëÄüëÄüëÄüëÄüëÄüëÄ\nWe will use the same Tensorflow Embedding layer. For this we need to construct an embedding matrix and initialize the layer with that matrix.\nTo do that we will:\n\nCreate the embedding index. i.e From the txt embedding file we will get the representation vector for each word! (token)\nWe will assign the pre-trained representation vector to each word(token) of our vocabulary. If the word is not in the pre-trained embedding, we will let a zeros vector\nIn the model we initialize the layer with our pre-trained embedding matrix\n\nSo we will get an embedding matrix of shape (voc_size, embedding_size). Note that each index corresponds to a specific word of our vocabulary. So we need to map index -&gt; embedding.\n\nimport os\n\n# Create a pre-trained embedding index\n\npath_to_glove_file = './glove.6B.50d.txt' # We can choose 300d,200d,100d,50d\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\nFound 400000 word vectors.\n\n\n\n# Word index of OUR vocabulary\nvoc = vectorization_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\n\n#word_index\n\n\n# We have to construct the embedding matrix with weigths from our own vocabulary\n\n# shape embedding matrix : (vocab_size, embedding_dim)\nnum_tokens = len(voc)\nembedding_dim = 50 # we download glove 100 dimension\nhits = []\nmisses = []\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits.append(word)\n    else:\n        misses.append(word)\nprint(\"Converted %d words (%d misses)\" % (len(hits), len(misses)))\n\nConverted 7692 words (2308 misses)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-1",
    "title": "2. Embeddings",
    "section": "Model",
    "text": "Model\nThe same architecture as above but now with the GloVe pre-trained embedding\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=True), # To Fine tune\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 250, 50)           500000    \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 50)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                816       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 500,833\nTrainable params: 500,833\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 3s 9ms/step - loss: 0.6637 - binary_accuracy: 0.5732 - val_loss: 0.6411 - val_binary_accuracy: 0.5588\nEpoch 2/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.5936 - binary_accuracy: 0.6099 - val_loss: 0.5518 - val_binary_accuracy: 0.6717\nEpoch 3/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.5024 - binary_accuracy: 0.7402 - val_loss: 0.4895 - val_binary_accuracy: 0.7879\nEpoch 4/100\n191/191 [==============================] - 1s 7ms/step - loss: 0.4393 - binary_accuracy: 0.7924 - val_loss: 0.4594 - val_binary_accuracy: 0.7984\nEpoch 5/100\n191/191 [==============================] - 1s 7ms/step - loss: 0.3949 - binary_accuracy: 0.8202 - val_loss: 0.4412 - val_binary_accuracy: 0.8050\nEpoch 6/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.3598 - binary_accuracy: 0.8415 - val_loss: 0.4314 - val_binary_accuracy: 0.8089\nEpoch 7/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.3287 - binary_accuracy: 0.8544 - val_loss: 0.4301 - val_binary_accuracy: 0.8116\nEpoch 8/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2976 - binary_accuracy: 0.8685 - val_loss: 0.4311 - val_binary_accuracy: 0.8063\nEpoch 9/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2732 - binary_accuracy: 0.8814 - val_loss: 0.4363 - val_binary_accuracy: 0.8070\nEpoch 10/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2517 - binary_accuracy: 0.8934 - val_loss: 0.4445 - val_binary_accuracy: 0.8076\nEpoch 11/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2306 - binary_accuracy: 0.9051 - val_loss: 0.4565 - val_binary_accuracy: 0.8070\nEpoch 12/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2083 - binary_accuracy: 0.9133 - val_loss: 0.4704 - val_binary_accuracy: 0.8050"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-1",
    "title": "2. Embeddings",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[1.5487207],\n       [1.0597007],\n       [3.9148426],\n       ...,\n       [1.958971 ],\n       [2.308964 ],\n       [1.0568069]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.8247289 ],\n       [0.74263334],\n       [0.98044634],\n       ...,\n       [0.8764216 ],\n       [0.9096167 ],\n       [0.74207985]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n1\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‚Ä∞√õ√í SAFETY FASTE...\n1\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows √ó 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"GloVe_Embedding_submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nü•∞ü•∞ü•∞ü•∞ü•∞ü•∞\nYes! We improve the score! Although it is not so much better. I think that means that is not to difficult learn the embedding from scratch in this use case! Maybe due we have enough data? or because we select a not to huge embedding dimension? What do you think?\n\nüß† Task: Try with an 300,200 or 100 d Glove Embedding!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-2",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-2",
    "title": "2. Embeddings",
    "section": "Model",
    "text": "Model\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=True\n    ),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 250, 400)          4000000   \n_________________________________________________________________\nglobal_average_pooling1d_2 ( (None, 400)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 16)                6416      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 4,006,433\nTrainable params: 4,006,433\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 9s 39ms/step - loss: 0.6561 - binary_accuracy: 0.5745 - val_loss: 0.6037 - val_binary_accuracy: 0.6179\nEpoch 2/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.5149 - binary_accuracy: 0.7233 - val_loss: 0.4822 - val_binary_accuracy: 0.7439\nEpoch 3/100\n191/191 [==============================] - 7s 39ms/step - loss: 0.3944 - binary_accuracy: 0.8220 - val_loss: 0.4769 - val_binary_accuracy: 0.7663\nEpoch 4/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.3212 - binary_accuracy: 0.8614 - val_loss: 0.4878 - val_binary_accuracy: 0.7827\nEpoch 5/100\n191/191 [==============================] - 7s 38ms/step - loss: 0.2681 - binary_accuracy: 0.8854 - val_loss: 0.5148 - val_binary_accuracy: 0.7846\nEpoch 6/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.2223 - binary_accuracy: 0.9080 - val_loss: 0.5104 - val_binary_accuracy: 0.7965\nEpoch 7/100\n191/191 [==============================] - 7s 39ms/step - loss: 0.1857 - binary_accuracy: 0.9268 - val_loss: 0.5446 - val_binary_accuracy: 0.7938\nEpoch 8/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.1563 - binary_accuracy: 0.9376 - val_loss: 0.5953 - val_binary_accuracy: 0.7859"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-2",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-2",
    "title": "2. Embeddings",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[-0.06506764],\n       [ 0.26650202],\n       [ 5.848435  ],\n       ...,\n       [ 1.5803046 ],\n       [ 1.5205853 ],\n       [ 0.33927795]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.48373884],\n       [0.56623393],\n       [0.9971239 ],\n       ...,\n       [0.8292477 ],\n       [0.82062465],\n       [0.58401513]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n0\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‚Ä∞√õ√í SAFETY FASTE...\n0\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows √ó 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n0\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\n#sub.to_csv(\"TTW2V_Embedding_Submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nNice! As we excpected it, We improve the score a little bit!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Diegulio‚Äôs Blog ü¶ä",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nIniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT\n\n\nCocina tu comida favorita con la ayuda de LLM\n\n\n\n\npython\n\n\nllm\n\n\nchatgpt\n\n\nlangchain\n\n\ngradio\n\n\n \n\n\n\n\nDiegulio\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nBirdClef 2023 Competition\n\n\nClasificando Sonidos de P√°jaros con Redes Neuronales\n\n\n\n\nAudio\n\n\ncode\n\n\nKaggle\n\n\nPytorch\n\n\n \n\n\n\n\nDiegulio\n\n\n23 min\n\n\n\n\n\n\n  \n\n\n\n\nBack To Sit\n\n\nA library to notify you when your code is done!\n\n\n\n\npython\n\n\nlibrary\n\n\n \n\n\n\n\nDiegulio\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nIdentificando desastres en Twitter con NLP\n\n\nAprende a usar Natural Language Processing (NLP) para identificar desastres en Twitter\n\n\n\n\nNLP\n\n\ncode\n\n\nKaggle\n\n\n \n\n\n\n\nDiegulio\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! Soy Diego Machado, Ingeniero Civil Industrial con mag√≠ster en Ciencias de la Ingenier√≠a. Soy un entusiasta en el mundo de la inteligencia artificial, me encuentro constantenemente aprendiendo y desarrollando nuevos proyectos. Tengo conocimientos en una gran variedad de aplicaciones de la IA, tales como Regresi√≥n, Clasificaci√≥n, Time Series, NLP, CV y Audio."
  }
]