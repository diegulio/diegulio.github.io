[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! Soy Diego Machado, Ingeniero Civil Industrial con mag√≠ster en Ciencias de la Ingenier√≠a. Soy un entusiasta en el mundo de la inteligencia artificial, me encuentro constantenemente aprendiendo y desarrollando nuevos proyectos. Tengo conocimientos en una gran variedad de aplicaciones de la IA, tales como Regresi√≥n, Clasificaci√≥n, Time Series, NLP, CV y Audio."
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html",
    "title": "2. Embeddings",
    "section": "",
    "text": "In the last notebook we use only the TextVectorization layer to represent text. I think it is not efficient because each position on each sentence will have a different number(index) depending on the token associated to it but the same weight! So we are sharing weights between words without capture some context.\nIt could be better if we encode each word as a vector(wich have magnitude and direction). So we could represent similar words with the magnitud and direction of the vectors! One way to do this is for example computing the (cosine similarity)[https://en.wikipedia.org/wiki/Cosine_similarity]\nWe will do that with Embeddings! This is a vector that represent in this case a token! (It could be represent a letter, sub-word, sentence, etc.)\nWe can use the (Tensorflow Embedding layer)[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding] for this. This will initialize the representations by random (see the parameter embedding_initializer). So the idea is fit these embeddings to the data and so learn to represent text.\nWe can also use pre-trained Embeddings like (Glove)[https://nlp.stanford.edu/projects/glove/] or (Word2Vec)[https://jalammar.github.io/illustrated-word2vec/]. This could be a better idea because we can fine-tune those.\nIn this notebook we will do both (Embedding And Pre-trained Embeddings).\n\n\n\nNLP-Embedding-NN.png\n\n\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\n\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 01:04:59.963536: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nüëÅ Note that Embedding Layer transform indexes to vectors! So we still need TextVectorization Layer!\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 250 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 01:05:00.242787: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\nDue to we will use embedding to represent text, we can use a bigger output_sequence_length because we hope reduce the dimension of that with the embedding. For this now output_sequence_length=250\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nWe going to use an Embedding where the input will be a tensor with the lenght of the sequences. And we wants to use a representation embedding of dimension 64. Frequently this number is less than the sequence (because we expected less dimension without loss of information).\nNote that after the Embedding Layer there is a GlobalAveragePooling Layer. Thats because there is one vector embedding to each token. So we are adding one dimension (See the Arquitecture). To reduce this dimension I saw two techniques (maybe there are more): 1. Take the average in the sequence dimension (250 in this case) 2. Concat all the embeddings and then Flatten()\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(max_features, 64),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\nThe input dim of Embedding layer should be the vocabulary size, because the Embedding at the end is a big matrix of each word represented by a vector (max_features, output)\n\n\n\nimage.png\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding (Embedding)        (None, 250, 64)           640000    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 64)                0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                1040      \n_________________________________________________________________\ndropout (Dropout)            (None, 16)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 641,057\nTrainable params: 641,057\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\nüîç Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\nIn this case, I will use EarlyStopping Callback to avoid overfitting.\nClick Here to learn more about Early Stopping\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 3s 10ms/step - loss: 0.6815 - binary_accuracy: 0.5732 - val_loss: 0.6823 - val_binary_accuracy: 0.5588\nEpoch 2/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6720 - binary_accuracy: 0.5732 - val_loss: 0.6682 - val_binary_accuracy: 0.5588\nEpoch 3/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6303 - binary_accuracy: 0.5772 - val_loss: 0.5971 - val_binary_accuracy: 0.5929\nEpoch 4/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.5252 - binary_accuracy: 0.6987 - val_loss: 0.5087 - val_binary_accuracy: 0.7682\nEpoch 5/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.4301 - binary_accuracy: 0.7957 - val_loss: 0.4672 - val_binary_accuracy: 0.7840\nEpoch 6/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.3725 - binary_accuracy: 0.8350 - val_loss: 0.4521 - val_binary_accuracy: 0.7892\nEpoch 7/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.3281 - binary_accuracy: 0.8586 - val_loss: 0.4481 - val_binary_accuracy: 0.7905\nEpoch 8/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2898 - binary_accuracy: 0.8785 - val_loss: 0.4517 - val_binary_accuracy: 0.7997\nEpoch 9/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2584 - binary_accuracy: 0.8934 - val_loss: 0.4595 - val_binary_accuracy: 0.7978\nEpoch 10/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2286 - binary_accuracy: 0.9071 - val_loss: 0.4727 - val_binary_accuracy: 0.7978\nEpoch 11/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2058 - binary_accuracy: 0.9176 - val_loss: 0.4892 - val_binary_accuracy: 0.7971\nEpoch 12/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.1839 - binary_accuracy: 0.9261 - val_loss: 0.5107 - val_binary_accuracy: 0.7965\n\n\nüôâ It looks like the NN learn a lot more! Val Accuracy of 0.8\n\n\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[1.626427 ],\n       [0.6582693],\n       [5.033337 ],\n       ...,\n       [2.6492274],\n       [2.4858732],\n       [1.0287898]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.8356796 ],\n       [0.6588715 ],\n       [0.9935252 ],\n       ...,\n       [0.9339634 ],\n       [0.92314553],\n       [0.73668116]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n1\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‚Ä∞√õ√í SAFETY FASTE...\n1\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows √ó 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"Embedding_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data",
    "title": "2. Embeddings",
    "section": "",
    "text": "# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 01:04:59.963536: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nüëÅ Note that Embedding Layer transform indexes to vectors! So we still need TextVectorization Layer!\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 250 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 01:05:00.242787: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\nDue to we will use embedding to represent text, we can use a bigger output_sequence_length because we hope reduce the dimension of that with the embedding. For this now output_sequence_length=250"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data-pipeline",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#data-pipeline",
    "title": "2. Embeddings",
    "section": "",
    "text": "Now we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model",
    "title": "2. Embeddings",
    "section": "",
    "text": "We going to use an Embedding where the input will be a tensor with the lenght of the sequences. And we wants to use a representation embedding of dimension 64. Frequently this number is less than the sequence (because we expected less dimension without loss of information).\nNote that after the Embedding Layer there is a GlobalAveragePooling Layer. Thats because there is one vector embedding to each token. So we are adding one dimension (See the Arquitecture). To reduce this dimension I saw two techniques (maybe there are more): 1. Take the average in the sequence dimension (250 in this case) 2. Concat all the embeddings and then Flatten()\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(max_features, 64),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\nThe input dim of Embedding layer should be the vocabulary size, because the Embedding at the end is a big matrix of each word represented by a vector (max_features, output)\n\n\n\nimage.png\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding (Embedding)        (None, 250, 64)           640000    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 64)                0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                1040      \n_________________________________________________________________\ndropout (Dropout)            (None, 16)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 641,057\nTrainable params: 641,057\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\nüîç Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\nIn this case, I will use EarlyStopping Callback to avoid overfitting.\nClick Here to learn more about Early Stopping\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 3s 10ms/step - loss: 0.6815 - binary_accuracy: 0.5732 - val_loss: 0.6823 - val_binary_accuracy: 0.5588\nEpoch 2/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6720 - binary_accuracy: 0.5732 - val_loss: 0.6682 - val_binary_accuracy: 0.5588\nEpoch 3/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.6303 - binary_accuracy: 0.5772 - val_loss: 0.5971 - val_binary_accuracy: 0.5929\nEpoch 4/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.5252 - binary_accuracy: 0.6987 - val_loss: 0.5087 - val_binary_accuracy: 0.7682\nEpoch 5/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.4301 - binary_accuracy: 0.7957 - val_loss: 0.4672 - val_binary_accuracy: 0.7840\nEpoch 6/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.3725 - binary_accuracy: 0.8350 - val_loss: 0.4521 - val_binary_accuracy: 0.7892\nEpoch 7/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.3281 - binary_accuracy: 0.8586 - val_loss: 0.4481 - val_binary_accuracy: 0.7905\nEpoch 8/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2898 - binary_accuracy: 0.8785 - val_loss: 0.4517 - val_binary_accuracy: 0.7997\nEpoch 9/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2584 - binary_accuracy: 0.8934 - val_loss: 0.4595 - val_binary_accuracy: 0.7978\nEpoch 10/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2286 - binary_accuracy: 0.9071 - val_loss: 0.4727 - val_binary_accuracy: 0.7978\nEpoch 11/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2058 - binary_accuracy: 0.9176 - val_loss: 0.4892 - val_binary_accuracy: 0.7971\nEpoch 12/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.1839 - binary_accuracy: 0.9261 - val_loss: 0.5107 - val_binary_accuracy: 0.7965\n\n\nüôâ It looks like the NN learn a lot more! Val Accuracy of 0.8"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test",
    "title": "2. Embeddings",
    "section": "",
    "text": "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[1.626427 ],\n       [0.6582693],\n       [5.033337 ],\n       ...,\n       [2.6492274],\n       [2.4858732],\n       [1.0287898]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.8356796 ],\n       [0.6588715 ],\n       [0.9935252 ],\n       ...,\n       [0.9339634 ],\n       [0.92314553],\n       [0.73668116]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n1\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‚Ä∞√õ√í SAFETY FASTE...\n1\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows √ó 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"Embedding_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#glove",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#glove",
    "title": "2. Embeddings",
    "section": "GloVe",
    "text": "GloVe\nIt is important to know how (and on what data) this embedding was trained. Here are some references:\n\nhttps://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010\nhttps://nlp.stanford.edu/projects/glove/\n\n\n# First we download the embedding or matrix !\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip\n\n--2022-12-12 01:05:29--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2022-12-12 01:05:29--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2022-12-12 01:05:30--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‚Äòglove.6B.zip‚Äô\n\nglove.6B.zip        100%[===================&gt;] 822.24M  5.01MB/s    in 2m 39s  \n\n2022-12-12 01:08:10 (5.17 MB/s) - ‚Äòglove.6B.zip‚Äô saved [862182613/862182613]\n\n\n\nüëÄüëÄüëÄüëÄüëÄüëÄüëÄüëÄüëÄ\nWe will use the same Tensorflow Embedding layer. For this we need to construct an embedding matrix and initialize the layer with that matrix.\nTo do that we will:\n\nCreate the embedding index. i.e From the txt embedding file we will get the representation vector for each word! (token)\nWe will assign the pre-trained representation vector to each word(token) of our vocabulary. If the word is not in the pre-trained embedding, we will let a zeros vector\nIn the model we initialize the layer with our pre-trained embedding matrix\n\nSo we will get an embedding matrix of shape (voc_size, embedding_size). Note that each index corresponds to a specific word of our vocabulary. So we need to map index -&gt; embedding.\n\nimport os\n\n# Create a pre-trained embedding index\n\npath_to_glove_file = './glove.6B.50d.txt' # We can choose 300d,200d,100d,50d\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\nFound 400000 word vectors.\n\n\n\n# Word index of OUR vocabulary\nvoc = vectorization_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\n\n#word_index\n\n\n# We have to construct the embedding matrix with weigths from our own vocabulary\n\n# shape embedding matrix : (vocab_size, embedding_dim)\nnum_tokens = len(voc)\nembedding_dim = 50 # we download glove 100 dimension\nhits = []\nmisses = []\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits.append(word)\n    else:\n        misses.append(word)\nprint(\"Converted %d words (%d misses)\" % (len(hits), len(misses)))\n\nConverted 7692 words (2308 misses)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-1",
    "title": "2. Embeddings",
    "section": "Model",
    "text": "Model\nThe same architecture as above but now with the GloVe pre-trained embedding\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=True), # To Fine tune\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 250, 50)           500000    \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 50)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                816       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 500,833\nTrainable params: 500,833\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 3s 9ms/step - loss: 0.6637 - binary_accuracy: 0.5732 - val_loss: 0.6411 - val_binary_accuracy: 0.5588\nEpoch 2/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.5936 - binary_accuracy: 0.6099 - val_loss: 0.5518 - val_binary_accuracy: 0.6717\nEpoch 3/100\n191/191 [==============================] - 1s 8ms/step - loss: 0.5024 - binary_accuracy: 0.7402 - val_loss: 0.4895 - val_binary_accuracy: 0.7879\nEpoch 4/100\n191/191 [==============================] - 1s 7ms/step - loss: 0.4393 - binary_accuracy: 0.7924 - val_loss: 0.4594 - val_binary_accuracy: 0.7984\nEpoch 5/100\n191/191 [==============================] - 1s 7ms/step - loss: 0.3949 - binary_accuracy: 0.8202 - val_loss: 0.4412 - val_binary_accuracy: 0.8050\nEpoch 6/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.3598 - binary_accuracy: 0.8415 - val_loss: 0.4314 - val_binary_accuracy: 0.8089\nEpoch 7/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.3287 - binary_accuracy: 0.8544 - val_loss: 0.4301 - val_binary_accuracy: 0.8116\nEpoch 8/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2976 - binary_accuracy: 0.8685 - val_loss: 0.4311 - val_binary_accuracy: 0.8063\nEpoch 9/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2732 - binary_accuracy: 0.8814 - val_loss: 0.4363 - val_binary_accuracy: 0.8070\nEpoch 10/100\n191/191 [==============================] - 2s 9ms/step - loss: 0.2517 - binary_accuracy: 0.8934 - val_loss: 0.4445 - val_binary_accuracy: 0.8076\nEpoch 11/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2306 - binary_accuracy: 0.9051 - val_loss: 0.4565 - val_binary_accuracy: 0.8070\nEpoch 12/100\n191/191 [==============================] - 2s 8ms/step - loss: 0.2083 - binary_accuracy: 0.9133 - val_loss: 0.4704 - val_binary_accuracy: 0.8050"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-1",
    "title": "2. Embeddings",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[1.5487207],\n       [1.0597007],\n       [3.9148426],\n       ...,\n       [1.958971 ],\n       [2.308964 ],\n       [1.0568069]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.8247289 ],\n       [0.74263334],\n       [0.98044634],\n       ...,\n       [0.8764216 ],\n       [0.9096167 ],\n       [0.74207985]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n1\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‚Ä∞√õ√í SAFETY FASTE...\n1\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows √ó 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"GloVe_Embedding_submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nü•∞ü•∞ü•∞ü•∞ü•∞ü•∞\nYes! We improve the score! Although it is not so much better. I think that means that is not to difficult learn the embedding from scratch in this use case! Maybe due we have enough data? or because we select a not to huge embedding dimension? What do you think?\n\nüß† Task: Try with an 300,200 or 100 d Glove Embedding!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-2",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#model-2",
    "title": "2. Embeddings",
    "section": "Model",
    "text": "Model\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=True\n    ),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(16),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 250)               0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 250, 400)          4000000   \n_________________________________________________________________\nglobal_average_pooling1d_2 ( (None, 400)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 16)                6416      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 4,006,433\nTrainable params: 4,006,433\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback])\n\nEpoch 1/100\n191/191 [==============================] - 9s 39ms/step - loss: 0.6561 - binary_accuracy: 0.5745 - val_loss: 0.6037 - val_binary_accuracy: 0.6179\nEpoch 2/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.5149 - binary_accuracy: 0.7233 - val_loss: 0.4822 - val_binary_accuracy: 0.7439\nEpoch 3/100\n191/191 [==============================] - 7s 39ms/step - loss: 0.3944 - binary_accuracy: 0.8220 - val_loss: 0.4769 - val_binary_accuracy: 0.7663\nEpoch 4/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.3212 - binary_accuracy: 0.8614 - val_loss: 0.4878 - val_binary_accuracy: 0.7827\nEpoch 5/100\n191/191 [==============================] - 7s 38ms/step - loss: 0.2681 - binary_accuracy: 0.8854 - val_loss: 0.5148 - val_binary_accuracy: 0.7846\nEpoch 6/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.2223 - binary_accuracy: 0.9080 - val_loss: 0.5104 - val_binary_accuracy: 0.7965\nEpoch 7/100\n191/191 [==============================] - 7s 39ms/step - loss: 0.1857 - binary_accuracy: 0.9268 - val_loss: 0.5446 - val_binary_accuracy: 0.7938\nEpoch 8/100\n191/191 [==============================] - 7s 37ms/step - loss: 0.1563 - binary_accuracy: 0.9376 - val_loss: 0.5953 - val_binary_accuracy: 0.7859"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-2",
    "href": "posts/kaggle_nlp_disaster/seqclass-2-embeddings.html#predict-test-2",
    "title": "2. Embeddings",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[-0.06506764],\n       [ 0.26650202],\n       [ 5.848435  ],\n       ...,\n       [ 1.5803046 ],\n       [ 1.5205853 ],\n       [ 0.33927795]], dtype=float32)\n\n\n\ntf.nn.sigmoid(preds)\n\n&lt;tf.Tensor: shape=(3263, 1), dtype=float32, numpy=\narray([[0.48373884],\n       [0.56623393],\n       [0.9971239 ],\n       ...,\n       [0.8292477 ],\n       [0.82062465],\n       [0.58401513]], dtype=float32)&gt;\n\n\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds))\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n0\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n1\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n1\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n1\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n3258\n10861\nNaN\nNaN\nEARTHQUAKE SAFETY LOS ANGELES ‚Ä∞√õ√í SAFETY FASTE...\n0\n\n\n3259\n10865\nNaN\nNaN\nStorm in RI worse than last hurricane. My city...\n1\n\n\n3260\n10868\nNaN\nNaN\nGreen Line derailment in Chicago http://t.co/U...\n1\n\n\n3261\n10874\nNaN\nNaN\nMEG issues Hazardous Weather Outlook (HWO) htt...\n1\n\n\n3262\n10875\nNaN\nNaN\n#CityofCalgary has activated its Municipal Eme...\n1\n\n\n\n\n3263 rows √ó 5 columns\n\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n0\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\n#sub.to_csv(\"TTW2V_Embedding_Submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nNice! As we excpected it, We improve the score a little bit!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html",
    "title": "5. My best solution",
    "section": "",
    "text": "In this notebook I will use all the knowledge that we acquiring with the previous notebooks!\nIn this solution I will be using:\n\nTransformers\nHuggingFace\nPreprocessing\nTensorflow\nEarlyStopping\n\nand more..\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\n\n# A dependency of the preprocessing for BERT inputs\n!pip install -q -U \"tensorflow-text==2.8.*\"\n\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;3,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\n\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;2.10,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\n\ntensorflow-serving-api 2.9.0 requires tensorflow&lt;3,&gt;=2.9.0, but you have tensorflow 2.8.4 which is incompatible.\n\ntensorflow-io 0.21.0 requires tensorflow&lt;2.7.0,&gt;=2.6.0, but you have tensorflow 2.8.4 which is incompatible.\n\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.28.0 which is incompatible.\n\npytorch-lightning 1.7.7 requires tensorboard&gt;=2.9.1, but you have tensorboard 2.8.0 which is incompatible.\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\n\n\n\n# Try with a large model\nmodel_name = \"bert-large-uncased\"\n\n\nfrom transformers import AutoTokenizer\n\n# properly tokenization\ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\ntokenizer_max_length = 161\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"], truncation=True, padding=True, max_length=tokenizer_max_length)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI will try a preprocessing that I find here: (I lost the notebook! so sorry, please if someone find it let me know in the comments!)\n\n# Some preprocess \ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\n\n def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\nimport spacy\nimport re\nnlp = spacy.load('en_core_web_sm')\ndef preprocessing(text):\n    text = text.replace('#','')\n    text = decontracted(text)\n    text = re.sub('\\S*@\\S*\\s?','',text)\n    text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n    #token=[]\n    #result=''\n    #text = re.sub('[^A-z]', ' ',text.lower())\n\n    #text = nlp(text)\n    #for t in text:\n    #    if not t.is_stop and len(t)&gt;2:  \n    #        token.append(t.lemma_)\n    #result = ' '.join([i for i in token])\n\n    return text.strip()\n\n\ntrain.text = train.text.apply(lambda x : preprocessing(x)).astype(str)\ntest.text = test.text.apply(lambda x : preprocessing(x)).astype(str)\n\n\n# Save processed data to disk\nNEW_TRAIN_PATH = \"preprocessed_train.csv\"\nNEW_TEST_PATH = \"preprocessed_test.csv\"\n\ntrain.to_csv(NEW_TRAIN_PATH, index = False)\ntest.to_csv(NEW_TEST_PATH, index = False)\n\ndel train\ndel test\ngc.collect()\n\n856\n\n\n\n\n\n\n# Now We can use HF Datasets\n# We need to load our data, for this we use HF datasets\nfrom datasets import load_dataset\ndata_files = {\"train\": NEW_TRAIN_PATH,\n             \"test\": NEW_TEST_PATH}\ndataset = load_dataset(\"csv\", data_files = data_files, usecols = ['text', 'target'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n\n\n\n\n\n\n\n\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n# Tokenization\ndataset = dataset.map(tokenize_dataset)\n\n\n\n\n\n\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\n# we transform the HF dataset into TF dataset\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = train_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True)\n\ntf_val_dataset = test_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False\n    )\n\n\n\n\nNow we will use restore_best_weights from EarlyStopping\n\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(6e-6),metrics = ['accuracy'])\n\n#checkpoint_filepath = 'checkpoint'\n#checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nearly_stop_callback = EarlyStopping(patience = 5, restore_best_weights = True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n#                              patience=2, min_lr=1e-9)\n\nmodel.fit(tf_train_dataset,\n         validation_data = tf_val_dataset,\n          epochs = 20,\n          callbacks = [early_stop_callback]\n         )\n\n\n\n\n2022-12-12 14:19:24.079862: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 125018112 exceeds 10% of free system memory.\n2022-12-12 14:19:24.239512: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.254974: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.272507: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.294475: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\nAll model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nEpoch 1/20\n380/380 [==============================] - 164s 344ms/step - loss: 0.4792 - accuracy: 0.7821 - val_loss: 0.4102 - val_accuracy: 0.8286\nEpoch 2/20\n380/380 [==============================] - 123s 325ms/step - loss: 0.3694 - accuracy: 0.8528 - val_loss: 0.3909 - val_accuracy: 0.8339\nEpoch 3/20\n380/380 [==============================] - 122s 321ms/step - loss: 0.3145 - accuracy: 0.8832 - val_loss: 0.4103 - val_accuracy: 0.8365\nEpoch 4/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2622 - accuracy: 0.9035 - val_loss: 0.4222 - val_accuracy: 0.8391\nEpoch 5/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2201 - accuracy: 0.9211 - val_loss: 0.4777 - val_accuracy: 0.8201\nEpoch 6/20\n380/380 [==============================] - 122s 320ms/step - loss: 0.1804 - accuracy: 0.9327 - val_loss: 0.4962 - val_accuracy: 0.8293\nEpoch 7/20\n380/380 [==============================] - 123s 323ms/step - loss: 0.1509 - accuracy: 0.9413 - val_loss: 0.5522 - val_accuracy: 0.8201\n\n\n&lt;keras.callbacks.History at 0x7fefa3775050&gt;\n\n\n\n\n\n\npred_dataset = load_dataset(\"csv\", data_files = \"/kaggle/input/nlp-getting-started/test.csv\", usecols = ['text'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n\n\n\n\n\n\n\n\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\npred_dataset = pred_dataset.map(tokenize_dataset)\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_pred_dataset = pred_dataset['train'].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False)\n\n\npreds = model.predict(tf_pred_dataset)\n\n\npreds = np.argmax(preds['logits'], axis = 1)\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_df[\"target\"] = preds\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#libraries",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#libraries",
    "title": "5. My best solution",
    "section": "",
    "text": "# A dependency of the preprocessing for BERT inputs\n!pip install -q -U \"tensorflow-text==2.8.*\"\n\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;3,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\n\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;2.10,&gt;=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\n\ntensorflow-serving-api 2.9.0 requires tensorflow&lt;3,&gt;=2.9.0, but you have tensorflow 2.8.4 which is incompatible.\n\ntensorflow-io 0.21.0 requires tensorflow&lt;2.7.0,&gt;=2.6.0, but you have tensorflow 2.8.4 which is incompatible.\n\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.28.0 which is incompatible.\n\npytorch-lightning 1.7.7 requires tensorboard&gt;=2.9.1, but you have tensorboard 2.8.0 which is incompatible.\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#huggingface-model",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#huggingface-model",
    "title": "5. My best solution",
    "section": "",
    "text": "# Try with a large model\nmodel_name = \"bert-large-uncased\"\n\n\nfrom transformers import AutoTokenizer\n\n# properly tokenization\ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\ntokenizer_max_length = 161\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"], truncation=True, padding=True, max_length=tokenizer_max_length)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#data-pre-processing",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#data-pre-processing",
    "title": "5. My best solution",
    "section": "",
    "text": "I will try a preprocessing that I find here: (I lost the notebook! so sorry, please if someone find it let me know in the comments!)\n\n# Some preprocess \ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\n\n def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\nimport spacy\nimport re\nnlp = spacy.load('en_core_web_sm')\ndef preprocessing(text):\n    text = text.replace('#','')\n    text = decontracted(text)\n    text = re.sub('\\S*@\\S*\\s?','',text)\n    text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n    #token=[]\n    #result=''\n    #text = re.sub('[^A-z]', ' ',text.lower())\n\n    #text = nlp(text)\n    #for t in text:\n    #    if not t.is_stop and len(t)&gt;2:  \n    #        token.append(t.lemma_)\n    #result = ' '.join([i for i in token])\n\n    return text.strip()\n\n\ntrain.text = train.text.apply(lambda x : preprocessing(x)).astype(str)\ntest.text = test.text.apply(lambda x : preprocessing(x)).astype(str)\n\n\n# Save processed data to disk\nNEW_TRAIN_PATH = \"preprocessed_train.csv\"\nNEW_TEST_PATH = \"preprocessed_test.csv\"\n\ntrain.to_csv(NEW_TRAIN_PATH, index = False)\ntest.to_csv(NEW_TEST_PATH, index = False)\n\ndel train\ndel test\ngc.collect()\n\n856"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#hf-dataset",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#hf-dataset",
    "title": "5. My best solution",
    "section": "",
    "text": "# Now We can use HF Datasets\n# We need to load our data, for this we use HF datasets\nfrom datasets import load_dataset\ndata_files = {\"train\": NEW_TRAIN_PATH,\n             \"test\": NEW_TEST_PATH}\ndataset = load_dataset(\"csv\", data_files = data_files, usecols = ['text', 'target'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n\n\n\n\n\n\n\n\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-fc2dc1866b45d737/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n# Tokenization\ndataset = dataset.map(tokenize_dataset)\n\n\n\n\n\n\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\n# we transform the HF dataset into TF dataset\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = train_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True)\n\ntf_val_dataset = test_dataset.to_tf_dataset(\n    columns=[\"input_ids\",\"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False\n    )"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#model",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#model",
    "title": "5. My best solution",
    "section": "",
    "text": "Now we will use restore_best_weights from EarlyStopping\n\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(6e-6),metrics = ['accuracy'])\n\n#checkpoint_filepath = 'checkpoint'\n#checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nearly_stop_callback = EarlyStopping(patience = 5, restore_best_weights = True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n#                              patience=2, min_lr=1e-9)\n\nmodel.fit(tf_train_dataset,\n         validation_data = tf_val_dataset,\n          epochs = 20,\n          callbacks = [early_stop_callback]\n         )\n\n\n\n\n2022-12-12 14:19:24.079862: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 125018112 exceeds 10% of free system memory.\n2022-12-12 14:19:24.239512: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.254974: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.272507: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n2022-12-12 14:19:24.294475: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\nAll model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nEpoch 1/20\n380/380 [==============================] - 164s 344ms/step - loss: 0.4792 - accuracy: 0.7821 - val_loss: 0.4102 - val_accuracy: 0.8286\nEpoch 2/20\n380/380 [==============================] - 123s 325ms/step - loss: 0.3694 - accuracy: 0.8528 - val_loss: 0.3909 - val_accuracy: 0.8339\nEpoch 3/20\n380/380 [==============================] - 122s 321ms/step - loss: 0.3145 - accuracy: 0.8832 - val_loss: 0.4103 - val_accuracy: 0.8365\nEpoch 4/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2622 - accuracy: 0.9035 - val_loss: 0.4222 - val_accuracy: 0.8391\nEpoch 5/20\n380/380 [==============================] - 122s 322ms/step - loss: 0.2201 - accuracy: 0.9211 - val_loss: 0.4777 - val_accuracy: 0.8201\nEpoch 6/20\n380/380 [==============================] - 122s 320ms/step - loss: 0.1804 - accuracy: 0.9327 - val_loss: 0.4962 - val_accuracy: 0.8293\nEpoch 7/20\n380/380 [==============================] - 123s 323ms/step - loss: 0.1509 - accuracy: 0.9413 - val_loss: 0.5522 - val_accuracy: 0.8201\n\n\n&lt;keras.callbacks.History at 0x7fefa3775050&gt;"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#predict-test",
    "href": "posts/kaggle_nlp_disaster/seqclass-5-mybestsolution.html#predict-test",
    "title": "5. My best solution",
    "section": "",
    "text": "pred_dataset = load_dataset(\"csv\", data_files = \"/kaggle/input/nlp-getting-started/test.csv\", usecols = ['text'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n\n\n\n\n\n\n\n\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\npred_dataset = pred_dataset.map(tokenize_dataset)\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_pred_dataset = pred_dataset['train'].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False)\n\n\npreds = model.predict(tf_pred_dataset)\n\n\npreds = np.argmax(preds['logits'], axis = 1)\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_df[\"target\"] = preds\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html",
    "title": "1. Simple Neural Network",
    "section": "",
    "text": "In this notebook I will use a simple Neural Network Arquitecture. We need to remember that we only can feed numbers to NN !\nThe Arquitecture will look like this:\n\n\n\nNLP-NN.png\n\n\nRemember that is belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\n\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-11 14:55:06.219408: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nInstantiate TextVectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 50 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-11 14:55:06.451771: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\n\n# Testing the Vectorization layer\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return vectorization_layer(text)\n\n# retrieve a batch (of 1 review and label) from the dataset\ntext_batch, label_batch = next(iter(train_ds))\nfirst_review, first_label = text_batch, label_batch\nprint(\"Review\", first_review)\nprint(\"Label\", first_label)\nprint(\"Vectorized review\", vectorize_text(first_review))\n\nReview tf.Tensor(b'Riot Kit Bah - part of the new concept Gear coming for Autumn/Winter\\n#menswear #fashion #urbanfashion\\xc2\\x89\\xc3\\x9b_ https://t.co/cCwzDTFbUS', shape=(), dtype=string)\nLabel tf.Tensor(0, shape=(), dtype=int64)\nVectorized review tf.Tensor(\n[[ 403 1278    1  572    6    2   44 3700    1  250   10    1    1  968\n  6559    1    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0]], shape=(1, 50), dtype=int64)\n\n\nüîç We see that the length of the vector is output_sequence_length = 250, and if the statement is not that long, it is paded with 0. We also note that no integer is greater than 10000, since we set this as our maximum vocabulary of tokens (vocabulary_size()). We also see that there are some integers = 1. This in the vocabulary corresponds to UNK, that is, unknown words (tokens), this is because our vocabulary is limited (TextVectorization chooses the 10000 most frequent ones).\nAn improvement can be to analyze the length of the dataset sentences and choose an output_sequence_lenght based on this (to avoid to much pad)\n\nvectorization_layer.vocabulary_size()\n\n10000\n\n\n\n# To see the vocabulary\nvectorization_layer.get_vocabulary()[:10]\n\n['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nWe will build a simple Sequential Model composed by two Dense Layers. I dont take to much time tunning it.\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string), # Input Layer\n    vectorization_layer, # Vectorization Layer\n    layers.Dense(64),\n    #layers.Dropout(0.1),\n    layers.Dense(32),\n    #layers.Dropout(0.1),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 50)                0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                3264      \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 5,377\nTrainable params: 5,377\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\nüîç Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\n\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs)\n\nEpoch 1/100\n191/191 [==============================] - 2s 4ms/step - loss: 76.1297 - binary_accuracy: 0.5123 - val_loss: 30.4719 - val_binary_accuracy: 0.5207\nEpoch 2/100\n191/191 [==============================] - 1s 3ms/step - loss: 37.2703 - binary_accuracy: 0.5167 - val_loss: 22.0519 - val_binary_accuracy: 0.5194\nEpoch 3/100\n191/191 [==============================] - 1s 3ms/step - loss: 34.7663 - binary_accuracy: 0.5118 - val_loss: 27.2791 - val_binary_accuracy: 0.5141\nEpoch 4/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.8605 - binary_accuracy: 0.5135 - val_loss: 26.1287 - val_binary_accuracy: 0.5253\nEpoch 5/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.4162 - binary_accuracy: 0.5123 - val_loss: 24.3497 - val_binary_accuracy: 0.5108\nEpoch 6/100\n191/191 [==============================] - 1s 3ms/step - loss: 26.5699 - binary_accuracy: 0.5084 - val_loss: 17.0479 - val_binary_accuracy: 0.5148\nEpoch 7/100\n191/191 [==============================] - 1s 3ms/step - loss: 24.1579 - binary_accuracy: 0.5136 - val_loss: 24.3595 - val_binary_accuracy: 0.5351\nEpoch 8/100\n191/191 [==============================] - 1s 3ms/step - loss: 22.3645 - binary_accuracy: 0.5110 - val_loss: 18.3034 - val_binary_accuracy: 0.5108\nEpoch 9/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.4826 - binary_accuracy: 0.5059 - val_loss: 17.5856 - val_binary_accuracy: 0.5161\nEpoch 10/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.1403 - binary_accuracy: 0.5156 - val_loss: 15.4736 - val_binary_accuracy: 0.5299\nEpoch 11/100\n191/191 [==============================] - 1s 3ms/step - loss: 16.6029 - binary_accuracy: 0.5156 - val_loss: 13.5105 - val_binary_accuracy: 0.5220\nEpoch 12/100\n191/191 [==============================] - 0s 3ms/step - loss: 16.8898 - binary_accuracy: 0.5166 - val_loss: 13.1816 - val_binary_accuracy: 0.5253\nEpoch 13/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.9538 - binary_accuracy: 0.5151 - val_loss: 14.4587 - val_binary_accuracy: 0.5391\nEpoch 14/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.0781 - binary_accuracy: 0.5085 - val_loss: 11.7277 - val_binary_accuracy: 0.5312\nEpoch 15/100\n191/191 [==============================] - 1s 3ms/step - loss: 13.2963 - binary_accuracy: 0.5072 - val_loss: 8.4516 - val_binary_accuracy: 0.5167\nEpoch 16/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.8151 - binary_accuracy: 0.5125 - val_loss: 13.2906 - val_binary_accuracy: 0.5351\nEpoch 17/100\n191/191 [==============================] - 1s 3ms/step - loss: 12.2239 - binary_accuracy: 0.5136 - val_loss: 10.8708 - val_binary_accuracy: 0.5378\nEpoch 18/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.7989 - binary_accuracy: 0.5046 - val_loss: 8.7974 - val_binary_accuracy: 0.5299\nEpoch 19/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5223 - binary_accuracy: 0.5103 - val_loss: 9.4426 - val_binary_accuracy: 0.5253\nEpoch 20/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5585 - binary_accuracy: 0.5122 - val_loss: 9.8687 - val_binary_accuracy: 0.5246\nEpoch 21/100\n191/191 [==============================] - 1s 3ms/step - loss: 9.2616 - binary_accuracy: 0.5167 - val_loss: 11.9928 - val_binary_accuracy: 0.5253\nEpoch 22/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.1322 - binary_accuracy: 0.5213 - val_loss: 7.8273 - val_binary_accuracy: 0.4990\nEpoch 23/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.4561 - binary_accuracy: 0.5200 - val_loss: 9.2781 - val_binary_accuracy: 0.5174\nEpoch 24/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.0963 - binary_accuracy: 0.5166 - val_loss: 5.4576 - val_binary_accuracy: 0.5167\nEpoch 25/100\n191/191 [==============================] - 1s 3ms/step - loss: 7.1907 - binary_accuracy: 0.5122 - val_loss: 8.8754 - val_binary_accuracy: 0.5003\nEpoch 26/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.9971 - binary_accuracy: 0.5190 - val_loss: 7.2822 - val_binary_accuracy: 0.5095\nEpoch 27/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.5726 - binary_accuracy: 0.5105 - val_loss: 5.3474 - val_binary_accuracy: 0.4879\nEpoch 28/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.0869 - binary_accuracy: 0.5243 - val_loss: 5.1194 - val_binary_accuracy: 0.5174\nEpoch 29/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.7851 - binary_accuracy: 0.5125 - val_loss: 6.9937 - val_binary_accuracy: 0.5076\nEpoch 30/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.1723 - binary_accuracy: 0.5148 - val_loss: 5.7589 - val_binary_accuracy: 0.5062\nEpoch 31/100\n191/191 [==============================] - 1s 4ms/step - loss: 5.1466 - binary_accuracy: 0.5125 - val_loss: 3.9673 - val_binary_accuracy: 0.5181\nEpoch 32/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.2948 - binary_accuracy: 0.5186 - val_loss: 5.4504 - val_binary_accuracy: 0.5089\nEpoch 33/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.4029 - binary_accuracy: 0.5095 - val_loss: 2.8821 - val_binary_accuracy: 0.5128\nEpoch 34/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.0808 - binary_accuracy: 0.5099 - val_loss: 3.8832 - val_binary_accuracy: 0.5161\nEpoch 35/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7501 - binary_accuracy: 0.5174 - val_loss: 3.9605 - val_binary_accuracy: 0.5135\nEpoch 36/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7382 - binary_accuracy: 0.5102 - val_loss: 3.6448 - val_binary_accuracy: 0.5115\nEpoch 37/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.1055 - binary_accuracy: 0.5245 - val_loss: 3.9398 - val_binary_accuracy: 0.5056\nEpoch 38/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.9880 - binary_accuracy: 0.5151 - val_loss: 2.8102 - val_binary_accuracy: 0.5082\nEpoch 39/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.7847 - binary_accuracy: 0.5215 - val_loss: 2.1289 - val_binary_accuracy: 0.5187\nEpoch 40/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.8415 - binary_accuracy: 0.5241 - val_loss: 2.8795 - val_binary_accuracy: 0.5115\nEpoch 41/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.5144 - binary_accuracy: 0.5223 - val_loss: 2.5149 - val_binary_accuracy: 0.5213\nEpoch 42/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0964 - binary_accuracy: 0.5287 - val_loss: 2.0305 - val_binary_accuracy: 0.5207\nEpoch 43/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.1527 - binary_accuracy: 0.5158 - val_loss: 1.8447 - val_binary_accuracy: 0.5095\nEpoch 44/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0041 - binary_accuracy: 0.5296 - val_loss: 1.7671 - val_binary_accuracy: 0.5095\nEpoch 45/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.7718 - binary_accuracy: 0.5232 - val_loss: 2.0167 - val_binary_accuracy: 0.5121\nEpoch 46/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.8157 - binary_accuracy: 0.5159 - val_loss: 1.7764 - val_binary_accuracy: 0.5227\nEpoch 47/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6907 - binary_accuracy: 0.5281 - val_loss: 1.3804 - val_binary_accuracy: 0.5233\nEpoch 48/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.5388 - binary_accuracy: 0.5233 - val_loss: 1.4022 - val_binary_accuracy: 0.5213\nEpoch 49/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6031 - binary_accuracy: 0.5259 - val_loss: 1.3513 - val_binary_accuracy: 0.5253\nEpoch 50/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.3865 - binary_accuracy: 0.5271 - val_loss: 1.1486 - val_binary_accuracy: 0.5246\nEpoch 51/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2101 - binary_accuracy: 0.5269 - val_loss: 1.0696 - val_binary_accuracy: 0.5227\nEpoch 52/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2028 - binary_accuracy: 0.5368 - val_loss: 1.1062 - val_binary_accuracy: 0.5292\nEpoch 53/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1968 - binary_accuracy: 0.5332 - val_loss: 0.9421 - val_binary_accuracy: 0.5318\nEpoch 54/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1506 - binary_accuracy: 0.5340 - val_loss: 1.0106 - val_binary_accuracy: 0.5338\nEpoch 55/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2173 - binary_accuracy: 0.5281 - val_loss: 0.9825 - val_binary_accuracy: 0.5378\nEpoch 56/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1079 - binary_accuracy: 0.5296 - val_loss: 0.8303 - val_binary_accuracy: 0.5515\nEpoch 57/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9980 - binary_accuracy: 0.5378 - val_loss: 0.8406 - val_binary_accuracy: 0.5456\nEpoch 58/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1121 - binary_accuracy: 0.5363 - val_loss: 1.8118 - val_binary_accuracy: 0.5548\nEpoch 59/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0127 - binary_accuracy: 0.5388 - val_loss: 0.9639 - val_binary_accuracy: 0.5483\nEpoch 60/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9924 - binary_accuracy: 0.5374 - val_loss: 0.8626 - val_binary_accuracy: 0.5483\nEpoch 61/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8680 - binary_accuracy: 0.5399 - val_loss: 0.8083 - val_binary_accuracy: 0.5542\nEpoch 62/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5440 - val_loss: 1.2377 - val_binary_accuracy: 0.5581\nEpoch 63/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9144 - binary_accuracy: 0.5396 - val_loss: 0.7873 - val_binary_accuracy: 0.5522\nEpoch 64/100\n191/191 [==============================] - 0s 3ms/step - loss: 0.9946 - binary_accuracy: 0.5343 - val_loss: 0.8474 - val_binary_accuracy: 0.5463\nEpoch 65/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9548 - binary_accuracy: 0.5365 - val_loss: 0.7671 - val_binary_accuracy: 0.5568\nEpoch 66/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9172 - binary_accuracy: 0.5432 - val_loss: 1.1670 - val_binary_accuracy: 0.5535\nEpoch 67/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9938 - binary_accuracy: 0.5473 - val_loss: 1.7050 - val_binary_accuracy: 0.5575\nEpoch 68/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8954 - binary_accuracy: 0.5394 - val_loss: 0.7633 - val_binary_accuracy: 0.5483\nEpoch 69/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8815 - binary_accuracy: 0.5460 - val_loss: 0.9965 - val_binary_accuracy: 0.5561\nEpoch 70/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5438 - val_loss: 1.0431 - val_binary_accuracy: 0.5509\nEpoch 71/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8825 - binary_accuracy: 0.5483 - val_loss: 1.0019 - val_binary_accuracy: 0.5601\nEpoch 72/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9897 - binary_accuracy: 0.5479 - val_loss: 1.3509 - val_binary_accuracy: 0.5502\nEpoch 73/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9775 - binary_accuracy: 0.5427 - val_loss: 0.9652 - val_binary_accuracy: 0.5581\nEpoch 74/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0205 - binary_accuracy: 0.5445 - val_loss: 1.2212 - val_binary_accuracy: 0.5614\nEpoch 75/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0616 - binary_accuracy: 0.5432 - val_loss: 1.1886 - val_binary_accuracy: 0.5575\nEpoch 76/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9314 - binary_accuracy: 0.5468 - val_loss: 1.3687 - val_binary_accuracy: 0.5548\nEpoch 77/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8521 - binary_accuracy: 0.5470 - val_loss: 0.7695 - val_binary_accuracy: 0.5548\nEpoch 78/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8151 - binary_accuracy: 0.5517 - val_loss: 0.9690 - val_binary_accuracy: 0.5509\nEpoch 79/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9553 - binary_accuracy: 0.5453 - val_loss: 0.8552 - val_binary_accuracy: 0.5575\nEpoch 80/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8296 - binary_accuracy: 0.5512 - val_loss: 0.7398 - val_binary_accuracy: 0.5555\nEpoch 81/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8471 - binary_accuracy: 0.5548 - val_loss: 0.7572 - val_binary_accuracy: 0.5529\nEpoch 82/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8430 - binary_accuracy: 0.5456 - val_loss: 0.7894 - val_binary_accuracy: 0.5575\nEpoch 83/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8508 - binary_accuracy: 0.5471 - val_loss: 0.9579 - val_binary_accuracy: 0.5476\nEpoch 84/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8445 - binary_accuracy: 0.5537 - val_loss: 0.8346 - val_binary_accuracy: 0.5581\nEpoch 85/100\n191/191 [==============================] - 1s 4ms/step - loss: 0.8542 - binary_accuracy: 0.5511 - val_loss: 0.7923 - val_binary_accuracy: 0.5542\nEpoch 86/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9084 - binary_accuracy: 0.5481 - val_loss: 0.9421 - val_binary_accuracy: 0.5575\nEpoch 87/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9367 - binary_accuracy: 0.5463 - val_loss: 0.9959 - val_binary_accuracy: 0.5450\nEpoch 88/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9629 - binary_accuracy: 0.5498 - val_loss: 1.1058 - val_binary_accuracy: 0.5607\nEpoch 89/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9141 - binary_accuracy: 0.5365 - val_loss: 0.7433 - val_binary_accuracy: 0.5561\nEpoch 90/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8145 - binary_accuracy: 0.5478 - val_loss: 0.7720 - val_binary_accuracy: 0.5529\nEpoch 91/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.7985 - binary_accuracy: 0.5560 - val_loss: 0.7784 - val_binary_accuracy: 0.5581\nEpoch 92/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8700 - binary_accuracy: 0.5468 - val_loss: 0.8171 - val_binary_accuracy: 0.5594\nEpoch 93/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8868 - binary_accuracy: 0.5502 - val_loss: 0.9612 - val_binary_accuracy: 0.5594\nEpoch 94/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8591 - binary_accuracy: 0.5493 - val_loss: 0.8276 - val_binary_accuracy: 0.5581\nEpoch 95/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8683 - binary_accuracy: 0.5435 - val_loss: 0.8003 - val_binary_accuracy: 0.5581\nEpoch 96/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8546 - binary_accuracy: 0.5415 - val_loss: 0.7477 - val_binary_accuracy: 0.5555\nEpoch 97/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9054 - binary_accuracy: 0.5484 - val_loss: 1.2473 - val_binary_accuracy: 0.5575\nEpoch 98/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8245 - binary_accuracy: 0.5479 - val_loss: 0.7795 - val_binary_accuracy: 0.5515\nEpoch 99/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8624 - binary_accuracy: 0.5435 - val_loss: 0.9183 - val_binary_accuracy: 0.5529\nEpoch 100/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8200 - binary_accuracy: 0.5470 - val_loss: 0.7893 - val_binary_accuracy: 0.5509\n\n\n\n\n\nNow we will predict the test to do the submission\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[-0.83098686],\n       [-0.71497   ],\n       [-0.4031287 ],\n       ...,\n       [-0.3488861 ],\n       [-0.4957207 ],\n       [-1.3229517 ]], dtype=float32)\n\n\nüìù Note that those are the logits!\nTo get a prediction we will compute the sigmoid function and round it to 1 or 0! (Thats because they are 2 classes, if there would be multi class classification then we would need Softmax Function)\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds)) # We use simple round function\ntest_df[\"target\"] = test_df[\"target\"].astype(int) # Submission needs int prediction\n\n\ntest_df.target.value_counts()\n\n0    3023\n1     240\nName: target, dtype: int64\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n0\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n9\n0\n\n\n4\n11\n0\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n0\n\n\n3260\n10868\n0\n\n\n3261\n10874\n0\n\n\n3262\n10875\n0\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\n#sub.to_csv(\"NN_submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#libraries",
    "href": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#libraries",
    "title": "1. Simple Neural Network",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#data",
    "href": "posts/kaggle_nlp_disaster/seqclass-1-simple-nn.html#data",
    "title": "1. Simple Neural Network",
    "section": "",
    "text": "# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-11 14:55:06.219408: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\nInstantiate TextVectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 50 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-11 14:55:06.451771: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\n\n# Testing the Vectorization layer\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return vectorization_layer(text)\n\n# retrieve a batch (of 1 review and label) from the dataset\ntext_batch, label_batch = next(iter(train_ds))\nfirst_review, first_label = text_batch, label_batch\nprint(\"Review\", first_review)\nprint(\"Label\", first_label)\nprint(\"Vectorized review\", vectorize_text(first_review))\n\nReview tf.Tensor(b'Riot Kit Bah - part of the new concept Gear coming for Autumn/Winter\\n#menswear #fashion #urbanfashion\\xc2\\x89\\xc3\\x9b_ https://t.co/cCwzDTFbUS', shape=(), dtype=string)\nLabel tf.Tensor(0, shape=(), dtype=int64)\nVectorized review tf.Tensor(\n[[ 403 1278    1  572    6    2   44 3700    1  250   10    1    1  968\n  6559    1    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0]], shape=(1, 50), dtype=int64)\n\n\nüîç We see that the length of the vector is output_sequence_length = 250, and if the statement is not that long, it is paded with 0. We also note that no integer is greater than 10000, since we set this as our maximum vocabulary of tokens (vocabulary_size()). We also see that there are some integers = 1. This in the vocabulary corresponds to UNK, that is, unknown words (tokens), this is because our vocabulary is limited (TextVectorization chooses the 10000 most frequent ones).\nAn improvement can be to analyze the length of the dataset sentences and choose an output_sequence_lenght based on this (to avoid to much pad)\n\nvectorization_layer.vocabulary_size()\n\n10000\n\n\n\n# To see the vocabulary\nvectorization_layer.get_vocabulary()[:10]\n\n['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']\n\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nWe will build a simple Sequential Model composed by two Dense Layers. I dont take to much time tunning it.\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string), # Input Layer\n    vectorization_layer, # Vectorization Layer\n    layers.Dense(64),\n    #layers.Dropout(0.1),\n    layers.Dense(32),\n    #layers.Dropout(0.1),\n    layers.Dense(1)\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 50)                0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                3264      \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 5,377\nTrainable params: 5,377\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\nüîç Due to our last Dense Layer has a linear activation function, it computes the logits. So the loss function needs to be computed with from_logits=True.\n\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs)\n\nEpoch 1/100\n191/191 [==============================] - 2s 4ms/step - loss: 76.1297 - binary_accuracy: 0.5123 - val_loss: 30.4719 - val_binary_accuracy: 0.5207\nEpoch 2/100\n191/191 [==============================] - 1s 3ms/step - loss: 37.2703 - binary_accuracy: 0.5167 - val_loss: 22.0519 - val_binary_accuracy: 0.5194\nEpoch 3/100\n191/191 [==============================] - 1s 3ms/step - loss: 34.7663 - binary_accuracy: 0.5118 - val_loss: 27.2791 - val_binary_accuracy: 0.5141\nEpoch 4/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.8605 - binary_accuracy: 0.5135 - val_loss: 26.1287 - val_binary_accuracy: 0.5253\nEpoch 5/100\n191/191 [==============================] - 1s 3ms/step - loss: 28.4162 - binary_accuracy: 0.5123 - val_loss: 24.3497 - val_binary_accuracy: 0.5108\nEpoch 6/100\n191/191 [==============================] - 1s 3ms/step - loss: 26.5699 - binary_accuracy: 0.5084 - val_loss: 17.0479 - val_binary_accuracy: 0.5148\nEpoch 7/100\n191/191 [==============================] - 1s 3ms/step - loss: 24.1579 - binary_accuracy: 0.5136 - val_loss: 24.3595 - val_binary_accuracy: 0.5351\nEpoch 8/100\n191/191 [==============================] - 1s 3ms/step - loss: 22.3645 - binary_accuracy: 0.5110 - val_loss: 18.3034 - val_binary_accuracy: 0.5108\nEpoch 9/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.4826 - binary_accuracy: 0.5059 - val_loss: 17.5856 - val_binary_accuracy: 0.5161\nEpoch 10/100\n191/191 [==============================] - 1s 3ms/step - loss: 19.1403 - binary_accuracy: 0.5156 - val_loss: 15.4736 - val_binary_accuracy: 0.5299\nEpoch 11/100\n191/191 [==============================] - 1s 3ms/step - loss: 16.6029 - binary_accuracy: 0.5156 - val_loss: 13.5105 - val_binary_accuracy: 0.5220\nEpoch 12/100\n191/191 [==============================] - 0s 3ms/step - loss: 16.8898 - binary_accuracy: 0.5166 - val_loss: 13.1816 - val_binary_accuracy: 0.5253\nEpoch 13/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.9538 - binary_accuracy: 0.5151 - val_loss: 14.4587 - val_binary_accuracy: 0.5391\nEpoch 14/100\n191/191 [==============================] - 1s 3ms/step - loss: 15.0781 - binary_accuracy: 0.5085 - val_loss: 11.7277 - val_binary_accuracy: 0.5312\nEpoch 15/100\n191/191 [==============================] - 1s 3ms/step - loss: 13.2963 - binary_accuracy: 0.5072 - val_loss: 8.4516 - val_binary_accuracy: 0.5167\nEpoch 16/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.8151 - binary_accuracy: 0.5125 - val_loss: 13.2906 - val_binary_accuracy: 0.5351\nEpoch 17/100\n191/191 [==============================] - 1s 3ms/step - loss: 12.2239 - binary_accuracy: 0.5136 - val_loss: 10.8708 - val_binary_accuracy: 0.5378\nEpoch 18/100\n191/191 [==============================] - 1s 3ms/step - loss: 11.7989 - binary_accuracy: 0.5046 - val_loss: 8.7974 - val_binary_accuracy: 0.5299\nEpoch 19/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5223 - binary_accuracy: 0.5103 - val_loss: 9.4426 - val_binary_accuracy: 0.5253\nEpoch 20/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.5585 - binary_accuracy: 0.5122 - val_loss: 9.8687 - val_binary_accuracy: 0.5246\nEpoch 21/100\n191/191 [==============================] - 1s 3ms/step - loss: 9.2616 - binary_accuracy: 0.5167 - val_loss: 11.9928 - val_binary_accuracy: 0.5253\nEpoch 22/100\n191/191 [==============================] - 1s 3ms/step - loss: 10.1322 - binary_accuracy: 0.5213 - val_loss: 7.8273 - val_binary_accuracy: 0.4990\nEpoch 23/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.4561 - binary_accuracy: 0.5200 - val_loss: 9.2781 - val_binary_accuracy: 0.5174\nEpoch 24/100\n191/191 [==============================] - 1s 3ms/step - loss: 8.0963 - binary_accuracy: 0.5166 - val_loss: 5.4576 - val_binary_accuracy: 0.5167\nEpoch 25/100\n191/191 [==============================] - 1s 3ms/step - loss: 7.1907 - binary_accuracy: 0.5122 - val_loss: 8.8754 - val_binary_accuracy: 0.5003\nEpoch 26/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.9971 - binary_accuracy: 0.5190 - val_loss: 7.2822 - val_binary_accuracy: 0.5095\nEpoch 27/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.5726 - binary_accuracy: 0.5105 - val_loss: 5.3474 - val_binary_accuracy: 0.4879\nEpoch 28/100\n191/191 [==============================] - 1s 3ms/step - loss: 6.0869 - binary_accuracy: 0.5243 - val_loss: 5.1194 - val_binary_accuracy: 0.5174\nEpoch 29/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.7851 - binary_accuracy: 0.5125 - val_loss: 6.9937 - val_binary_accuracy: 0.5076\nEpoch 30/100\n191/191 [==============================] - 1s 3ms/step - loss: 5.1723 - binary_accuracy: 0.5148 - val_loss: 5.7589 - val_binary_accuracy: 0.5062\nEpoch 31/100\n191/191 [==============================] - 1s 4ms/step - loss: 5.1466 - binary_accuracy: 0.5125 - val_loss: 3.9673 - val_binary_accuracy: 0.5181\nEpoch 32/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.2948 - binary_accuracy: 0.5186 - val_loss: 5.4504 - val_binary_accuracy: 0.5089\nEpoch 33/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.4029 - binary_accuracy: 0.5095 - val_loss: 2.8821 - val_binary_accuracy: 0.5128\nEpoch 34/100\n191/191 [==============================] - 1s 3ms/step - loss: 4.0808 - binary_accuracy: 0.5099 - val_loss: 3.8832 - val_binary_accuracy: 0.5161\nEpoch 35/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7501 - binary_accuracy: 0.5174 - val_loss: 3.9605 - val_binary_accuracy: 0.5135\nEpoch 36/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.7382 - binary_accuracy: 0.5102 - val_loss: 3.6448 - val_binary_accuracy: 0.5115\nEpoch 37/100\n191/191 [==============================] - 1s 3ms/step - loss: 3.1055 - binary_accuracy: 0.5245 - val_loss: 3.9398 - val_binary_accuracy: 0.5056\nEpoch 38/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.9880 - binary_accuracy: 0.5151 - val_loss: 2.8102 - val_binary_accuracy: 0.5082\nEpoch 39/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.7847 - binary_accuracy: 0.5215 - val_loss: 2.1289 - val_binary_accuracy: 0.5187\nEpoch 40/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.8415 - binary_accuracy: 0.5241 - val_loss: 2.8795 - val_binary_accuracy: 0.5115\nEpoch 41/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.5144 - binary_accuracy: 0.5223 - val_loss: 2.5149 - val_binary_accuracy: 0.5213\nEpoch 42/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0964 - binary_accuracy: 0.5287 - val_loss: 2.0305 - val_binary_accuracy: 0.5207\nEpoch 43/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.1527 - binary_accuracy: 0.5158 - val_loss: 1.8447 - val_binary_accuracy: 0.5095\nEpoch 44/100\n191/191 [==============================] - 1s 3ms/step - loss: 2.0041 - binary_accuracy: 0.5296 - val_loss: 1.7671 - val_binary_accuracy: 0.5095\nEpoch 45/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.7718 - binary_accuracy: 0.5232 - val_loss: 2.0167 - val_binary_accuracy: 0.5121\nEpoch 46/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.8157 - binary_accuracy: 0.5159 - val_loss: 1.7764 - val_binary_accuracy: 0.5227\nEpoch 47/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6907 - binary_accuracy: 0.5281 - val_loss: 1.3804 - val_binary_accuracy: 0.5233\nEpoch 48/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.5388 - binary_accuracy: 0.5233 - val_loss: 1.4022 - val_binary_accuracy: 0.5213\nEpoch 49/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.6031 - binary_accuracy: 0.5259 - val_loss: 1.3513 - val_binary_accuracy: 0.5253\nEpoch 50/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.3865 - binary_accuracy: 0.5271 - val_loss: 1.1486 - val_binary_accuracy: 0.5246\nEpoch 51/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2101 - binary_accuracy: 0.5269 - val_loss: 1.0696 - val_binary_accuracy: 0.5227\nEpoch 52/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2028 - binary_accuracy: 0.5368 - val_loss: 1.1062 - val_binary_accuracy: 0.5292\nEpoch 53/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1968 - binary_accuracy: 0.5332 - val_loss: 0.9421 - val_binary_accuracy: 0.5318\nEpoch 54/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1506 - binary_accuracy: 0.5340 - val_loss: 1.0106 - val_binary_accuracy: 0.5338\nEpoch 55/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.2173 - binary_accuracy: 0.5281 - val_loss: 0.9825 - val_binary_accuracy: 0.5378\nEpoch 56/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1079 - binary_accuracy: 0.5296 - val_loss: 0.8303 - val_binary_accuracy: 0.5515\nEpoch 57/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9980 - binary_accuracy: 0.5378 - val_loss: 0.8406 - val_binary_accuracy: 0.5456\nEpoch 58/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.1121 - binary_accuracy: 0.5363 - val_loss: 1.8118 - val_binary_accuracy: 0.5548\nEpoch 59/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0127 - binary_accuracy: 0.5388 - val_loss: 0.9639 - val_binary_accuracy: 0.5483\nEpoch 60/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9924 - binary_accuracy: 0.5374 - val_loss: 0.8626 - val_binary_accuracy: 0.5483\nEpoch 61/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8680 - binary_accuracy: 0.5399 - val_loss: 0.8083 - val_binary_accuracy: 0.5542\nEpoch 62/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5440 - val_loss: 1.2377 - val_binary_accuracy: 0.5581\nEpoch 63/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9144 - binary_accuracy: 0.5396 - val_loss: 0.7873 - val_binary_accuracy: 0.5522\nEpoch 64/100\n191/191 [==============================] - 0s 3ms/step - loss: 0.9946 - binary_accuracy: 0.5343 - val_loss: 0.8474 - val_binary_accuracy: 0.5463\nEpoch 65/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9548 - binary_accuracy: 0.5365 - val_loss: 0.7671 - val_binary_accuracy: 0.5568\nEpoch 66/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9172 - binary_accuracy: 0.5432 - val_loss: 1.1670 - val_binary_accuracy: 0.5535\nEpoch 67/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9938 - binary_accuracy: 0.5473 - val_loss: 1.7050 - val_binary_accuracy: 0.5575\nEpoch 68/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8954 - binary_accuracy: 0.5394 - val_loss: 0.7633 - val_binary_accuracy: 0.5483\nEpoch 69/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8815 - binary_accuracy: 0.5460 - val_loss: 0.9965 - val_binary_accuracy: 0.5561\nEpoch 70/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8643 - binary_accuracy: 0.5438 - val_loss: 1.0431 - val_binary_accuracy: 0.5509\nEpoch 71/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8825 - binary_accuracy: 0.5483 - val_loss: 1.0019 - val_binary_accuracy: 0.5601\nEpoch 72/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9897 - binary_accuracy: 0.5479 - val_loss: 1.3509 - val_binary_accuracy: 0.5502\nEpoch 73/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9775 - binary_accuracy: 0.5427 - val_loss: 0.9652 - val_binary_accuracy: 0.5581\nEpoch 74/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0205 - binary_accuracy: 0.5445 - val_loss: 1.2212 - val_binary_accuracy: 0.5614\nEpoch 75/100\n191/191 [==============================] - 1s 3ms/step - loss: 1.0616 - binary_accuracy: 0.5432 - val_loss: 1.1886 - val_binary_accuracy: 0.5575\nEpoch 76/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9314 - binary_accuracy: 0.5468 - val_loss: 1.3687 - val_binary_accuracy: 0.5548\nEpoch 77/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8521 - binary_accuracy: 0.5470 - val_loss: 0.7695 - val_binary_accuracy: 0.5548\nEpoch 78/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8151 - binary_accuracy: 0.5517 - val_loss: 0.9690 - val_binary_accuracy: 0.5509\nEpoch 79/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9553 - binary_accuracy: 0.5453 - val_loss: 0.8552 - val_binary_accuracy: 0.5575\nEpoch 80/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8296 - binary_accuracy: 0.5512 - val_loss: 0.7398 - val_binary_accuracy: 0.5555\nEpoch 81/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8471 - binary_accuracy: 0.5548 - val_loss: 0.7572 - val_binary_accuracy: 0.5529\nEpoch 82/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8430 - binary_accuracy: 0.5456 - val_loss: 0.7894 - val_binary_accuracy: 0.5575\nEpoch 83/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8508 - binary_accuracy: 0.5471 - val_loss: 0.9579 - val_binary_accuracy: 0.5476\nEpoch 84/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8445 - binary_accuracy: 0.5537 - val_loss: 0.8346 - val_binary_accuracy: 0.5581\nEpoch 85/100\n191/191 [==============================] - 1s 4ms/step - loss: 0.8542 - binary_accuracy: 0.5511 - val_loss: 0.7923 - val_binary_accuracy: 0.5542\nEpoch 86/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9084 - binary_accuracy: 0.5481 - val_loss: 0.9421 - val_binary_accuracy: 0.5575\nEpoch 87/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9367 - binary_accuracy: 0.5463 - val_loss: 0.9959 - val_binary_accuracy: 0.5450\nEpoch 88/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9629 - binary_accuracy: 0.5498 - val_loss: 1.1058 - val_binary_accuracy: 0.5607\nEpoch 89/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9141 - binary_accuracy: 0.5365 - val_loss: 0.7433 - val_binary_accuracy: 0.5561\nEpoch 90/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8145 - binary_accuracy: 0.5478 - val_loss: 0.7720 - val_binary_accuracy: 0.5529\nEpoch 91/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.7985 - binary_accuracy: 0.5560 - val_loss: 0.7784 - val_binary_accuracy: 0.5581\nEpoch 92/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8700 - binary_accuracy: 0.5468 - val_loss: 0.8171 - val_binary_accuracy: 0.5594\nEpoch 93/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8868 - binary_accuracy: 0.5502 - val_loss: 0.9612 - val_binary_accuracy: 0.5594\nEpoch 94/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8591 - binary_accuracy: 0.5493 - val_loss: 0.8276 - val_binary_accuracy: 0.5581\nEpoch 95/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8683 - binary_accuracy: 0.5435 - val_loss: 0.8003 - val_binary_accuracy: 0.5581\nEpoch 96/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8546 - binary_accuracy: 0.5415 - val_loss: 0.7477 - val_binary_accuracy: 0.5555\nEpoch 97/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.9054 - binary_accuracy: 0.5484 - val_loss: 1.2473 - val_binary_accuracy: 0.5575\nEpoch 98/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8245 - binary_accuracy: 0.5479 - val_loss: 0.7795 - val_binary_accuracy: 0.5515\nEpoch 99/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8624 - binary_accuracy: 0.5435 - val_loss: 0.9183 - val_binary_accuracy: 0.5529\nEpoch 100/100\n191/191 [==============================] - 1s 3ms/step - loss: 0.8200 - binary_accuracy: 0.5470 - val_loss: 0.7893 - val_binary_accuracy: 0.5509\n\n\n\n\n\nNow we will predict the test to do the submission\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds\n\narray([[-0.83098686],\n       [-0.71497   ],\n       [-0.4031287 ],\n       ...,\n       [-0.3488861 ],\n       [-0.4957207 ],\n       [-1.3229517 ]], dtype=float32)\n\n\nüìù Note that those are the logits!\nTo get a prediction we will compute the sigmoid function and round it to 1 or 0! (Thats because they are 2 classes, if there would be multi class classification then we would need Softmax Function)\n\ntest_df[\"target\"] = tf.round(tf.nn.sigmoid(preds)) # We use simple round function\ntest_df[\"target\"] = test_df[\"target\"].astype(int) # Submission needs int prediction\n\n\ntest_df.target.value_counts()\n\n0    3023\n1     240\nName: target, dtype: int64\n\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n0\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n9\n0\n\n\n4\n11\n0\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n0\n\n\n3260\n10868\n0\n\n\n3261\n10874\n0\n\n\n3262\n10875\n0\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\n#sub.to_csv(\"NN_submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/llm-cv-assistant/main.html#document-as-context",
    "href": "posts/llm-cv-assistant/main.html#document-as-context",
    "title": "Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum",
    "section": "Document As Context",
    "text": "Document As Context\nDesde una vista general, los LLM utilizan la informaci√≥n de documentos como contexto. Imaginemos tenemos un documento de texto (un PDF) bastante simple que contiene algo como:\n\nDiego tiene 25 a√±os, es Ingeniero Civil Industrial, le gusta ver anim√©, jugar videojuegos, ir al gimnasio y jugar p√°del.\n\nLuego, si un usuario pregunta:\n\n\n\n\n\n\nNoteüëµüèΩ\n\n\n\nHola, a diego le gusta hacer deporte?\n\n\nSi usamos el LLM como tal, sin entregarle el documento, el modelo responder√≠a algo como:\n\n\n\n\n\n\nNoteü§ñ\n\n\n\nY quien car***o es diego?\n\n\nQuiz√° exager√© un poco, pero se entiende. El LLM no tiene la informaci√≥n del documento que necesito considere. Como mencion√© anteriormente, √©sto se soluciona agregando la informaci√≥n como contexto en el prompt, quedando algo por el estilo:\n\n\n\n\n\n\nNoteüëµüèΩ\n\n\n\nUtiliza las siguientes piezas de contexto para responder la pregunta al final. Por favor si no sabes la respuesta, s√≥lo di que no sabes, no intentes inventarla.\nDiego tiene 25 a√±os, es Ingeniero Civil Industrial, le gusta ver anim√©, jugar videojuegos, ir al gimnasio y jugar p√°del.\nPregunta: Hola, a diego le gusta hacer deporte?\nRespuesta:\n\n\nFijemonos que se le agregaron tanto instrucciones y contexto. Dejando que luego de eso el LLM responda la pregunta deseada tal y como lo pregunt√≥ el/la usuaria.\nHasta ahora bastante sencillo cierto? Apuesto a que imaginabas que por detr√°s se volv√≠a a entrenar el modelo agreg√°ndole preguntas y respuestas del documento ‚Ä¶ blablabla. no? porque yo si lo pensaba üòÜ\nVeamos un diagrama de lo que tendr√≠amos por el momento:\n\n\n\nImage by diegulio\n\n\nHasta ahora todo bien, pero no debemos olvidar una limitante importante en los prompt de los LLM. El famoso context_length ! Este canalla (como dir√≠a mi abuela üëµüèº) nos restringe la cantidad de caracteres, tokens, etc que podemos ingresar en el prompt. Entonces cuando tenemos documentos muy grandes y con gran cantidad de caracteres, que hacemos?"
  },
  {
    "objectID": "posts/llm-cv-assistant/main.html#data-connection",
    "href": "posts/llm-cv-assistant/main.html#data-connection",
    "title": "Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum",
    "section": "Data Connection",
    "text": "Data Connection\nYa sabemos que suponiendo un documento tan grande que sobrepase el context_lenght no podremos agregar esta informaci√≥n como contexto a nuestro prompt. Una opci√≥n v√°lida ser√≠a simplemente tomar un extracto aleatorio del documento e insertarlo como contexto. Lamentablemente ser√° muy probable que el contexto adecuado para responder la pregunta del usuario no se encuentre en el extracto aleatorio.\nüí°¬†Una mejor idea seria extraer partes del documento que se relacionen con la pregunta. Entonces la estrategia queda como:\n\nLoad Documents: Cargar los documentos\nSplit Documents: Dividir los documentos en piezas de texto\nEmbedding: Extraer features de las piezas de texto\nVector Store: Guardar features en una base de datos para utilizarlo despu√©s.\n\nAc√° les presento un diagrama que simboliza este proceso:\n\n\n\nImage by diegulio\n\n\nEl objetivo final es poder representar cada pieza (chunk) de texto de forma sem√°ntica, para as√≠ poder hacer la relaci√≥n con la pregunta del usuario, y decidir que chunks incluir en el prompt final. Ahora naveguemos un poco por cada paso:\n\n1. Load Documents\nEsta etapa es bastante simple, s√≥lo requiere poder pasar los documentos a texto. Langchain posee una gran variedad de m√©todos para hacer esto, ac√° te dejo la documentaci√≥n.\nEn nuestro caso es poder tomar un CV en PDF y extraer el texto de este. Tambi√©n podemos extraer texto de DataFrames, Json, Latex, Wikipedia, etc.\n\n\n2. Split Documents\nExisten varias formas para dividir documentos, puede ser por caracteres, por elementos Markdowns, tokens, etc. Creo que no vale la pena que yo te lo explique cuando existe una buena documentaci√≥n para aquello.\nAc√° hay dos conceptos que pueden ser importantes, que son el tama√±o del chunk chunk_size y el tama√±o del overlap en los distintos chunks overlap_size. Si el tama√±o del chunk es muy peque√±o entonces puede ser dif√≠cil extraer un buen contexto, pero si por el contrario es muy grande puede que estemos extrayendo informaci√≥n poco valiosa, y adem√°s arriesg√°ndonos podemos sobrepasar el context_length.\nEn general, los divisores de texto funcionan de la siguiente manera:\n\nDividir el texto en peque√±os fragmentos sem√°nticamente significativos (a menudo oraciones).\nComenzar a combinar estos peque√±os fragmentos en un fragmento m√°s grande hasta que alcance un cierto tama√±o chunk_size (seg√∫n lo medido por alguna funci√≥n).\nUna vez que alcance ese tama√±o, hace que ese fragmento sea su propio fragmento de texto y luego comience a crear un nuevo fragmento de texto con algo de superposici√≥n overlap_size (para mantener el contexto entre los fragmentos).\n\n\n\n3. Embeddings\nEl concepto de Embeddings es muy importante en Machine Learning en general. Siendo muy utilizados cuando hablamos de textos. Puedes ver como en el post Identificando desastres en Twitter con NLP utilizamos embeddings para representar texto de forma num√©rica (usando vectores en este caso).\nPodr√≠a escribir un blog entero sobre esto, pero creo que encontrar√°s mejor informaci√≥n en internet. Ac√° te dejo con un starter que puede ser la vieja y confiable Wikipedia.\nCreo que s√≥lo nos basta con saber que podemos representar tanto palabras, car√°cteres, sentencias, documentos, etc con vectores. Adem√°s podemos calcular medidas de similaridad entre estos utilizando operaciones vectoriales, como el conocido producto punto, o calculando el coseno del √°ngulo entre los vectores. Vuelvo a repetir que este concepto es muy importante y que si lo desconoces debes ya ir a darle unas vueltas! üöÄ\n\n\n4. Vector Stores\nImaginemos que tenemos este gran documento, el cual dividimos en distintos chunks y transformamos a vectores. Muchas veces el/los documentos ser√°n tan grandes que ni siquiera cabr√°n en la memoria de nuestro computador (en la RAM). Es por esto que se utilizan los llamados Vector Stores, que son almacenamientos de embeddings. Ac√° podremos guardar cada palabra, sentecia, documento con su respectivo embedding, para luego simplemente consultar esta base de datos y no tener que calcular el embedding todo el tiempo.\nExisten vector stores que almacenan esta informaci√≥n en la nube, podr√°s encontrar compa√±√≠as que ofrecen estos servicios como Pinecone, Weviate, GCP con Matching Engine. Hay otros como FAISS por Facebook AI e incluso algunos que almacenan la informaci√≥n en la memoria RAM (si es posible) como Chroma.\nAdem√°s, estas herramientas no s√≥lo ofrecen almacenar esta informaci√≥n, si no que tambi√©n calcular la relaci√≥n sem√°ntica entre alguna frase, pregunta, prompt y los vectores presentes en la base datos de forma muy eficiente.\n\n\n\nSource: https://python.langchain.com/docs/modules/data_connection/vectorstores/\n\n\nOtro punto importante a considerar ac√° son los distintos m√©todos disponibles para calcular la similaridad entre la query y los vectores para obtener aquellos m√°s similares."
  },
  {
    "objectID": "posts/llm-cv-assistant/main.html#retrievers",
    "href": "posts/llm-cv-assistant/main.html#retrievers",
    "title": "Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum",
    "section": "Retrievers",
    "text": "Retrievers\nVolvamos al diagrama que teniamos hace un rato:\n\n\n\nImage by diegulio\n\n\nHabiamos quedado estancados con el hecho de que algunos documentos podrian no caber en el contexto entregado al prompt debidos al limitado context_length.\nAhora dividimos el documento, los convertimos a vectores, los almacenamos y adem√°s tenemos t√©cnicas para calcular la similaridad entre una query y los distintos chunks.\nEntonces nuestra misi√≥n se resume a: En base a una pregunta, obtener las piezas de texto (chunks) m√°s relevantes para incluirlos en el prompt final.\nPara entenderlo mejor, consideremos como ejemplo nuestro caso de uso para chatear con los curriculums, tenemos la pregunta del usuario:\n\n\n\n\n\n\nNoteüëµüèΩ\n\n\n\nWhat is the candidate name?\n\n\nE imaginemos que tenemos un CV muy grande (poco recomendable), algo del estilo:\nNombre: Diego Machado\nEdad: 25 a√±os\nHobbies: Ir al gym, jugar p√°del, jugar videojuegos\n\n.... texto .... bla bla ..\n\nDiego Machado estudi√≥ en .... bla bla\n\n.... texto ...\n\nTrabaj√≥ 1 a√±o en ... luego trabaj√≥ en ...\n\nSus habilidades son ....\n\n.... texto .....\n\n.... mucho texto ...\nPodemos ver que en este texto se encuentre la respuesta al prompt de manera expl√≠cita en dos ocasiones. Lo que esperamos que logre el procedimiento mostrado en Data Connection ser√° obtener los chunks de texto:\nNombre: Diego Machado\nEdad: 25 a√±os\nHobbies: Ir al gym, jugar p√°del, jugar videojuegos \ny\nDiego Machado estudi√≥ en .... bla bla\n\n.... texto ...  \nAs√≠, respetamos el chunk_size y adem√°s le entregamos los documentos m√°s relevantes al LLM en base al calculo de similaridad entre la pregunta ‚ÄúWhat is the candidate name?‚Äù y los distintos chunks.\nEntonces ahora tenemos un buen contexto que no incumple alguna norma, por lo que el prompt final se transforma en:\n\n\n\n\n\n\nNoteüëµüèΩ\n\n\n\nUtiliza las siguientes piezas de contexto para responder la pregunta al final. Por favor si no sabes la respuesta, s√≥lo di que no sabes, no intentes inventarla.\nNombre: Diego Machado Edad: 25 a√±os Hobbies: Ir al gym, jugar p√°del, jugar videojuegos Diego Machado estudi√≥ en ‚Ä¶. bla bla ‚Ä¶. texto ‚Ä¶\nPregunta: What is the candidate name?\nRespuesta:\n\n\ny lo que esperamos que suceda ser√° una respuesta del estilo:\n\n\n\n\n\n\nNoteü§ñ\n\n\n\nThe candidate‚Äôs name is Diego Machado\n\n\nY listo! Asi podemos resolver el problema de los documentos largos. Entonces el diagrama anterior resulta en:\n\n\n\nImage by diegulio\n\n\n\n\n\n\n\n\nNote‚òùüèΩ\n\n\n\nLa forma en la que se decide que chunks son relevantes se les denomina retrievers, y existen diversas metodolog√≠as muy interesantes y robustas. Te invito a buscar m√°s informaci√≥n ac√°\n\n\n\nChain Documents\nALTO AH√ç! A√∫n falta algo. Si nos fijamos bien, lo que hice con los chunks de texto relevantes fue simplemente concatenarlos! uno sobre el otro!\nNombre: Diego Machado\nEdad: 25 a√±os\nHobbies: Ir al gym, jugar p√°del, jugar videojuegos\nDiego Machado estudi√≥ en .... bla bla\n.... texto ...\nA esto se le llama stuff. Pero tambi√©n existen distintas metodolog√≠as para hacer esto de forma muy eficiente y robusta. Algunas de ellas incluso utilizan LLM auxiliares para refinar este contexto! Te invito a leer m√°s sobre eso ac√°.\nFiuf! Creo que eso ser√≠a ‚Äútodo‚Äù, por √∫ltimo te dejo un diagrama general, el cual muestra todos los elementos utilizados:\n\n\n\nhttps://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/"
  },
  {
    "objectID": "posts/llm-cv-assistant/main.html#load-document",
    "href": "posts/llm-cv-assistant/main.html#load-document",
    "title": "Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum",
    "section": "1. Load Document",
    "text": "1. Load Document\nLangchain tiene integraciones con un mont√≥n de tipos de documentos, para nuestro caso, en donde suponemos curriculums en pdf, debemos cargarlo de la siguiente forma:\n\nfrom langchain.document_loaders import PyPDFLoader\n#¬†Langchain loader\nloader = PyPDFLoader(\"../docs/CV_DMV.pdf\")\n# Load pages\npages = loader.load()\nImportante notar que tambi√©n podemos extraer metadata:\npages[0].metadata\n\n#¬†{'source': '../docs/CV_DMV.pdf', 'page': 0}"
  },
  {
    "objectID": "posts/llm-cv-assistant/main.html#splitting-documents",
    "href": "posts/llm-cv-assistant/main.html#splitting-documents",
    "title": "Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum",
    "section": "2. Splitting documents",
    "text": "2. Splitting documents\nEn este caso utilizaremos el m√©todo Recursive Character Text Splitter.\nEste separador de texto es el recomendado para texto gen√©rico. Se parametriza mediante una lista de caracteres. Se Intenta dividirlos en orden hasta que los trozos sean lo suficientemente peque√±os. La lista predeterminada es [‚Äú‚Äù, ‚Äú‚Äù, ‚Äù ‚Äú,‚Äù‚Äù]. Esto tiene el efecto de tratar de mantener todos los p√°rrafos (y luego las oraciones y luego las palabras) juntos el mayor tiempo posible, ya que gen√©ricamente parecer√≠an ser los fragmentos de texto m√°s relacionados sem√°nticamente.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    # Set a really small chunk size, just to show.\n    chunk_size = 1000,\n    chunk_overlap  = 100,\n)\n\ntexts = text_splitter.split_documents(pages)\nNotemos que ac√° aparecen los par√°metros mencionados anteriormente: chunk_size, chunk_overlap\nLo que esta funci√≥n retorna es una lista de Documents\ntexts = [\nDocument(page_content='Nombre:DiegoMachadoEdad:25a√±os..', metadata={..}),\nDocument(page_content='..texto..', metadata={..}),\nDocument(page_content='DiegoMachadoestudi√≥en....', metadata={..}),\n]"
  },
  {
    "objectID": "posts/llm-cv-assistant/main.html#vector-store",
    "href": "posts/llm-cv-assistant/main.html#vector-store",
    "title": "Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum",
    "section": "3. Vector Store",
    "text": "3. Vector Store\nAhora creamos nuestro vector store, en este caso Chroma. Notar que debemos ingresar como par√°metro la funci√≥n de embeddings a utilizar. En este caso utilizamos Vertex AI Embeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms import VertexAI\nfrom langchain.embeddings import VertexAIEmbeddings\n\n# Embeddings fn\nembeddings = VertexAIEmbeddings(project='gcp-project')\n\n# Persist Directory\npersist_directory = 'docs/chroma/'\n\n# Vector db\nvectordb = Chroma.from_documents(\n    documents=texts,\n    embedding=embeddings,\n    persist_directory=persist_directory\n)\nPodemos utilizar persist directory si queremos almacenar el vector store en nuestro local, para asi no tener que cargarlo cada vez que lo instanciamos:\nvectordb.persist()\nUtilizando el vector store podemos ocupar su funcionalidad de similarity_search para encontrar los chunks m√°s similares a una query en particular:\n# Test embeddings\nquestion = \"What is the name of the candidate?\"\ndocs = vectordb.similarity_search(question,k=2)\n\nprint(docs[0].page_content[:300]) # Solo imprimiremos los primeros 300 caracteres del primer documento\nDIEGOMACHADO\n/_4782ndAugust1997 /_475dmachadovz@gmail.com /phone+56950917953 /map_markerSantiago,Chile\n/linkedinwww.linkedin.com/in/DiegulioMachado /githubhttps://github.com/diegulio ·Ωë7diegulio.github.io\nBRIEFDESCRIPTION\nIndustrialengineeringgraduatedwitha\nmaster‚Äôsdegreeinengineeringsciences.\nPassio\nPodemos notar que el nombre del candidato aparece en el documento m√°s similar a la query! esto cumple con nuestras expectativas üòé"
  },
  {
    "objectID": "posts/llm-cv-assistant/main.html#conversational-retrieval-chain",
    "href": "posts/llm-cv-assistant/main.html#conversational-retrieval-chain",
    "title": "Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum",
    "section": "Conversational Retrieval Chain",
    "text": "Conversational Retrieval Chain\nAhora es donde podemos crear una cadena personalizada que tome el contexto seg√∫n la query, que cree el prompt final y que incluso vaya conservando la memoria.\nLa buena noticia es que Langchain ya tiene una cadena pre-construida que se encarga de todo esto! por lo que utilizarlo es muy f√°cil:\nfrom langchain.memory import ConversationBufferMemory\n\n# Primero instanciamos el tipo de memoria\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True\n)\nEsta cadena personalizada necesita 3 cosas: un LLM, el retriever y la memoria. En este caso utilizaremos el retriever base de Chroma, pero recuerda que podemos plantear otros!\nfrom langchain.chains import ConversationalRetrievalChain\n\n# Instanciamos LLM\nllm = VertexAI(project_id = 'gcp-project')\n\n#¬†Retriever\nretriever=vectordb.as_retriever()\n\nqa = ConversationalRetrievalChain.from_llm(\n    llm,\n    retriever=retriever,\n    memory=memory\n)\nY listo! la parte del c√≥digo parece ser lo m√°s sencillo, todo gracias al framework Langchain ü¶ú‚õìÔ∏è‚ù§Ô∏è\nAhora podemos probarlo:\nquestion = \"Does the candidate has been teacher assistant?\"\nresult = qa({\"question\": question})\n{'question': 'Does the candidate has been teacher assistant?',\n 'chat_history': [HumanMessage(content='Does the candidate has been teacher assistant?', additional_kwargs={}, example=False),\n  AIMessage(content='Yes, the candidate has been a teacher assistant.', additional_kwargs={}, example=False)],\n 'answer': 'Yes, the candidate has been a teacher assistant.'}\nSi volvemos a preguntar, en el chat_history se ir√° guardando autom√°ticamente toda la info del chat, por lo que nuestro bot se acordar√° de preguntas anteriores ! ü§ñ\nquestion = \"In which universities?\"\nresult = qa({\"question\": question})\n{'question': 'In which universities?',\n 'chat_history': [HumanMessage(content='Does the candidate has been teacher assistant?', additional_kwargs={}, example=False),\n  AIMessage(content='Yes, the candidate has been a teacher assistant.', additional_kwargs={}, example=False),\n  HumanMessage(content='In which universities?', additional_kwargs={}, example=False),\n  AIMessage(content='The candidate has been a teacher assistant at Universidad de Santiago and Universidad Adolfo Iba√±ez.', additional_kwargs={}, example=False)],\n 'answer': 'The candidate has been a teacher assistant at Universidad de Santiago and Universidad Adolfo Iba√±ez.'}"
  },
  {
    "objectID": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html",
    "href": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html",
    "title": "Leveraging MCPs with LLMs",
    "section": "",
    "text": "Status: ‚úÖ"
  },
  {
    "objectID": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#youtube-transcriptions",
    "href": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#youtube-transcriptions",
    "title": "Leveraging MCPs with LLMs",
    "section": "Youtube Transcriptions",
    "text": "Youtube Transcriptions\n\nQueremos extraer la transcripci√≥n de la clases para ingresarla al LLM como contexto. En mi caso, las clases de periodos anteriores est√°n en Youtube, pero en otros casos de uso podr√≠an utilizarse distintas herramientas de transcripci√≥n.\nPara el caso de youtube, existe una librer√≠a espec√≠fica para obtener transcripciones, esto nos va a facilitar la vida:\n\n\n\nsource: https://pypi.org/project/youtube-transcript-api\n\n\nsource: https://pypi.org/project/youtube-transcript-api\nSu uso es bastante sencillo, s√≥lo se necesita el ID del video, el cual encontraremos de forma sencilla en el url de este:\n\n\n\nimage.png\n\n\ny para obtener la transcripci√≥n s√≥lo debemos hacer:\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\ntranscript = YouTubeTranscriptApi().fetch(video_id)\nEn mi caso, yo queria insertar directamente el url del video de youtube, por lo que agregu√© una funci√≥n auxiliar, resultando en:\n# Funci√≥n auxiliar para obtener el video_id desde la url\ndef get_video_id(url):\n    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n    match = re.search(pattern, url)\n    return match.group(1) if match else None\n\n# Funci√≥n para obtener la transcripci√≥n del video\ndef get_yt_transcript(video_url: str) -&gt; str:\n    video_id = get_video_id(video_url)\n    transcript = YouTubeTranscriptApi().fetch(video_id)\n\n    text = ' '.join(snippet.text for snippet in transcript)\n\n    return text\nEsto es todo lo que necesito para mi caso de uso, pero imagina que podriamos crear otras funciones m√°s como: Un lector de comentarios, Obtener descripciones de los videos, b√∫squeda de titulos, etc."
  },
  {
    "objectID": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#mcp-server-youtube",
    "href": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#mcp-server-youtube",
    "title": "Leveraging MCPs with LLMs",
    "section": "MCP Server: Youtube",
    "text": "MCP Server: Youtube\nAhora, como llevamos esto a que sea un Servidor MCP? Gracias a que Anthropic cre√≥ un SDK en variados lenguajes (en nuestro caso usaremos el de python), la tarea es muy sencilla.\nAntes, quiero detenerme un poco para explicar unos conceptos previos. Existen 3 elementos bases que los servidores MCPs pueden proveer:\n\nüìÅ¬†Resources: Estos son archivos que pueden ser leidos por los clientes (imaginemos un .txt o algo por el estilo)\nüõ†Ô∏è¬†Tools: Este elemento debe ser el m√°s conocido y utilizado, son b√°sicamente funciones que pueden ser llamadas por los LLM.\nüí¨¬†Prompts: Estos son templates pre-hechos que pueden ayudar a los usuarios a ejecutar de mejor manera algunas tareas.\n\nEn nuestro caso, lo que queremos incluir en el servidor es una funci√≥n de Python, por lo que encaja con el elemento Tool. Como mencion√© anteriormente, gracias al SDK (Software Development Kit) de Python el esqueleto para crearlo es algo como:\n# Se importan librerias\nfrom mcp.server.fastmcp import FastMCP\n\n# Se inicializa MCP Server\nmcp = FastMCP(\"youtube\")\n\n# Se definen elementos (resources, tools, prompts)\n...\n\n# Se corre el servidor\nif __name__ == \"__main__\":\n    mcp.run(transport='stdio')\nS√≥lo nos falta definir el tool , y esto es muy sencillo, basta con agregar el decorador @mcp.tool() en la funci√≥n que queremos transformar, resultando:\n# Funci√≥n auxiliar para obtener el video_id desde la url\ndef get_video_id(url):\n    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n    match = re.search(pattern, url)\n    return match.group(1) if match else None\n\n# Funci√≥n para obtener la transcripci√≥n del video\n@mcp.tool() # &lt;--- Agregamos esto\ndef get_yt_transcript(video_url: str) -&gt; str:\n    video_id = get_video_id(video_url)\n    transcript = YouTubeTranscriptApi().fetch(video_id)\n\n    text = ' '.join(snippet.text for snippet in transcript)\n\n    return text\nSin embargo, recordemos que esta informaci√≥n es entregada por el MCP como contexto. Una pregunta v√°lida es ¬øComo sabe el modelo qu√© hace la funci√≥n? podriamos quiz√°s pensar que lee el c√≥digo para entender bien de que trata, pero en realidad es mas directo que esto.\nEl MCP server le entrega metadata como los tipos de entrada, salida y descripci√≥n de la funci√≥n. Por esto √∫ltimo es que es muy importante comentar de forma adecuada nuestro tool:\n# Funci√≥n auxiliar para obtener el video_id desde la url\ndef get_video_id(url):\n    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n    match = re.search(pattern, url)\n    return match.group(1) if match else None\n\n# Funci√≥n para obtener la transcripci√≥n del video\n@mcp.tool()\ndef get_yt_transcript(video_url: **str**) -&gt; **str**:\n        **\"\"\"Fetches the transcript of a YouTube video.\n    Args:\n        video_url (str): The URL of the YouTube video.\n    Returns:\n        str: The transcript of the video.\"\"\"**\n    video_id = get_video_id(video_url)\n    transcript = YouTubeTranscriptApi().fetch(video_id)\n\n    text = ' '.join(snippet.text for snippet in transcript)\n\n    return text\nFinalmente, nuestro main.py o server.py resulta:\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom mcp.server.fastmcp import FastMCP\nimport re\n\n# Initialize FastMCP server\nmcp = FastMCP(\"youtube\")\n\ndef get_video_id(url):\n    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n    match = re.search(pattern, url)\n    return match.group(1) if match else None\n\n@mcp.tool()\ndef get_yt_transcript(video_url: str) -&gt; str:\n    \"\"\"Fetches the transcript of a YouTube video.\n    Args:\n        video_url (str): The URL of the YouTube video.\n    Returns:\n        str: The transcript of the video.\"\"\"\n    video_id = get_video_id(video_url)\n    transcript = YouTubeTranscriptApi().fetch(video_id)\n\n    text = ' '.join(snippet.text for snippet in transcript)\n\n    return text\n\nif __name__ == \"__main__\":\n    mcp.run(transport='stdio')\n\nTest MCP Server\nAhora que hemos definido la configuraci√≥n de nuestro MCP Server de forma sencilla (existen un mont√≥n de otras configuraciones m√°s complejas posibles), podemos utilizar la herramienta Inspector para probar el servidor, para esto debemos correr en nuestra terminal:\nnpx @modelcontextprotocol/inspector \\\n  uv \\\n  --directory path/to/server \\\n  run \\\n  package-name \\\n  args...\nEn mi caso, corr√≠:\nnpx @modelcontextprotocol/inspector \\\n  uv \\\n  --directory path/to/server \\\n  run \\\n  main.py\nEsto nos abrir√° una interfaz donde podremos interactuar con nuestro servidor. Ver alguno de los elementos que hayamos definido, ejecutar herramientas, entre otros.\nScreen Recording 2025-08-28 at 7.28.25‚ÄØPM.mov\nComo vemos en el video, seleccionamos la √∫nica herramienta disponible y le ingresamos un video_url v√°lido, obteniendo as√≠ un resultado success con la transcripci√≥n del video. Es bastante importante el manejo de Exceptions/errores en las funciones, pero esto se escapa del scope del post."
  },
  {
    "objectID": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#mcp-server-notion",
    "href": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#mcp-server-notion",
    "title": "Leveraging MCPs with LLMs",
    "section": "MCP Server: Notion",
    "text": "MCP Server: Notion\nUna vez ya tenemos el MCP de Youtube, que ayudar√° a nuestro LLM a obtener contexto sobre la clase, necesitamos conectarlo con Notion, que es el lugar donde queremos dejar el resumen de las notas discutidas con el LLM.\nLa parte buena de esto, es que Notion ya tiene definido su propio Notion MCP Server. A continuaci√≥n definiremos el paso a paso para configurar este servidor, ya que requiere ciertos par√°metros; como a que p√°ginas les daremos accesos, y que tipo de accesos. Cabe destacar que el requisito m√≠nimo es tener una cuenta en Notion.\n\nPasos configuraci√≥n Notion MCP\n\nCrear integraci√≥n en Notion: Notion requiere crear integraciones para cualquier conexi√≥n a sus APIs, o en este caso, para el MCP. Esto se hace ingresando a https://www.notion.so/profile/integrations\n\n\n\n\nimage.png\n\n\nAc√° podemos seleccionar que accesos tendr√° el MCP Client\n\n\n\nimage.png\n\n\n\nüîë\nAl crear esta integraci√≥n, Notion nos entregar√° un token, este token nos permitir√° hacer la conexi√≥n a nuestra cuenta.\n\n\nSeleccionamos a que secciones o paginas de Notion queremos dar acceso a el MCP Client (Claude Desktop)\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\ny ya est√°! no es nada m√°s complicado que eso. Ahora tenemos los MCP Server (Youtube y Notion) necesarios para conectarlos al MCP Client mediante el MCP Host (Claude Desktop en nuestro caso)\nPara ver estos pasos de forma m√°s detallada, puedes visitar: https://github.com/makenotion/notion-mcp-server"
  },
  {
    "objectID": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#conectando-mcp-servers-a-claude-desktop",
    "href": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#conectando-mcp-servers-a-claude-desktop",
    "title": "Leveraging MCPs with LLMs",
    "section": "Conectando MCP Servers a Claude Desktop",
    "text": "Conectando MCP Servers a Claude Desktop\nUna vez que ya tenemos los servidores creados o configurados, nos toca conectarlos a el Host MCP (En este caso, Claude Desktop). Necesitamos informarle al Host sobre los servidores MCP para que √©ste inserte apropiadamente la informaci√≥n como contexto al LLM (Claude Sonnet).\nClaude Desktop cuenta con una conexi√≥n sencilla cuando se trata de conectar a servidores remotos (como lo es el caso de Notion MCP). Simplemente debemos ir a Settings ‚Üí Connectors ‚Üí Browse Connectors\n\n\n\nimage.png\n\n\nEn este lugar podremos encontrar un mont√≥n de herramientas, aunque s√≥lo se permite si eres usuario Pro üí∏¬†üò¢. Felizmente existe una forma gratuita de conectar los servidores MCPs a Claude Desktop, agregando la informaci√≥n del servidor a un archivo .json de configuraci√≥n, el cual se encuentra ingresando a Settings ‚Üí Developer ‚Üí Edit Config. Esto te dirigir√° al archivo claude_desktop_config.json el cual debemos abrir con alg√∫n editor de texto; dentro ver√°s el archivo con unas llaves {} . Ac√° debemos ingresar la informaci√≥n de nuestros MCP Servers para que Claude Desktop pueda instanciarlos.\nSiguiendo la documentaci√≥n del Notion MCP, debemos agregar algo como:\n\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n      }\n    }\n \nEn donde debemos reemplazar ntn_**** con el token de Notion que obtuvimos cuando creamos la integraci√≥n. Notemos que esto son s√≥lo instrucciones para lograr inicializar el servidor, con sus respectivos argumentos y/o variables de ambiente.\nEs por esto que para el caso de nuestro Youtube MCP Server la cosa no es muy diferente:\n\"youtube\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Path/to/server/\",\n        \"run\",\n        \"server.py\"\n      ]\n   }\nPara notion utilizamos npx para correr el MCP Server, en el caso de Youtube utilizamos uv (es lo recomendado por la documentaci√≥n de MCP).\nEstas son las configuraciones por separado, para agregarlo correctamente a claude_desktop_config.json queda:\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_***\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n      }\n    },\n    \"youtube\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Path/to/server/\",\n        \"run\",\n        \"server.py\"\n      ]\n   }\n }\n}\nPodr√°s imaginar que para seguir agregando MCP Servers debemos ir a√±adiendolos ac√° y listo. Ahora s√≥lo queda guardar üíæ¬† el archivo y reiniciar Claude Desktop ! üí•¬†Deberias poder ver las conexiones cuando seleccionas ‚ÄúSearch and Tools‚Äù , o incluso en Settings ‚Üí Connectors\n\n\n\nMCP Servers en el Chat\n\n\nMCP Servers en el Chat\n\n\n\nMCP Servers en Settings ‚Üí Connectors\n\n\nMCP Servers en Settings ‚Üí Connectors\n\n‚ö†Ô∏è\nPara que Claude Desktop pueda inicializar los servidores localmente, necesitamos installar nodejs (para el uso de npx) y uv.\n\n# Puedes usar tu administrador de paquetes favorito (o seguir las instrucciones\n# de la web de cada uno). En mi caso, utilizo brew \n\nbrew install node@22\n\nbrew install uv\n\n# Si no te funciona con brew, prueba descargando el instalador directamente desde la web"
  },
  {
    "objectID": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#voil√†-vamos-a-testear",
    "href": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#voil√†-vamos-a-testear",
    "title": "Leveraging MCPs with LLMs",
    "section": "ü•ñ¬†Voil√† ! Vamos a testear",
    "text": "ü•ñ¬†Voil√† ! Vamos a testear\nProbemos primero solicitando que nos hable de que trata alg√∫n video random de youtube, en mi caso probar√© directamente con una de las clases:\n\n\n\nimage.png\n\n\nFunciona!! Ahora podemos darle ese contexto a Claude Sonnet a la hora de preguntarle respecto a la claseüìù. Notemos que Claude Sonnet utiliz√≥ correctamente la funci√≥n get_yt_transcript que creamos anteriormente.\n\nüëÄ\nNotar√°s que Claude Desktop te pedir√° autorizaci√≥n cada vez que quiera ejecutar una tool de alguno de tus servidores. Esto le agrega un componente importante llamado human-in-the-loop y nos permite checkear que el LLM no est√© ejecutando acciones indebidas o que no estemos usando una herramienta peligrosa para nuestro sistema.\n\n\n\n\nimage.png\n\n\nVe√°mos como nos va con el MCP Server de Notion. Recordemos que s√≥lo tiene acceso a la p√°gina que le permitimos en la creaci√≥n de la integraci√≥n:\n\n\n\nimage.png\n\n\nEn Notion, vemos:\n\n\n\nimage.png\n\n\nAgreg√≥ la p√°gina e incluso a√±adi√≥ material introductorio dentro üí•¬†Ahora ya tenemos todo listo para poder estudiar eficientemente! Alguna de las solicitudes podrian ser tales como:\n\nPorfavor resume los puntos importantes de la clase y agregalas en una clase nueva en Notion, elige el titulo que prefieras!\n\n\nComo lleg√≥ a ese resultado utilizando la regla de la cadena? Escribe el paso a paso en la secci√≥n ‚Äútroubleshooting‚Äù de mi p√°gina en notion\n\n\nPor qu√© la regla de la cadena se aplica de esa forma en los LSTM? ‚Ä¶. (luego de una discusi√≥n intensa)‚Ä¶ perfecto, deja los puntos que aclaramos en la pagina de la clase correspondiente en Notion"
  },
  {
    "objectID": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#que-sucede-tras-bambalinas",
    "href": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#que-sucede-tras-bambalinas",
    "title": "Leveraging MCPs with LLMs",
    "section": "üì∏¬†Que sucede tras bambalinas?",
    "text": "üì∏¬†Que sucede tras bambalinas?\n\n(Previo a la pregunta) El Client MCP (Claude Desktop) le da el contexto de los elementos disponibles para utilizar al LLM (Claude Sonnet).\nSe env√≠a la pregunta a Claude Sonnet\nClaude Sonnet analiza las herramientas disponibles y decide si es necesario o no utilizar alguna\nEl MCP Client ejecuta la herramienta seleccionada (si es que seleccion√≥ una) mediante el MCP Server\nLos resultados son enviados a Claude Sonnet (y agregados al contexto)\nClaude Sonnet formula la respuesta a enviar al usuario (nosotros) mediante el MCP Host (Claude Desktop)"
  },
  {
    "objectID": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#troubleshooting",
    "href": "posts/mcp/Leveraging MCPs with LLMs 254da060cb6180f192c3d9ddd73631e8 copy.html#troubleshooting",
    "title": "Leveraging MCPs with LLMs",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nUn problema que estuve teniendo en el uso de este sistema que acabamos de crear, es que Claude Sonnet estaba teniendo problemas para escribir los titulos o formulas en Notion. Para solucionar esto, utilic√© el elemento prompting de los MCPs Servers. Para iterar r√°pido, esto lo agregu√© en el MCP Youtube que creamos üòÖ. Claramente lo ideal ser√≠a que estuviese en el MCP de Notion.\n## server.py\n\n@mcp.prompt()\ndef notion_formatting() -&gt; str:\n    \"\"\" \n    This prompt give useful information about how to use the notion tools\n    when you want to use headlines or formulas.\n    If you want to write titles/headlines or formulas, you mus include this prompt\n    \"\"\"\n\n    return \"\"\"\n    This is the schema for writing formula blocks in Notion:\n      {\n        \"children\": [\n            {\n                \"object\": \"block\",\n                \"type\": \"equation\",\n                \"equation\": {\n                    \"expression\": \"a^2 + b^2 = c^2\"\n                }\n            }\n        ]\n    }\n\n    This is the schema for writing headlines/titles in Notion:\n    {\n        \"children\": [\n            {\n                \"object\": \"block\",\n                \"type\": \"heading_1\",\n                \"heading_1\": {\n                    \"rich_text\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": {\n                                \"content\": \"Your Headline Text Here\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    }\n    \"\"\"\nAhora las f√≥rmulas y headlines son escritos de forma apropiada!\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/pytorch_breed_classification/main.html#config",
    "href": "posts/pytorch_breed_classification/main.html#config",
    "title": "Image Classification with Pytorch Lightning",
    "section": "Config",
    "text": "Config\nNormalmente, se utiliza un archivo, clase, etc. para definir la configuraci√≥n general de nuestra aplicaci√≥n. Por ejemplo: el nombre del modelo que utilizaremos, la ruta a la imagen, la ruta a el archivo de etiquetas, etc.\nPor ahora, nuestra clase config ser√°:\nclass CFG:\n  IMG_PATH = 'PATH/TO/IMAGES' # Ruta a imagenes\n  LABEL_PATH = '/PATH/TO/labels.csv' # Ruta a archivo de etiquetas\n\n  # Data\n  TEST_SIZE = 0.2 # Tama√±o del conjunto de test\n  VAL_SIZE = 0.1 #¬†Tama√±o del conjunto de validaci√≥n\nA medida avancemos, iremos agregando cada vez m√°s variables."
  },
  {
    "objectID": "posts/pytorch_breed_classification/main.html#dataset",
    "href": "posts/pytorch_breed_classification/main.html#dataset",
    "title": "Image Classification with Pytorch Lightning",
    "section": "1. Dataset",
    "text": "1. Dataset\nAl ser un problema supervisado, necesitaremos im√°genes y sus respectivas etiquetas. Un dato freak, es que este proyecto ya lo habia resuelto hace unos a√±os utilizando Tensorflow. En esa ocasi√≥n me tom√© el tiempo de recolectar datos de distintas fuentes para construir el dataset final, para mayor informaci√≥n de como lo hice (no es tan importante), leer aqu√≠.\nTLDR: extraje una base de datos de Stanford de razas de perro, un dataset de Kaggle de gatos, y descargu√© manualmente algunas im√°genes pseudo-aleatorias (otros animales, personas, cosas, paisajes). Juntando todo esto cre√© un dataset con las siguientes clases:\n\n120 razas de perro\nGato\nNo detectado\n\nLo que tenemos, es una carpeta con cientos de im√°genes. Adem√°s, tenemos un archivo .csv que nos indica la clase seg√∫n el nombre de la imagen. Esto nos ser√° util para la construcci√≥n de la clase personalizada Dataset en Pytorch:\n\n\n\nsample archivo labels.csv\n\n\nTodas las im√°genes son jpg, por lo que la extensi√≥n la agrego en el c√≥digo. Algo importante a mencionar, es que este dataset est√° balanceado (o almenos, no preocupantemente imbalanceado). Por lo que tenemos una cantidad decente de im√°genes para cada clase.\n\nSplit\nComo todo problema de ML, necesitaremos dividir nuestros datos en entrenamiento, validaci√≥n y testeo. Para esto, s√≥lo necesitaremos dividir el dataframe de el archivo de labels, ya que este mismo ser√° utilizando en el siguiente paso (Pytorch Dataset).\nfrom sklearn.model_selection import train_test_split\n\n# Leemos el archivo de anotaciones/etiquetas\nlabels = pd.read_csv(CFG.LABEL_PATH)\n\nX = labels.id #¬†Data (paths)\ny = labels.breed # Target\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=CFG.TEST_SIZE, random_state=13, shuffle = True, stratify = y)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=CFG.VAL_SIZE, random_state=13, shuffle = True, stratify = y_train)\n\n# Dejamos todo en un df para cada conjunto\ntrain_labels = pd.concat([X_train, y_train], axis = 1).reset_index(drop = True)\nval_labels = pd.concat([X_val, y_val], axis = 1).reset_index(drop = True)\ntest_labels = pd.concat([X_test, y_test], axis = 1).reset_index(drop = True)\n\n\nPytorch Dataset\nCode time üë®üèæ‚Äçüíª¬†!\nPara crear una clase Dataset personalizada en Pytorch, se necesitan 3 m√©todos fundamentales:\n\n__init__: La funci√≥n __init__ se ejecuta una vez al crear una instancia del objeto Dataset. Inicializamos el directorio que contiene las im√°genes, el archivo de anotaciones y transformaciones.\n__len__: La funci√≥n __len__ devuelve el n√∫mero de muestras en nuestro conjunto de datos.\n__getitem__: La funci√≥n __getitem__ carga y devuelve una muestra del conjunto de datos en el √≠ndice idx dado.\n\n\nHagamos esto de forma iterativa, en un paso 0 deberiamos tener algo como:\n\nfrom torch.utils.data import Dataset\n\n# Clase Dataset\nclass PetDataSet(Dataset):\n\n  def __init__(self):\n    pass\n\n  def __len__(self):\n    pass\n\n  def __getitem__(self, idx):\n    pass\n\nAhora, comenzaremos por el m√©todo __init__, ac√° es donde se inicializa la instancia. Es en donde buscaremos definir los atributos a usar en los siguientes m√©todos (__len__ y __getitem__). B√°sicamente, lo que necesitaremos son: las im√°genes (o la ruta a donde est√°n), y el archivo labels que nos dice la clase de cada imagen seg√∫n su nombre.\n\nAdem√°s ver√°s un par√°metro llamado transforms. Por ahora, imagina que esta es una funci√≥n que le cambia el tama√±o a la imagen. Esto es importante ya que no podemos alimentar al modelo con im√°genes de distintos tama√±os. M√°s adelante, veremos como esta funci√≥n puede hacer mucho m√°s que s√≥lo cambiar el tama√±o a las im√°genes.\nfrom torch.utils.data import Dataset\n\nclass PetDataSet(Dataset):\n\n  def __init__(self, config, labels, transform):\n    self.labels = labels # DataFrame de etiquetas\n    self.dir = config.IMG_PATH #¬†Path de imagenes\n    self.config = config # Clase configuraciones**\n\n  def __len__(self):\n    pass\n\n  def __getitem__(self, idx):\n    pass\n\n__len__: Ahora debemos calcular el largo de nuestro dataset. Esto es directo, ya que ser√° igual a la cantidad de filas de nuestro DataFrame labels.\n\nfrom torch.utils.data import Dataset\n\nclass PetDataSet(Dataset):\n\n  def __init__(self, config, labels, transform):\n    self.labels = labels # DataFrame de etiquetas\n    self.dir = config.IMG_PATH #¬†Path de imagenes\n    self.config = config # Clase configuraciones\n\n  def __len__(self):\n    return len(self.labels) # largo de dataset**\n\n  def __getitem__(self, idx):\n    pass\n\nFinalmente, el m√©todo __getitem__, el m√°s complejo. Debemos asumir que la entrada ser√° un indice, y necesitaremos que devuelva la imagen correspondiente (en pixeles), y su clase.\n\nfrom torch.utils.data import Dataset\n\nclass PetDataSet(Dataset):\n\n  def __init__(self, config, labels, transform):\n    self.labels = labels # DataFrame de etiquetas\n    self.dir = config.IMG_PATH #¬†Path a imagenes\n    self.config = config # Configuraciones\n    self.transform = transform # Transformaciones\n\n  def __len__(self):\n    return len(self.labels)  # largo de dataset\n\n  def __getitem__(self, idx):\n    breed = self.labels.iloc[idx, 1] # Etiqueta desde dataframe\n    img_path = self.labels.iloc[idx, 0] #¬†Nombre imagen desde dataframe\n    full_path = os.path.join(self.dir, f'{img_path}.jpg') # Path completo a imagen\n    image = read_image(full_path)/255 #¬†Se lee y normaliza la imagen \n    img = self.transform(image)  # Funci√≥n que cambia el tama√±o de la imagen\n    return img, breed #¬†Se retorna la imagen y la clase**\n\n\n\n\n\n\nNoteüö®\n\n\n\nALTO AHI! Si nos damos cuenta, tomamos las im√°genes y las convertimos a pixeles (n√∫meros). Esto es super l√≥gico, ya que los modelos s√≥lo leen n√∫meros! pero ¬øpor qu√© estamos retornando breed, si breed es una palabra?\nLa verdad es que esto es un error, por lo que debemos arregarlo. Para eso, le asignaremos un numero entero (√≠ndice) a cada clase, y as√≠ el modelo podr√° trabajar tranquilo.\n\n\nPara esto creamos un diccionario que le asignar√° un n√∫mero entero (√≠ndice) a cada clase i.g dalmata ‚Üí 0. Adem√°s crearemos un diccionario que seg√∫n un n√∫mero entero, nos indique a que clase pertenece i.g 0 ‚Üí dalmata. Todo esto lo haremos en nuestra clase de configuraci√≥n:\nclass CFG:\n  IMG_PATH = 'PATH/TO/IMAGES' # Ruta a imagenes\n  LABEL_PATH = '/PATH/TO/labels.csv' # Ruta a archivo de etiquetas\n\n  # Data\n  TEST_SIZE = 0.2\n  VAL_SIZE = 0.1\n\n  labels = pd.read_csv(LABEL_PATH) # leemos archivo de etiquetas\n  idx_to_class = dict(enumerate(labels.breed.unique())) # id -&gt; clase\n  class_to_idx = {c:i for i,c in idx_to_class.items()} # clase -&gt; id**\nAhora nuestra clase dataset quedar√° como:\nfrom torch.utils.data import Dataset\n\nclass PetDataSet(Dataset):\n  def __init__(self, config, labels, transform):\n    self.labels = labels # DataFrame de etiquetas\n    self.dir = config.IMG_PATH #¬†Path de imagenes\n    self.config = config # Configuraciones\n    self.transform = transform # Transformaciones\n\n  def __len__(self):\n    return len(self.labels)\n\n  def __getitem__(self, idx):\n    breed = self.labels.iloc[idx, 1] # Etiqueta desde dataframe\n    class_id = self.config.class_to_idx[breed]** # Convertimos clase a n√∫mero\n    img_path = self.labels.iloc[idx, 0] #¬†Nombre imagen\n    full_path = os.path.join(self.dir, f'{img_path}.jpg') # Path completo a imagen\n    image = read_image(full_path)/255 # Normalizaci√≥n de la imagen\n    img = self.transform(image)  # Funci√≥n que cambia el tama√±o de la imagen\n    return img, class_id # Se retorna la imagen y el indice de la clase**\nVo√≠la! Hemos terminado nuestra clase Dataset, s√≥lo falta inicializarla.\ntrain_dataset = PetDataSet(config = CFG, labels = train_labels, transform = train_transform)\nval_dataset = PetDataSet(config = CFG, labels = val_labels, transform = test_transform)\ntest_dataset = PetDataSet(config = CFG, labels = test_labels, transform = test_transform)\nAhora podemos consultar nuestra data, por ejemplo:\nlen(train_dataset) # -&gt; nos entregar√° el largo del dataset\n\npixels, class_id = train_dataset[0] # nos entregara la informaci√≥n del elemento 0"
  },
  {
    "objectID": "posts/pytorch_breed_classification/main.html#dataloader",
    "href": "posts/pytorch_breed_classification/main.html#dataloader",
    "title": "Image Classification with Pytorch Lightning",
    "section": "2. DataLoader",
    "text": "2. DataLoader\nEl modelo no ir√° consultando nuestra data uno por uno, ni toda a la vez. En realidad, nosotros le iremos entregando nuestras im√°genes en batches. Gracias a esto, nuestro modelo es m√°s eficiente y evitamos cualquier problemas de memoria (imagina cargar un mill√≥n de im√°genes a la vez en nuestra memoria RAM üí•).\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers = 1)\nval_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers = 1)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers = 1)\nExisten otras cosas que los DataLoader pueden hacer, pero por ahora con esto basta. Para mayor informaci√≥n visita la documentaci√≥n."
  },
  {
    "objectID": "posts/pytorch_breed_classification/main.html#model",
    "href": "posts/pytorch_breed_classification/main.html#model",
    "title": "Image Classification with Pytorch Lightning",
    "section": "3. Model",
    "text": "3. Model\nPara el modelo, utilizaremos una t√©cnica llamada Transfer Learning.\n\n\n\n\n\n\nNoteü§ñ\n\n\n\nTransfer learning (aprendizaje por transferencia) es una t√©cnica de aprendizaje autom√°tico en la que se aprovecha el conocimiento adquirido por un modelo entrenado en una tarea espec√≠fica y se aplica a una tarea relacionada. En lugar de entrenar un modelo desde cero, se toma un modelo pre-entrenado y se ajusta o se ‚Äútransfiere el conocimiento‚Äù para adaptarse a una nueva tarea. Esto a menudo ahorra tiempo y recursos, y puede resultar en un rendimiento mejorado en la nueva tarea, especialmente cuando los datos de entrenamiento son limitados.\n\n\nExiste una gran variedad de modelos de Computer Vision, los m√°s conocidos son las redes convolucionales. Por bastante tiempo esta t√©cnica ha estado en el estado del arte de computer vision. Te recomiendo leer m√°s ac√°.\n\nEn este caso, utilizaremos como base alguna de las arquitecturas m√°s conocidas pre-entrenadas. Luego, cambiaremos la parte de Clasificaci√≥n, para que prediga entre las clases de nuestro propio caso de uso y entrenaremos en base a eso.\nDebemos recordar que cuando uno entrena un modelo, lo que hace es ajustar ciertos pesos(par√°metros) de forma que se reduzca la funci√≥n de p√©rdida. En estas arquitecturas, existen millones de pesos a lo largo de la red. Debido a que el modelo est√° pre-entrenado, estos pesos ya han logrado capturar ciertos patrones, lo que facilitar√° la convergencia de nuestro modelo en el caso de uso que queremos: detectar razas de mascotas.\nIntentar ajustar todos los pesos har√° que nuestro modelo demore en converger y requiramos m√°s recursos computacionales, adem√°s puede que no tengamos los mejores resultados. Una pr√°ctica com√∫n es congelarü•∂ los pesos de la red pre-entrenada, y s√≥lo ajustar los pesos de la parte de clasificaci√≥n, y a veces algunas de las √∫ltimas capas. ¬ø Porque s√≥lo las √∫ltimas capas? Existen estudios que se√±alan que las √∫ltimas capas son aquellas que capturan los patrones m√°s espec√≠ficos/complejos de los casos de uso. Entonces si el modelo fue pre-entrenado en cebras, las √∫ltimas capas probablemente se fijar√°n en caracter√≠sticas espec√≠ficas o m√°s complejas de las cebras, mientras las primeras capas en caracter√≠sticas m√°s generales de los animales. Esto es muy conveniente, nos gustar√≠a conservar las caracter√≠sticas generales y ajustar aquellos pesos que detectan las caracter√≠sticas m√°s espec√≠ficas, para as√≠ detectar patrones complejos en nuestro propio caso de uso.\n\nPara resumir, el procedimiento ser√° el siguiente:\n\nTomamos una arquitectura base (Backbone)\nSustituimos la secci√≥n de clasificaci√≥n por una propia.\nCongelamos los par√°metros (pesos) de el modelo pre-entrenado.\nEntrenamos con nuestra data.\n\n\n\n\nArriba, el modelo pre-entrenado en el dataset ImageNet. Abajo el mismo modelo (par√°metros) aplicado a salud, utilizando un dataset m√©dico. Notar que la secci√≥n de Fully Connected Layers no es la misma, ya que la primera fue dise√±ada para predecir una gran variedad de clases, mientras que la segunda para predecir entre benigno o maligno (dos clases). Fuente: https://www.mdpi.com/1424-8220/23/2/570\n\n\n\nTIMM (pyTorch Image Models)\nUna primera pregunta ser√≠a ¬øQue modelo base utilizar? No hay una respuesta completamente te√≥rica, creo que la respuesta simple ser√≠a: experimentar.\nExiste un sin-fin de modelos base pre-entrenados, nosotros probaremos algunos de los m√°s conocidos: EfficientNet, VGG, Inception y Resnet. Lo que haremos ser√° entrenar estos modelos y quedarnos con aqu√©l que nos entregue mejores resultados.\nPara extraer los modelos pre-entrenados, utilizaremos timm: timm es una biblioteca de aprendizaje profundo creada por Ross Wightman y es una colecci√≥n de modelos, capas, utilidades, optimizadores, programadores, cargadores de datos, aumentos y tambi√©n scripts de entrenamiento/validaci√≥n de computer vision SOTA models con capacidad para reproducir resultados de entrenamiento de ImageNet.\nB√°sicamente es un repositorio de modelos pre-entrenados. Existen varios, Pytorch y tensorflow tambi√©n tienen sus propios HUBs.\nTraer un modelo pre-entrenado en timm es muy simple. S√≥lo debemos:\nimport timm\nbase_model = timm.create_model(\"model_name\", pretrained = True, num_classes = len(CFG.idx_to_class))\nAl entregarle el par√°metro num_classes. Timm autom√°ticamente reemplaza la capa final de clasificaci√≥n para que nos entregue predicciones acordes a nuestro caso de uso!\nA√∫n as√≠, para utilizar Pytorch Lightning debemos crear nuestra clase Model.\nAntes de comenzar a crear nuestra clase modelo, actualicemos nuestra clase configuraci√≥n con par√°metros relacionados a √©sta.\nclass CFG:\n  IMG_PATH = 'PATH/TO/IMAGES' # Ruta a imagenes\n  LABEL_PATH = '/PATH/TO/labels.csv' # Ruta a archivo de etiquetas\n\n  # Data\n  TEST_SIZE = 0.2\n  VAL_SIZE = 0.1\n\n  labels = pd.read_csv(LABEL_PATH) # leemos archivo de etiquetas\n  idx_to_class = dict(enumerate(labels.breed.unique())) # id -&gt; clase\n  class_to_idx = {c:i for i,c in idx_to_class.items()} # clase -&gt; id\n\n  # Model related\n  LEARNING_RATE = 0.001\n  MODEL = 'inception_v4' # timm name\n  PRETRAINED = True\n\n\nPytorch Lightning ‚ö°Ô∏è\nAl igual que en Dataset, Pytorch requiere algunos m√©todos fundamentales en la creaci√≥n de la clase Model. Estos son:\n\n__init__(): M√©todo que define las capas y otros componentes de un modelo.\nforward()¬†: m√©todo donde se realiza el c√°lculo a trav√©s de la red. Tener en cuenta que podemos imprimir el modelo, o cualquiera de sus subm√≥dulos, para conocer su estructura.\n\nA√∫n as√≠, Pytorch Lightning necesita unos m√©todos extras:\n\ntraining_step(): l√≥gica de entrenamiento.\nconfigure_optimizers(): definir optimizadores y/o programadores LR.\n\nExisten un mont√≥n de otros m√©todos importantes y √∫tiles, los puedes ver ac√°.\n\nComencemos con el paso 0:\n\nclass PetRecognitionModel(**L.LightningModule**):\n\n  def __init__(self):\n    super().__init__()\n    pass\n\n  def forward(self, x):\n    pass\n\n  def training_step(self, batch, batch_idx):\n    pass\n\n  def configure_optimizers(self):\n    pass\nEn el c√≥digo original, agregu√© otros m√©todos que utilic√©, pero que s√≥lo mostrar√© en este blog. Muchos de ellos eran para hacer la validaci√≥n, testeo, predicci√≥n y logging.\n\nEn cuanto al m√©todo __init__(), inicializaremos la configuraci√≥n, el modelo base y la m√©trica a utilizar:\n\nclass PetRecognitionModel(L.LightningModule):\n\n  def __init__(self, base_model, config):\n    super().__init__()\n    self.config = config\n    self.num_classes = len(self.config.idx_to_class)\n    self.metric = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n        \n    self.pretrained_model = base_model\n\n  def forward(self, x):\n    pass\n\n  def training_step(self, batch, batch_idx):\n    pass\n\n  def configure_optimizers(self):\n    pass\n\nforward() el m√©todo forward es bastante sencillo, debemos definir como nuestro input pasar√° a trav√©s de nuestra arquitectura. Debido a que es sencilla, simplemente har√° un paso por el modelo base, resultando:\n\nclass PetRecognitionModel(L.LightningModule):\n  def __init__(self, base_model, config):\n    super().__init__()\n    self.config = config\n    self.num_classes = len(self.config.idx_to_class)\n    self.metric = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n        \n    self.pretrained_model = base_model\n\n  def forward(self, x):\n    return self.pretrained_model(x)**\n\n  def training_step(self, batch, batch_idx):\n    pass\n\n  def configure_optimizers(self):\n    pass\n\ntraining_step() puede ser un poco m√°s complicado, la entrada de esta funci√≥n ser√° el batch de im√°genes y labels, y debemos retornar la funci√≥n de p√©rdida para ese batch.\n\nclass PetRecognitionModel(L.LightningModule):\n  def __init__(self, base_model, config):\n    super().__init__()\n    self.config = config\n    self.num_classes = len(self.config.idx_to_class)\n    self.metric = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n        \n    self.pretrained_model = base_model\n\n  def forward(self, x):\n    return self.pretrained_model(x)\n\n  def training_step(self, batch, batch_idx):\n    x,y = batch # dividimos el batch en data y labels\n    logits = self.forward(x) # la data pasa a trav√©s del modelo\n    loss = F.cross_entropy(logits, y) # Calculamos la funci√≥n de p√©rdida\n    self.log_dict({'train_loss': loss}) # Logueamos el resultado\n    return loss #¬†retornamos la p√©rdida\n\n  def configure_optimizers(self):\n    pass\nPor ahora, no te preocupes de la linea self.log_dict({'train_loss': loss}), m√°s adelante veremos para que es.\n\nconfigure_optimizers(): Finalmente debemos definir nuestro optimizador, el cu√°l ser√° bastante est√°ndar: utilizaremos Adam con alg√∫n learning_rate dado.\n\nfrom torch import optim\n\nclass PetRecognitionModel(L.LightningModule):\n  def __init__(self, base_model, config):\n    super().__init__()\n    self.config = config\n    self.num_classes = len(self.config.idx_to_class)\n    self.metric = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n        \n    self.pretrained_model = base_model\n\n  def forward(self, x):\n    return self.pretrained_model(x)\n\n  def training_step(self, batch, batch_idx):\n    x,y = batch # dividimos el batch en data y labels\n    logits = self.forward(x) # la data pasa a trav√©s del modelo\n    loss = F.cross_entropy(logits, y) # Calculamos la funci√≥n de p√©rdida\n    self.log_dict({'train_loss': loss}) # Logueamos el resultado\n    return loss #¬†retornamos la p√©rdida\n\n  def configure_optimizers(self):\n    optimizer = optim.Adam(self.parameters(), lr=self.config.LEARNING_RATE)\n    return {'optimizer': optimizer}\nListo! ya tenemos nuestro modelo para entrenar. Si quieres ver la implementaci√≥n completa, te recomiendo visitar el c√≥digo en el repositorio de github. A√∫n as√≠, ahora te mostrar√© el c√≥digo final luego de agregarle m√©todos adicionales:\nclass PetRecognitionModel(L.LightningModule):\n  def __init__(self, base_model, config):\n    super().__init__()\n    self.config = config\n    self.num_classes = len(self.config.idx_to_class)\n    metric = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n    self.train_acc = metric.clone()\n    self.val_acc = metric.clone()\n    self.test_acc = metric.clone()\n    self.training_step_outputs = []\n    self.validation_step_outputs = []\n    self.test_step_outputs = []\n\n    self.pretrained_model = base_model\n\n  def forward(self, x):\n    x = self.pretrained_model(x)\n    return x\n\n  def training_step(self, batch, batch_idx):\n    x,y = batch\n    logits = self.forward(x) # -&gt; logits\n    loss = F.cross_entropy(logits, y)\n    self.log_dict({'train_loss': loss})\n    self.training_step_outputs.append({'loss': loss, 'logits': logits, 'y':y})\n    return loss\n\n  def on_train_epoch_end(self):\n    # Concat batches\n    outputs = self.training_step_outputs\n    logits = torch.cat([x['logits'] for x in outputs])\n    y = torch.cat([x['y'] for x in outputs])\n    self.train_acc(logits, y)\n    self.log_dict({\n        'train_acc': self.train_acc,\n      },\n      on_step = False,\n      on_epoch = True,\n      prog_bar = True)\n    self.training_step_outputs.clear()\n\n  def validation_step(self, batch, batch_idx):\n    x,y = batch\n    logits = self.forward(x)\n    loss = F.cross_entropy(logits, y)\n    self.log_dict({'val_loss': loss})\n    self.validation_step_outputs.append({'loss': loss, 'logits': logits, 'y':y})\n    return loss\n\n  def on_validation_epoch_end(self):\n    # Concat batches\n    outputs = self.validation_step_outputs\n    logits = torch.cat([x['logits'] for x in outputs])\n    y = torch.cat([x['y'] for x in outputs])\n    self.val_acc(logits, y)\n    self.log_dict({\n        'val_acc': self.val_acc,\n      },\n      on_step = False,\n      on_epoch = True,\n      prog_bar = True)\n    self.validation_step_outputs.clear()\n\n  def test_step(self, batch, batch_idx):\n    x,y = batch\n    logits = self.forward(x)\n    loss = F.cross_entropy(logits, y)\n    self.log_dict({'test_loss': loss})\n    self.test_step_outputs.append({'loss': loss, 'logits': logits, 'y':y})\n    return loss\n\n  def on_test_epoch_end(self):\n    # Concat batches\n    outputs = self.test_step_outputs\n    logits = torch.cat([x['logits'] for x in outputs])\n    y = torch.cat([x['y'] for x in outputs])\n    self.test_acc(logits, y)\n    self.log_dict({\n        'test_acc': self.test_acc,\n      },\n      on_step = False,\n      on_epoch = True,\n      prog_bar = True)\n    self.test_step_outputs.clear()\n\n  def predict_step(self, batch):\n        x, y = batch\n        return self.model(x, y)\n\n  def configure_optimizers(self):\n    optimizer = optim.Adam(self.parameters(), lr=self.config.LEARNING_RATE)\n    lr_scheduler = ReduceLROnPlateau(optimizer, mode = 'min', patience = 3)\n    lr_scheduler_dict = {\n        \"scheduler\": lr_scheduler,\n        \"interval\": \"epoch\",\n         \"monitor\": \"val_loss\",\n    }\n    return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler_dict}\nLa mayor parte de los m√©todos adicionales fueron hechos para poder ir monitoreando las m√©tricas de el modelo, m√°s adelante veremos por qu√©.\nFinalmente, s√≥lo nos falta instanciar el modelo.\nmodel = PetRecognitionModel(base_model, config = CFG)\nAhora, una parte importante ser√° congelar aquellos par√°metros que no queremos entrenar. Para esto, congelaremos todos los par√°metros del modelo, y luego descongelaremos aquellos pertenecientes a las √∫ltimas capas:\ndef freeze_pretrained_layers(model, model_name):\n    '''Freeze all layers except the last layer(fc or classifier)'''\n        #¬†Freeze All params\n    for param in model.parameters():\n            param.requires_grad = False\n\n    # Unfreeze Classifier Parameters EfficientNET\n    if model_name == 'efficientnet_b2':\n      model.pretrained_model.classifier.weight.requires_grad = True\n      model.pretrained_model.classifier.bias.requires_grad = True\n    # Unfreeze Classifier Parameters VGG19\n    elif model_name == 'vgg19_bn':\n      model.pretrained_model.head.fc.weight.requires_grad = True\n      model.pretrained_model.head.fc.bias.requires_grad = True\n        # Unfreeze Classifier Parameters Inception\n    elif model_name == 'inception_v4':\n      model.pretrained_model.last_linear.weight.requires_grad = True\n      model.pretrained_model.last_linear.bias.requires_grad = True\n        # Unfreeze Classifier Parameters Resnet\n    elif model_name == 'resnet50':\n      model.pretrained_model.fc.weight.requires_grad = True\n      model.pretrained_model.fc.bias.requires_grad = True\n    else:\n      raise Exception('Modelo no encontrado')\nPuedes notar que los nombres de las capas varian seg√∫n el modelo. Ahora s√≥lo debemos aplicar la funci√≥n.\nfreeze_pretrained_layers(model, model_name = CFG.MODEL)"
  },
  {
    "objectID": "posts/pytorch_breed_classification/main.html#trainer",
    "href": "posts/pytorch_breed_classification/main.html#trainer",
    "title": "Image Classification with Pytorch Lightning",
    "section": "4. Trainer",
    "text": "4. Trainer\nAlfin, ahora s√≥lo nos queda entrenar. Esto ser√° muy sencillo gracias al m√≥dulo Trainer de Pytorch Lightning, el cual se encargar√° de elaborar la rutina de entrenamiento en base a lo que definimos anteriormente.\nPrimero, definimos Trainer junto con las configuraciones que deseemos:\n\nActualizamos clase CFG\n\nclass CFG:\n  IMG_PATH = 'PATH/TO/IMAGES' # Ruta a imagenes\n  LABEL_PATH = '/PATH/TO/labels.csv' # Ruta a archivo de etiquetas\n\n  # Data\n  TEST_SIZE = 0.2\n  VAL_SIZE = 0.1\n\n  labels = pd.read_csv(LABEL_PATH) # leemos archivo de etiquetas\n  idx_to_class = dict(enumerate(labels.breed.unique())) # id -&gt; clase\n  class_to_idx = {c:i for i,c in idx_to_class.items()} # clase -&gt; id\n\n    # Model related\n  LEARNING_RATE = 0.001\n  MODEL = 'inception_v4'\n  PRETRAINED = True\n\n  # Trainer\n  PRECISION = 16 # Float Precision\n  MIN_EPOCHS = 1 # Min epochs\n  MAX_EPOCHS = 3 # Max epochs\n  ACCELERATOR = 'gpu' # Device\n\nInstanciamos el m√≥dulo Trainer\n\nimport lightning as L\n\ntrainer = L.Trainer(\n    accelerator=CFG.ACCELERATOR,\n    devices=1, #¬†solo 1 device (gpu en nuestro caso)\n    min_epochs=CFG.MIN_EPOCHS,\n    max_epochs=CFG.MAX_EPOCHS,\n    precision=CFG.PRECISION,\n)\nüöÄ¬†listo! ahora s√≥lo nos queda ejecutar el ansiado .fit(). Pero a√∫n falta algo importante ¬øComo sabremos que modelo es mejor? Nos basaremos en la m√©trica Accuracy. Pero ¬ødonde la monitorearemos?\n\nüêù¬†Weight and Biases Logging\nWeight and Biases (W&B) es una plataforma que permite realizar un seguimiento, visualizaci√≥n y colaboraci√≥n en proyectos de aprendizaje autom√°tico, lo que facilita el registro y el an√°lisis de experimentos y modelos de Machine Learning.\nEn este caso, la utilizaremos para monitorear las m√©tricas de nuestros modelos! Quiz√° en un futuro haga un blog de como utilizar esta maravillosa herramienta. En este caso es sencillo, s√≥lo debemos inicializar Wandb logger, y comunic√°rselo a nuestro m√≥dulo Trainer.\n\nActualizamos CFG con la info de nuestro proyecto en WandB\n\nclass CFG:\n  IMG_PATH = 'PATH/TO/IMAGES' # Ruta a imagenes\n  LABEL_PATH = '/PATH/TO/labels.csv' # Ruta a archivo de etiquetas\n\n  # Data\n  TEST_SIZE = 0.2\n  VAL_SIZE = 0.1\n\n  labels = pd.read_csv(LABEL_PATH) # leemos archivo de etiquetas\n  idx_to_class = dict(enumerate(labels.breed.unique())) # id -&gt; clase\n  class_to_idx = {c:i for i,c in idx_to_class.items()} # clase -&gt; id\n\n  # Model related\n  LEARNING_RATE = 0.001\n  MODEL = 'inception_v4'\n  PRETRAINED = True\n\n  # Trainer\n  PRECISION = 16 # Float Precision\n  MIN_EPOCHS = 1 # Min epochs\n  MAX_EPOCHS = 20 # Max epochs\n  ACCELERATOR = 'gpu' # Device\n\n  # Wandb related\n  WANDB_PROJECT = 'My-wandb-project'\n  WANDB_ENTITY = 'diegulio'\n\nInstanciamos el logger\n\nwandb_logger = WandbLogger(project = CFG.WANDB_PROJECT,\n                           entity = CFG.WANDB_ENTITY,\n                           name = f\"{CFG.MODEL}_baseline\", # exp name\n                           log_model=False, # no guardar artefacto\n                           config = wandb_config, # config\n                           group = 'pretrained', #¬†grupo\n                           job_type = 'training') # tipo de trabajo\n\nAgregamos el logger al Trainer\n\ntrainer = L.Trainer(\n    accelerator=CFG.ACCELERATOR,\n    devices=1,\n    min_epochs=CFG.MIN_EPOCHS,\n    max_epochs=CFG.MAX_EPOCHS,\n    precision=CFG.PRECISION,\n    logger = wandb_logger, # Agregamos el logger\n)"
  },
  {
    "objectID": "posts/pytorch_breed_classification/main.html#fit",
    "href": "posts/pytorch_breed_classification/main.html#fit",
    "title": "Image Classification with Pytorch Lightning",
    "section": "5. Fit",
    "text": "5. Fit\nAhora somos libres de entrenar!\ntrainer.fit(model, train_dataloader, val_dataloader)\nVeremos algo este estilo m√°s una barrita de carga:\n\nObservemos que la cantidad total de par√°metros es de 43.3 Millones ! A√∫n as√≠ nosotros s√≥lo entrenamos 2.1 Millones, y el resto lo congelamos ü•∂"
  },
  {
    "objectID": "posts/pytorch_breed_classification/main.html#evaluate",
    "href": "posts/pytorch_breed_classification/main.html#evaluate",
    "title": "Image Classification with Pytorch Lightning",
    "section": "6. Evaluate",
    "text": "6. Evaluate\nCon Pytorch Lightning la evaluaci√≥n tambi√©n es sencilla ya que previamente establecimos el m√©todo on_test_epoch_end(). S√≥lo debemos ejectuar:\ntrainer.test(model, test_dataloader)\n\nNota: El test accuracy mostrado ac√° fue con el modelo sin pre-entrenar. Podemos notar lo mal que le fue debido a que necesitaba m√°s tiempo para converger."
  },
  {
    "objectID": "posts/back_to_sit/main.html#installation",
    "href": "posts/back_to_sit/main.html#installation",
    "title": "Back To Sit",
    "section": "Installation",
    "text": "Installation\nTo install back_to_sit library you can use pip:\npip install back-to-sit"
  },
  {
    "objectID": "posts/back_to_sit/main.html#usage",
    "href": "posts/back_to_sit/main.html#usage",
    "title": "Back To Sit",
    "section": "Usage",
    "text": "Usage\nUsing the Back to Sit library is easy. First, you need to create a Telegram bot and obtain an API token. You can follow the instructions on the Telegram website to create a bot and obtain an API token (or easily read the instructions below). Lastly, you need to obtain your chat id.\nHere I will show you how to get both:\n\nAPI Token ü§ñ\nFirst you need to create your own bot ü§ñ. This bot will be your messenger! üíå\nTo create it, follow the next steps:\n\nInstall Telegram in your smartphone\nGo to Chats ‚Üí Search Chats ‚Üí Type BotFather\nThere to create your bot you can simply follow the instructions. But it should be something like this:\n\n/newbot\n\nbot_name # You choose this\n\nexamplebot_bot # You choose this\n\nFinally, get the token API that BotFather bring to you. Nice! As simple as that you have your API token\n\n\n\nChat ID\nNow, you will create a ‚Äúuniversal‚Äù bot, it means that anyone could chat with him/her. So you need to correctly identify the chat between the bot and you! To do this follow the steps:\n\nSearch your bot. In telegram go to Chats ‚Üí Search Chats ‚Üí Type the username of your bot (Ex: examplebot_bot) and enter to the chat.\nWrite a dummy message to the bot. (For example, ‚ÄùHello world‚Äù)\nIn your navigator, enter to https://api.telegram.org/bot{TOKEN_API}/getUpdates. Replace {TOKEN_API} with your API token.\nHere you will see a payload with the messages, from the payload you should just get the ‚Äúid‚Äù number.\n\nPayload Example:\n{\"ok\":true,\"result\":[{\"update_id\":9421735642,\n\"message\":{\"message_id\":3,\"from\":{\"id\":6276864755,\"is_bot\":false,\"first_name\":\"Diego\",\"last_name\":\"Machado\",\"language_code\":\"es\"},\"chat\":{\"id\":6276864755,\"first_name\":\"Diego\",\"last_name\":\"Machado\",\"type\":\"private\"},\"date\":1682916041,\"text\":\"Hello World\"}}]}\nIn the example case, the chat_id is 6276864755\n\n\n\n\n\n\nNote\n\n\n\nüëÄ If you get something like {‚Äúok‚Äù:true,‚Äúresult‚Äù:[]}. Try to send more messages and reload the step 3.\n\n\n\nWell Done! üöÄüöÄüöÄüöÄüöÄ\nOnce you have the API token and the chat id, you are already done to use back to sit! You have two available options, use the back_to_sit()function or use the @back_to_sit_decorator(). We will navigate through them a little bit in the following sections:\n\n\nFunction mode üî®\nThis mode is useful when you are running tasks in your Jupyter notebook and don‚Äôt want to alter your function behavior.\nYou can use back_to_sit() function mode in your code like this:\n# Import Libraries\nfrom back_to_sit import back_to_sit\nimport time\n\n# Define necessary constants\nMESSAGE = \"Hey! wake up! your code is done!\"\nCHAT_ID = \"CHATIDNUMBER\"\nAPI_TOKEN = \"API:TOKEN\"\n\n# Start time is optional if you want backtosit to report the time consumed\nstart_time = time.time()\n\n# Some task code here ...\ntime.sleep(10)\n####\n\n# This line is executed after your task code is done! \nback_to_sit(MESSAGE, CHAT_ID, API_TOKEN, start_time, notebook = False)\n\n\n\nfunction mode\n\n\n\n\n\n\n\n\nNote\n\n\n\nüëÄ Important! If your are running it in Jupyter notebooks, you should give the input parameter notebook=True , otherwise you should do notebook=False\n\n\n\n\n\nDecorator mode ‚ú®\nThis mode is useful when you want to get the back to sit report every time you use the function! or maybe if don‚Äôt want to add an extra line of code!\nYou can use back-to-sit decorator mode in your code like this:\n# Import Libraries\nfrom back_to_sit import back_to_sit\nimport time\n\n# Define necessary constants\nMESSAGE = \"Hey! wake up! your code is done!\"\nCHAT_ID = \"CHATIDNUMBER\"\nAPI_TOKEN = \"API:TOKEN\"\n\n@back_to_sit_decorator(MESSAGE, CHAT_ID, API_TOKEN, notebook = False)\ndef sleep_ten_seconds():\n  time.sleep(10)\n  return None\nIn the decorator mode, the start time is configured automatically. So every time you execute the function you will get the back to sit report!\ngo_sleep = sleep_ten_seconds()\n\n\n\ndecorator mode\n\n\n\n\n\n\n\n\nNote\n\n\n\nüëÄ Important! If your are running it in Jupyter notebooks, you should give the input parameter notebook=True , otherwise you should do notebook=False"
  },
  {
    "objectID": "posts/back_to_sit/main.html#license",
    "href": "posts/back_to_sit/main.html#license",
    "title": "Back To Sit",
    "section": "License",
    "text": "License\nThe Back to Sit library is licensed under the MIT license. See the LICENSE file for more information."
  },
  {
    "objectID": "posts/gan/main.html#generative-adversarial-networks",
    "href": "posts/gan/main.html#generative-adversarial-networks",
    "title": "Paper Implementation Series: Generative Adversarial Networks",
    "section": "Generative Adversarial Networks",
    "text": "Generative Adversarial Networks\nA grandes rasgos, en este paper muestran un nuevo camino para la generaci√≥n de datos. Es importante que s√≥lo le digamos camino, porque no buscan traer la mejor soluci√≥n que promete superar a todo, si no que remarcan que el objetivo es dar a conocer una forma prometedora en la que se podr√≠a generar datos, y que con el tiempo podria traer grandes resultados con ayuda de la comunidad investigadora. No s√≥lo generar im√°genes, si no tambi√©n otros tipos de datos como audio o texto.\nEfectivamente, esta metodolog√≠a marc√≥ un antes y un despu√©s para la generaci√≥n de im√°genes, que luego fue superada por los modelos de difusi√≥n debido a la inestabilidad y dificultad de entrenamiento que las GAN traen. A√∫n as√≠, la comunidad mejor√≥ este primer paper trayendo una gran cantidad de GANs al mundo, logrando grandiosos resultados (DCGAN, cGAN, styleGAN, CycleGAN, etc)\nEn resumen, esta metodolog√≠a consta de 2 modelos, un Generador y un Discriminador. La misi√≥n del Generador es, como lo indica su nombre, generar im√°genes (a partir de aqu√≠ solo hablaremos de im√°genes) que sigan la misma distribuci√≥n que el conjunto de datos, esto de forma indirecta, ya que en realidad lo entrenamos para que logre enga√±ar al Discriminador. El Discriminador, por otro lado, es entrenado para clasificar entre im√°genes provenientes de la data real e im√°genes generadas por el Generador.\n\n\n\nGenerative Adversarial Networks\n\n\nEn la pr√≥xima secci√≥n comenzaremos a construir esta metodolog√≠a."
  },
  {
    "objectID": "posts/gan/main.html#implementando-gan",
    "href": "posts/gan/main.html#implementando-gan",
    "title": "Paper Implementation Series: Generative Adversarial Networks",
    "section": "üë®üèæ‚Äçüíª¬†Implementando GAN",
    "text": "üë®üèæ‚Äçüíª¬†Implementando GAN\nSeg√∫n el paper, para implementar la soluci√≥n, necesitamos 3 componentes:\n\nGenerador\nDiscriminador\nRutina de Entrenamiento\n\nEl objetivo es construir un modelo que genere im√°genes que provengan de la misma distribuci√≥n que nuestro conjunto de datos (que se parezcan). En esta ocasi√≥n utilizaremos el conocido conjunto de datos MNIST, ya que es uno de los utilizados en el paper y es bastante simple de encontrar y utilizar.\n\nWe trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database(TFD) [28], and CIFAR-10 [21].\n\nAc√° un vistazo de como luce el conjunto de datos MNIST (son im√°genes de n√∫meros escritos a mano)\n\n\n\nMNIST Dataset\n\n\n\n1. Generador\nLeamos que dice el paper sobre esto:\n\nTo learn the generator‚Äôs distribution \\(p_g\\) over data \\(x\\), we define a prior on input noise variables \\(p_z(z)\\), then represent a mapping to data space as \\(G(z;\\theta_g)\\), where \\(G\\) is a differentiable function represented by a multilayer perceptron with parameters \\(\\theta_g\\).\n\nLo que se entiende de ac√° es que nosotros definiremos una distribuci√≥n a priori para el input \\(z\\) del generador \\(G\\). Este ruido \\(z\\) es luego transformado por el Generador (una red neuronal) para obtener una imagen generada que sigue la distribuci√≥n \\(p_g\\). Nuestro objetivo entonces es que la distribuci√≥n \\(p_g = p_{data}\\) , esto tambi√©n lo podemos concluir de el extracto del paper:\n\nThe generator \\(G\\) implicitly defines a probability distribution \\(p_g\\) as the distribution of the samples \\(G(z)\\) obtained when \\(z‚àºp_z\\). Therefore, we would like Algorithm 1 to converge to a good estimator of \\(p_{data}\\), if given enough capacity and training time.\n\n\n\n\nGenerator\n\n\nNo se ven tan complicado, ahora veamos m√°s detalles sobre el generador en si:\n\nThe generator nets used a mixture of rectifier linear activations [19,9] and sigmoid activations ‚Ä¶. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network.\n\nAc√° nos topamos con la primera complejidad: No tenemos muchos detalles. Bien podemos ver que no nos dan informaci√≥n sobre cuantos layers tiene el MLP, cuantas neuronas cada layer, ni como transformamos el resultado a una imagen! Esto √∫ltimo era bastante confuso para mi, en primera instancia creia que se utilizaban Convolutional Neural Networks, pero luego me di cuenta que no era necesariamente as√≠.\nCreo que el motivo del por qu√© no hay tanto detalle en el modelo, es que buscan dejarlo abierto a que la comunidad comience a explorar y experimentar con esta arquitectura de soluci√≥n.\nAdem√°s del paper, me apoy√© de algunos otros blogs donde explicaban este modelo, la mayor√≠a hablaba directamente de Convolutional Neural Networks. A√∫n as√≠, me quise apegar lo m√°s posible al paper y descart√© esta soluci√≥n por el momento. Incluso el paper sugiere el uso de CNN bien sutilmente cuando escriben lo siguiente en el pie de foto de los resultados\n\n\nCIFAR-10 (convolutional discriminator and ‚Äúdeconvolutional‚Äù generator)\n\n\n√âsta era la √∫nica pista de que al menos para CIFAR-10 utilizaron redes convolucionales.\nLuego de algunas iteraciones, llegu√© a la conclusi√≥n que ser√≠a algo como lo siguiente:\n\n\n\nGeneration of an Image from Noise\n\n\nLa operaci√≥n ser√≠a:\n\nObtenemos \\(noise\\) vector de la distribuci√≥n a priori \\(p_z\\) de dimensi√≥n \\(latent\\_size\\) (la distribuci√≥n a priori suele ser gaussiana o uniforme)\nPasamos el vector por una red neuronal (NN)\nTransformamos el vector final proveniente del √∫ltimo layer de la NN a las dimensiones de la imagen ( un simple .reshape(img_size))\n\nVamos a ver el c√≥digo ! ü§ñü§ñü§ñ\n# 1. Noise Vector\nnoise = torch.randn(LATENT_SIZE)\n\n# 2. Generator NN\nclass Generator(nn.Module):\n\n    \"\"\"Generator model: In charge of generate \n    real-like images from a noise distribution\n\n    \"\"\"\n    def __init__(self, latent_size, img_size):\n        super(Generator, self).__init__()\n\n        # layers to use\n        self.model = nn.Sequential(\n            nn.Linear(latent_size, 128),  # Nx100\n            nn.LeakyReLU(),\n            nn.Linear(128,256), # Nx256\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(),\n            nn.Linear(256, 512), # Nx512\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(),\n            nn.Linear(512, 1024), # Nx1024\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(),\n            nn.Linear(1024, img_size*img_size), # Nx28*28\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n# 3. Generate Img\nout = G(noise.unsqueeze(0))\nimg = out.view(IMG_SIZE, IMG_SIZE)\nplt.imshow(img.detach().numpy(), cmap='gray')\nCon el modelo sin entrenar, el resultado es algo del estilo:\n\n\n\nGenerated Image from nontrained Generator\n\n\nSer√° muy interesante ver como el modelo evoluciona de tal manera que pueda convertir este ruido en im√°genes parecidas al conjunto de datos!\nObservaciones üëÄ\n\nLas dimensiones del conjunto de datos MNIST es (28x28x1) ‚Üí son im√°genes en blanco y negro\nNotar que la √∫ltima capa debe tener una dimensi√≥n de salida igual a la dimensi√≥n de la imagen, esto con el objetivo de poder moldear(reshape) el vector resultante a la imagen. Esto es, el vector de salida debe ser de \\(28*28*1=784\\)\nAl comienzo utilic√© ReLU como funci√≥n de activaci√≥n, en el paper dicen: The generator nets used a mixture of rectifier linear activations. Pero luego de algunas iteraciones vi que era mejor LeakyReLU\nAl comienzo no utilic√© BatchNorm, pero tambi√©n me di cuenta (navegando por internet) que era importante para este tipo de modelo para estabilidad y convergencia.\nLa salida de el modelo es pasado por una funci√≥n \\(Tanh()\\) para que se encuentre en un rango [-1,1]. Las im√°genes originales tambi√©n son normalizadas previamente para que pertenezcan a este rango.\nLATENT_SIZE suele ser 100\nImportante mencionar que al principio s√≥lo comenc√© con unas cuantas capas, y fui iterando hasta llegar a la arquitectura mostrada arriba seg√∫n los resultados y experimentos de otras personas.\nEn el paper DCGAN (veremos m√°s adelante), mencionan que el modelo se ve beneficiado al utilizar BatchNorm en todas las capas menos en la primera del Generador y la √∫ltima del Discriminador.\n\n\n\n2. Discriminador\nYa tenemos el modelo que generar√° imagenes y que intentar√° enga√±ar al Discriminador. El Discriminador es bastante m√°s sencillo, s√≥lo debemos pensar que es un clasificador de im√°genes tal y como lo conocemos. Esto es, recibimos una imagen (pixeles), y devolvemos una clase (0 si es fake, 1 si es real)\nEl paper dice lo siguiente del Discriminador D:\n\nWe also define a second multilayer perceptron \\(D(x;\\theta_d)\\) that outputs a single scalar. \\(D(x)\\)represents the probability that \\(x\\) came from the data rather than \\(p_g\\). We train D to maximize the probability of assigning the correct label to both training examples and samples from G.\n\nEn c√∫anto a los detalles, s√≥lo tenemos :\n\nwhile the discriminator net used maxout [10] activations. Dropout [17] was applied in training the discriminator net.\n\nSi lo visualizamos, ser√≠a algo como:\n\n\n\nDiscriminator\n\n\nEl proceso es el siguiente:\n\nLa entrada puede ser una imagen real proveniente del dataset (\\(p_{data}\\)) o proveniente del Generador (\\(p_{g}\\))\nLa Imagen entra al Discriminador (NN)\nEl resultado es una probabilidad [0,1], en donde 0 es imagen fake y 1 real.\n\nEs algo as√≠ como el proceso contrario de el Generador. Vamos al c√≥digo!\n#0. Define MaxOut Activation\nclass MaxOut(nn.Module):\n    def __init__(self, num_units, num_pieces):\n        super(MaxOut, self).__init__()\n        self.num_units = num_units\n        self.num_pieces = num_pieces\n        self.fc = nn.Linear(num_units, num_units * num_pieces)\n\n    def forward(self, x):\n        # Reshape the output to separate pieces\n        maxout_output = self.fc(x).view(-1, self.num_pieces, self.num_units)\n        # Take the maximum value across pieces\n        output, _ = torch.max(maxout_output, dim=1)\n        return output\n\n# 1. Imagen o Imagen Generada\nout # Esto viene del Generador que hicimos arriba (o de el dataset original)\n\n# 2. Discriminador\nclass Discriminator(nn.Module):\n    \"\"\"Generator model: In charge of classify\n    images between real and syntetic generated\n    by the generator \n\n    \"\"\"\n    def __init__(self, img_size):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(img_size*img_size, 512), #N x 512\n            MaxOut(512, 4),\n            nn.Linear(512, 256), # N x 256\n            MaxOut(256, 4),\n            nn.Linear(256, 1), # N x 1\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n# 3. Se predice\nD = Discriminator()\np = D(out)\nEl resultado es simple, ser√° algo como 0.495. Algo as√≠ como que es tan probable que sea una imagen real como fake seg√∫n el discriminador. Al comienzo puede ser as√≠ ya que no est√° entrenado.\nObservaciones üëÄ\n\nLa entrada es la misma dimensi√≥n que la salida del Generador (784)\nAc√° no utilizamos BatchNorm (aunque si deb√≠ hacerlo seg√∫n el paper DCGAN)\nUtilizamos MaxOut activation como lo indica el paper, √©ste no se encuentra por defecto en Pytorch, por lo que hay que definirlo ‚Äúa mano‚Äù\nOlvid√© utilizar Dropout como lo recomienda el paper, esto podr√≠a mejorar los resultados, al igual que agregar BatchNorm\nAl igual que en el Generador, la arquitectura se fue iterando seg√∫n resultados.\n\n\n\n3. Rutina de Entrenamiento\nUna vez tenemos el Generador y el Discriminador, s√≥lo nos queda entrenar. Seg√∫n el paper (lo que entend√≠), no lo hacen de la forma tradicional (iterando sobre el dataset original). Ellos proponen lo siguiente:\n\n\n\nGAN Training Algorithm\n\n\nProponen, en cada iteraci√≥n, primero optimizar el Discriminador por K pasos:\n\nExtraen un minibatch de imagenes reales\nExtraen un minibatch de imagenes generadas por G\nHacen un update en base a Gradient Ascent\n\nLuego, optimizan el Generador en un s√≥lo paso:\n\nExtrae un minibatch de imagenes generadas por G\nHacen un update de los par√°metros en base a Gradient Descend\n\nSeg√∫n indican en el paper, updatean m√°s veces el Discriminador, ya que buscan que el Discriminador se mantenga cercano al √≥ptimo mientras el Generador se va actualizando lentamente hasta converger.\n\nThis results in D being maintained near its optimal solution, so long as G changes slowly enough.\n\nLo que vemos ac√°, es que el generador actualiza sus par√°metros utilizando informaci√≥n implicita entregada por el Discriminador. Optimizamos el Generador para lograr enga√±ar al Discriminador.\nAlgo importante a poner atenci√≥n, es la funci√≥n objetivo, que tenemos 2, la del discriminador y la del generador, vamos a ver esto con m√°s detalle.\n\nLoss Functions üìé\nDiscriminador\nRecordemos que el Discriminador \\(D\\) es un clasificador com√∫n, por lo que podemos utilizar la funci√≥n com√∫n para estos casos (Cross Entropy Loss). Desentra√±emos esto hasta llegar a lo que tienen en el paper:\nQueremos clasificar correctamente los positivos (imagenes reales) y negativos (imagenes generadas), la funci√≥n objetivo Binary Cross Entropy est√° dada por:\n\\[\n-\\dfrac{1}{m} \\sum{y_ilog(\\hat{y_i}) + (1-y_i)log(1-\\hat{y_i})}\n\\]\ndonde \\(y_i\\) es el label (0 o 1) de la imagen i ; \\(\\hat{y_i}\\) es la predicci√≥n de la imagen i, fake o real ; \\(m\\) es el tama√±o del minibatch\nNotemos que\n\ncuando \\(y_i = 1\\), la imagen \\(x_i\\) es real , osea que \\(\\hat{y_i} = D(x_i)\\) y lo de la derecha es 0 en la ecuaci√≥n\ncuando \\(y_i=0\\), la imagen \\(G(z_i)\\) es fake ,osea que \\(\\hat{y_i} = D(G(z_i))\\) y lo de la izquierda es 0 en la ecuaci√≥n\n\nEs por esto que podemos reducir la ecuaci√≥n a:\n\\[\nMin -\\dfrac{1}{m} \\sum{ log(D(x_i)) + log(1-D(G(z_i)))}\n\\]\nSi bien esto se puede traducir a que queremos maximizar\n\\[\nMax \\dfrac{1}{m} \\sum{ log(D(x_i)) + log(1-D(G(z_i)))}\n\\]\ntal como sale en el paper (sin el signo negativo), por conveniencia no lo haremos y optaremos por minimizar el Binary Cross Entropy y asi el c√≥digo se reduce a utilizar la funci√≥n ya hecha en Pytorch BCELoss() quedando algo como\nBCELoss([D(x), D(G(z))], [1, 0])\nOtras soluciones optan por optimizarlas por separado, o por promediar ambos. En mi caso las concaten√© junto con sus targets.\n\n\n\n\n\n\nNoteüí°\n\n\n\nMaximizar una funci√≥n Z es equivalente a Minimizar -Z\n\n\nGenerador\nPara el generador, es similar, s√≥lo que ahora s√≥lo le entregamos im√°genes falsas y adem√°s queremos que \\(D\\) se equivoque. Por lo tanto, maximizamos la funci√≥n de p√©rdida Binary Cross Entropy (que se equivoque), y s√≥lo observamos cuando \\(y_i = 0\\) (s√≥lo im√°genes falsas), teniendo\n\\[\nMax -\\dfrac{1}{m} \\sum{  log(1-D(G(z_i)))}\n\\]\nAhora lo llevamos a minimizar por conveniencia en c√≥digo (en Pytorch por defecto se busca minimizar la funci√≥n objetivo)\n\\[\nMin \\dfrac{1}{m} \\sum{  log(1-D(G(z_i)))}\n\\]\nobteniendo lo que aparece en el paper. En c√≥digo ser√≠a algo como\n-BCELoss(D(G(z)), 0) # El signo - es porque implicitamente maximizamos BCE\n\nFuaaa, cuanta matem√°tica ü§Øü§Øü§Ø, comprobemos esto de forma intuitiva:\nPara el Discriminador tenemos entonces:\n\\[\nMax \\dfrac{1}{m} \\sum{ log(D(x_i)) + log(1-D(G(z_i)))}\n\\]\nsi lo separamos, queremos entonces:\n\\[\nMax \\dfrac{1}{m} \\sum{ log(D(x_i)) }\n\\]\n\\[\nMax \\dfrac{1}{m} \\sum{ log(1-D(G(z_i)) }\n\\]\nLa primera funci√≥n (olvidando la suma y eso) es \\(ln(x)\\) ‚Üí si, la base utilizada es la exponencial, por lo que es el logaritmo natural. Su gr√°fica es:\n\n\n\nln(x)\n\n\nEl eje horizontal es \\(D(x_i)\\): La probabilidad de que la imagen real \\(x_i\\) sea clasificada como real, con un rango [0,1]. Ya que la imagen es real, queremos que D prediga un valor alto (ojal√° 1), lo que si vemos la gr√°fica es igual a alcanzar el m√°ximo de la funci√≥n (m√°x optimo). Es por esto que maximizamos \\(\\dfrac{1}{m} \\sum{ log(D(x_i)) }\\)\nAhora vamos con \\(log(1-D(G(z_i))\\) , cuya gr√°fica es:\n\n\n\nln(1-x) Discriminador\n\n\nAhora la gr√°fica cambia, el eje horizontal est√° dado por \\(D(G(z_i))\\): La probabilidad de que la imagen falsa \\(G(z_i)\\) sea clasificada como real. Vemos que la funci√≥n se maximiza cuando \\(D(G(z_i))\\) se acerca a 0, y esto es lo que queremos porque \\(G(z_i)\\) es una imagen falsa y debe ser clasificada como tal en el Discriminador (ojal√° 0). Es por esto que buscamos maximizar \\(\\dfrac{1}{m} \\sum{ log(1-D(G(z_i)) }\\)\n\n\n\n\n\n\nNoteüî•\n\n\n\nIMPORTANTE! Ac√° vemos la funci√≥n objetivo tal y como est√° en el paper (maximizando), pero en el c√≥digo, por conveniencia, lo llevamos a minimizar la BCELoss, lo cual es equivalente, tal como lo mostramos en la parte matem√°tica. √âsto en los gr√°ficos es parecido s√≥lo que ahora buscamos minimizar ambas partes, dejar√© este ejercicio como tarea para el lector!\n\n\nPara el Generador, es muy similar a la segunda ecuaci√≥n del Discriminador, s√≥lo que ahora buscamos minimizar (ir hacia a la derecha).\n\n\n\nln(1-x) Generador\n\n\nVemos que la idea es ir hacia la derecha porque queremos que el Discriminador se equivoque, o sea que aunque le entreguemos una imagen falsa \\(D(G(z_i))\\), el discriminador prediga que es real (se acerque a 1). Si vemos bien, a medida \\(D(G(z_i))\\) se acerca a 1, la funci√≥n \\(\\dfrac{1}{m} \\sum{  log(1-D(G(z_i)))}\\) se acerca a Esto puede provocar inestabilidad en la optimizaci√≥n, y es por esto que mucha gente utiliza la equivalencia:\n\\[\nMin \\dfrac{1}{m} \\sum{ log(D(G(z_i))) }\n\\]\ncomo funci√≥n objetivo de el Generador. Ac√° vemos que utilizamos la parte donde \\(y_i = 1\\) , osea que la imagen es real, pero minimizamos haciendo que el Discriminador diga que es falsa. Intuitivamente se pierde un poco el sentido ya que en realidad la imagen entregada no es real, pero matem√°ticamente es equivalente y logra mayor estabilidad y al minimizar el √≥ptimo es 0 y no \\(-\\infin\\) (a√∫n as√≠ yo utilic√© la primera forma)\nEspero esta secci√≥n se haya entendido, es un tanto complicado de escribir con palabras. El proceso de entendimiento de la funci√≥n objetivo, fue un proceso bastante entretenido e interesante, que sin duda me sirvi√≥ para mejorar algunas aptitudes a la hora de leer papers. A√∫n as√≠ mencionar que es importante apoyarse en la literatura (libros, blogs, papers), el objetivo no es comprobar que tu podr√≠as haber llegado a las mismas conclusiones que los autores por tu lado, si no que eres capaz de entender (matem√°tica- o intuitiva- mente) lo que se propone. Invito al lector a iniciar una conversaci√≥n conmigo si le qued√≥ alguna duda, o si not√≥ que pude haberme equivocado en algo.\n\nTrain üèãüèæ\nAhora ya podemos comenzar a entrenar el modelo, primero vamos a inicializar algunos componentes importantes:\n\nConfiguraci√≥n\n\n# CFG\nlatent_size = 100 # noise dimension\nimg_size = 28 # image shape\ndevice = \"cuda\" # GPU\nepochs = 20000 # TRAINING ITERATIONS\nk = 1 # Discriminator steps\n\nModelos: Utilizamos los modelos construidos anteriormente\n\n# Models \nG = Generator(cfg.latent_dim, cfg.img_size)\nD = Discriminator(cfg.img_size)\n\nG.to(cfg.device)\nD.to(cfg.device)\n\nLoss Functions: Recordemos que gracias a que re-definimos la matem√°tica de las funciones objetivos, podemos ocupar la funci√≥n pre-construida en Pytorch BCELoss()\n\n# Losses\nD_LOSS = nn.BCELoss()\nG_LOSS = nn.BCELoss()\n\nOptimizadores: Utilizamos Adam, con un learning rate bastante m√°s peque√±o que el usual y unos betas espec√≠ficos. Esto es uno de los problemas de GAN, es muy sensible a los hyperpar√°metros, cuando normalmente los modelos (Deeplearning) son robustos a estos.\n\n# Optimizers\nd_optimizer = torch.optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))\ng_optimizer = torch.optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n\nDataset: Ya que MNIST es bastante conocido, ya se encuentra disponible en Pytorch para descargar\n\n# Datasets (Images and Noise)\ntransform = transforms.Compose([\n    transforms.Resize((cfg.img_size, cfg.img_size)),\n    transforms.ToTensor(),  # Convert PIL image or numpy array to tensor\n    transforms.Normalize((0.5,), (0.5,))  # Normalize the tensor with mean and standard deviation\n])\n\ndataset = MNIST(root = '', download = True, transform =transform )\nsampler = RandomSampler(dataset) # To get random images each iteration\n\n# DataLoader\noriginal_dl = DataLoader(dataset, batch_size = cfg.batch_size, sampler = sampler, pin_memory=torch.cuda.is_available())\nNotar que ac√° normalizamos las im√°genes para que est√©n en el rango [-1,1] y nos aseguramos que tengan el mismo tama√±o 28x28. Tambi√©n, ya que no iremos avanzando batch por batch, iremos sampleando aleatoriamente el dataset tal como lo dice la rutina de entrenamiento pertenciente al paper.\n\nRutina de entrenamiento: Iremos sentencia por sentencia navegando la rutina de entrenamiento y codeando!\n\n\nfor number of training iterations do\n\nfor epoch in range(cfg.epochs):\n\nfor k steps do\n\nfor epoch in range(cfg.epochs):\n    for k in range(cfg.k):\n\nSample minibatch of m noise samples {z(1), . . . , z(m)} from noise prior \\(p_g(z)\\).\n\nfor epoch in range(cfg.epochs):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n\nSample minibatch of m examples {x(1), . . . , x(m)} from data generating distribution \\(p_{data}(x)\\)\n\nfor epoch in range(cfg.epochs):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n        # original minibatch\n        x, _ = next(iter(original_dl))\n\nUpdate the discriminator by ascending its stochastic gradient\n\nfor epoch in range(cfg.epochs):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n        # original minibatch\n        x, _ = next(iter(original_dl))\n\n        ##############################\n        ## Discriminator Optimization \n        ##############################\n        d_optimizer.zero_grad()\n        G_z = G(z) # Generated Image\n        D_x  = D(x) # real image's probability of being real\n        D_G_z = D(G_z.detach()) # fake image's probability of being real\n        \n        # Concat real and fakes\n        samples = torch.cat([D_G_z, D_x]).to(cfg.device)\n        # Make targets (1 for real, 0 for fakes)\n        targets = torch.cat([torch.zeros(D_G_z.size()[0]), torch.ones(D_x.size()[0])]).to(cfg.device)\n\n        # Discriminator Loss\n        # Loss\n        d_loss = D_LOSS(samples, targets.unsqueeze(-1))\n        d_loss.backward() # backward\n        \n        # Adjust learning weights\n        d_optimizer.step()\n\nEnd for. Sample minibatch of m noise samples {z(1), . . . , z(m)} from noise prior \\(p_g(z)\\).\n\nfor epoch in tqdm(range(cfg.epochs)):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n        # original minibatch\n        x, _ = next(iter(original_dl))\n\n        ##############################\n        ## Discriminator Optimization \n        ##############################\n        d_optimizer.zero_grad()\n        G_z = G(z) # Generated Image\n        D_x  = D(x) # real image's probability of being real\n        D_G_z = D(G_z.detach()) # fake image's probability of being real\n        \n        # Concat real and fakes\n        samples = torch.cat([D_G_z, D_x]).to(cfg.device)\n        # make targets (1 for real, 0 for fakes)\n        targets = torch.cat([torch.zeros(D_G_z.size()[0]), torch.ones(D_x.size()[0])]).to(cfg.device)\n\n        # Discriminator Loss\n        # Loss\n        d_loss = D_LOSS(samples, targets.unsqueeze(-1))\n        d_loss.backward() # backward\n        \n        # Adjust learning weights\n        d_optimizer.step()\n    \n    # Minibatch from noise prior\n    z = torch.randn(batch_size, cfg.latent_dim).to(cfg.device)\n\nUpdate the generator by descending its stochastic gradient:\n\nfor epoch in tqdm(range(cfg.epochs)):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n        # original minibatch\n        x, _ = next(iter(original_dl))\n\n        ##############################\n        ## Discriminator Optimization \n        ##############################\n        d_optimizer.zero_grad()\n        G_z = G(z) # Generated Image\n        D_x  = D(x) # real image's probability of being real\n        D_G_z = D(G_z.detach()) # fake image's probability of being real\n        \n        # Concat real and fakes\n        samples = torch.cat([D_G_z, D_x]).to(cfg.device)\n        # make targets (1 for real, 0 for fakes)\n        targets = torch.cat([torch.zeros(D_G_z.size()[0]), torch.ones(D_x.size()[0])]).to(cfg.device)\n\n        # Discriminator Loss\n        # Loss\n        d_loss = D_LOSS(samples, targets.unsqueeze(-1))\n        d_loss.backward() # backward\n        \n        # Adjust learning weights\n        d_optimizer.step()\n    \n    # Minibatch from noise prior\n    z = torch.randn(batch_size, cfg.latent_dim).to(cfg.device)\n\n    ##############################\n    ## Generator Optimization \n    ##############################\n    g_optimizer.zero_grad()\n    G_z = G(z) # Generated Images\n    D_G_z = D(G_z) # fake/generated image's probability of being real\n    # targets\n    targets = torch.zeros(D_G_z.size()[0]).to(cfg.device)\n    # Loss\n    g_loss = -G_LOSS(D_G_z, targets.unsqueeze(-1)) # Max BCE\n    g_loss.backward()\n\n    # Adjust learning weights\n    g_optimizer.step()\nY con eso ya estamos! a correr y a observar los resultados, a continuaci√≥n algunas observaciones:\n\nTambi√©n se puede hacer el entrenamiento com√∫n (minibatch stochastic gradient descend )\nVi que en algunos lados utilizan el mismo noise \\(z\\) para ambas partes de la optimizaci√≥n\nEn la linea D_G_z = D(G_z.detach()) , el .detach() es para no actualizar parametros del Generador en la parte del Discriminador, por lo que quitamos G_z de del grafo computacional.\nEntren√© por 20.000 iteraciones! se demor√≥ aprox 15 minutos. Notemos que no son epochs (pasadas por el training set), si no que s√≥lo iteraciones del algoritmo.\n\n\n\n\n\n\n\nNote‚ö†Ô∏è\n\n\n\nWARNING: Si quieres utilizar el c√≥digo, te recomiendo verlo directamente desde Github ya que ac√° quit√© algunas cosas para que se pueda entender mejor\n\n\n\n\n4. Resultados\nLo primero a observar es la curva de aprendizaje, que no es tan bonita\n\n\n\nLearning Curve GAN\n\n\nPodemos ver lo inestable que es el entrenamiento, aunque si tiende a minimizar ambas funciones. Ahora vemos que im√°genes genera:\n\n\n\nSample Results\n\n\nFuaaa ü§©¬†Parecer ser que el modelo intenta converger y genera algunas im√°genes que si podrian enga√±ar al ojo humano.\nSi te lo est√°s preguntando, la evaluaci√≥n real propuesta en el paper no es simple, en el paper utilizan unas t√©cnicas espec√≠ficas, pero para dejarlo simple, ac√° simplemente evaluaremos con la vista!\n\nAlgo importante es que idealmente en generaci√≥n de im√°genes no se aprenda de memoria las caracter√≠sticas de el conjunto de entrenamiento, si no que le agregue un poco de su saz√≥n! e.g al generar caras, en donde el set de entrenamiento no tienen ninguna cara con barba, nos gustar√≠a que pudiese generar caras con barba (esto es una limitante)\nVeamos como va evolucionando el modelo cada 1000 iteraciones!\n\n\n\nIteraci√≥n 0\n\n\n\n\n\nIteraci√≥n 2000\n\n\n\n\n\nIteraci√≥n 1000\n\n\n\n\n\nIteraci√≥n 3000\n\n\n\n\n\nIteraci√≥n 4000\n\n\n\n\n\nIteraci√≥n 15000\n\n\nObservamos que ya a partir de la iteraci√≥n 4000 logra hacer un ‚Äú6‚Äù bastante decente.\n\n\n\n\n\n\nNoteüìù\n\n\n\nPD: Mis disculpas por no haber hecho un plot m√°s ordenado ac√°!"
  },
  {
    "objectID": "posts/gan/GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77.html",
    "href": "posts/gan/GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77.html",
    "title": "GAN: Paper Implementation",
    "section": "",
    "text": "Status: üë®üèæ‚Äçüíª"
  },
  {
    "objectID": "posts/gan/GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77.html#generative-adversarial-networks",
    "href": "posts/gan/GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77.html#generative-adversarial-networks",
    "title": "GAN: Paper Implementation",
    "section": "Generative Adversarial Networks",
    "text": "Generative Adversarial Networks\nA grandes rasgos, en este paper muestran un nuevo camino para la generaci√≥n de informaci√≥n. Es importante que s√≥lo le digamos camino, porque no buscan traer la mejor soluci√≥n que promete superar a todo, si no que remarcan que el objetivo es dar a conocer una forma prometedora en la que se podr√≠a generar informaci√≥n, y que con el tiempo podria traer grandes resultados con ayuda de la comunidad investigadora. No s√≥lo generar im√°genes, si no tambi√©n otros tipos de informaci√≥n como audio o texto.\nEfectivamente, esta metodolog√≠a marc√≥ un antes y un despu√©s para la generaci√≥n de im√°genes, que luego fue superada por los modelos de difusi√≥n debido a la inestabilidad y dificultad de entrenamiento que las GAN traen. A√∫n as√≠, la comunidad mejor√≥ este primer paper trayendo una gran cantidad de GANs al mundo, logrando grandiosos resultados (DCGAN, cGAN, styleGAN, CycleGAN, etc)\nEn resumen, esta metodolog√≠a consta de 2 modelos, un Generador y un Discriminador. La misi√≥n del Generador es, como lo indica su nombre, generar im√°genes (a partir de aqu√≠ solo hablaremos de im√°genes) que sigan la misma distribuci√≥n que el conjunto de datos, esto de forma indirecta, ya que en realidad lo entrenamos para que logre enga√±ar al Discriminador. El Discriminador, por otro lado, es entrenado para clasificar entre im√°genes provenientes de la data real e im√°genes generadas por el Generador.\n\n\n\nGenerative Adversarial Networks\n\n\nGenerative Adversarial Networks\nEn la pr√≥xima secci√≥n comenzaremos a construir esta metodolog√≠a."
  },
  {
    "objectID": "posts/gan/GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77.html#implementando-gan",
    "href": "posts/gan/GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77.html#implementando-gan",
    "title": "GAN: Paper Implementation",
    "section": "üë®üèæ‚Äçüíª¬†Implementando GAN",
    "text": "üë®üèæ‚Äçüíª¬†Implementando GAN\nSeg√∫n el paper, para implementar la soluci√≥n, necesitamos 3 componentes:\n\nGenerador\nDiscriminador\nRutina de Entrenamiento\n\nEl objetivo es construir un modelo que genere im√°genes que provengan de la misma distribuci√≥n que nuestro conjunto de datos (que se parezcan). En esta ocasi√≥n utilizaremos el conocido conjunto de datos MNIST, ya que es uno de los utilizados en el paper y es bastante simple de encontrar y utilizar.\n\nWe trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database(TFD) [28], and CIFAR-10 [21].\n\nAc√° un vistazo de como luce el conjunto de datos MNIST (son im√°genes de n√∫meros escritos a mano)\n\n\n\nMNIST Dataset\n\n\nMNIST Dataset\n\n1. Generador\nLeamos que dice el paper sobre esto:\n\nTo learn the generator‚Äôs distribution \\(p_g\\) over data \\(x\\), we define a prior on input noise variables \\(p_z(z)\\), then represent a mapping to data space as \\(G(z;\\theta_g)\\), where \\(G\\) is a differentiable function represented by a multilayer perceptron with parameters \\(\\theta_g\\).\n\nLo que se entiende de ac√° es que nosotros definiremos una distribuci√≥n a priori para el input \\(z\\) del generador \\(G\\). Este ruido \\(z\\) es luego transformado por el Generador (una red neuronal) para obtener una imagen generada que sigue la distribuci√≥n \\(p_g\\). Nuestro objetivo entonces es que la distribuci√≥n \\(p_g = p_{data}\\) , esto tambi√©n lo podemos concluir de el extracto del paper:\n\nThe generator \\(G\\) implicitly defines a probability distribution \\(p_g\\) as the distribution of the samples \\(G(z)\\) obtained when \\(z‚àºp_z\\). Therefore, we would like Algorithm 1 to converge to a good estimator of \\(p_{data}\\), if given enough capacity and training time.\n\n\n\n\nGenerator\n\n\nGenerator\nNo se ven tan complicado, ahora veamos m√°s detalles sobre el generador en si:\n\nThe generator nets used a mixture of rectifier linear activations [19,9] and sigmoid activations ‚Ä¶. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network.\n\nAc√° nos topamos con la primera complejidad: No tenemos muchos detalles. Bien podemos ver que no nos dan informaci√≥n sobre cuantos layers tiene el MLP, cuantas neuronas cada layer, ni como transformamos el resultado a una imagen! Esto √∫ltimo era bastante confuso para mi, en primera instancia creia que se utilizaban Convolutional Neural Networks, pero luego me di cuenta que no era necesariamente as√≠.\nCreo que el motivo del por qu√© no hay tanto detalle en el modelo, es que buscan dejarlo abierto a que la comunidad comience a explorar y experimentar con esta arquitectura de soluci√≥n.\nAdem√°s del paper, me apoy√© de algunos otros blogs donde explicaban este modelo, la mayor√≠a hablaba directamente de Convolutional Neural Networks. A√∫n as√≠, me quise apegar lo m√°s posible al paper y descart√© esta soluci√≥n por el momento. Incluso el paper sugiere el uso de CNN bien sutilmente cuando escriben lo siguiente en el pie de foto de los resultados\n\n\nCIFAR-10 (convolutional discriminator and ‚Äúdeconvolutional‚Äù generator)\n\n\n√âsta era la √∫nica pista de que al menos para CIFAR-10 utilizaron redes convolucionales.\nLuego de algunas iteraciones, llegu√© a la conclusi√≥n que ser√≠a algo como lo siguiente:\n\n\n\nUntitled\n\n\nLa operaci√≥n ser√≠a:\n\nObtenemos \\(noise\\) vector de la distribuci√≥n a priori \\(p_z\\) de dimensi√≥n \\(latent\\_size\\) (la distribuci√≥n a priori suele ser gaussiana o uniforme)\nPasamos el vector por una red neuronal (NN)\nTransformamos el vector final proveniente del √∫ltimo layer de la NN a las dimensiones de la imagen ( un simple .reshape(img_size))\n\nVamos a ver el c√≥digo ! ü§ñü§ñü§ñ\n# 1. Noise Vector\nnoise = torch.randn(LATENT_SIZE)\n\n# 2. Generator NN\nclass Generator(nn.Module):\n\n    \"\"\"Generator model: In charge of generate \n    real-like images from a noise distribution\n\n    \"\"\"\n    def __init__(self, latent_size, img_size):\n        super(Generator, self).__init__()\n\n        # layers to use\n        self.model = nn.Sequential(\n            nn.Linear(latent_size, 128),  # Nx100\n            nn.LeakyReLU(),\n            nn.Linear(128,256), # Nx256\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(),\n            nn.Linear(256, 512), # Nx512\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(),\n            nn.Linear(512, 1024), # Nx1024\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(),\n            nn.Linear(1024, img_size*img_size), # Nx28*28\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n# 3. Generate Img\nout = G(noise.unsqueeze(0))\nimg = out.view(IMG_SIZE, IMG_SIZE)\nplt.imshow(img.detach().numpy(), cmap='gray')\nCon el modelo sin entrenar, el resultado es algo del estilo:\n\n\n\nUntitled\n\n\nSer√° muy interesante ver como el modelo evoluciona de tal manera que pueda convertir este ruido en im√°genes parecidas al conjunto de datos!\nObservaciones üëÄ\n\nLas dimensiones del conjunto de datos MNIST es (28x28x1) ‚Üí son im√°genes en blanco y negro\nNotar que la √∫ltima capa debe tener una dimensi√≥n de salida igual a la dimensi√≥n de la imagen, esto con el objetivo de poder moldear(reshape) el vector resultante a la imagen. Esto es, el vector de salida debe ser de \\(28*28*1=784\\)\nAl comienzo utilic√© ReLU como funci√≥n de activaci√≥n, en el paper dicen: The generator nets used a mixture of rectifier linear activations. Pero luego de algunas iteraciones vi que era mejor LeakyReLU\nAl comienzo no utilic√© BatchNorm, pero tambi√©n me di cuenta (navegando por internet) que era importante para este tipo de modelo para estabilidad y convergencia.\nLa salida de el modelo es pasado por una funci√≥n \\(Tanh()\\) para que se encuentre en un rango [-1,1]. Las im√°genes originales tambi√©n son normalizadas previamente para que pertenezcan a este rango.\nLATENT_SIZE suele ser 100\nImportante mencionar que al principio s√≥lo comenc√© con unas cuantas capas, y fui iterando hasta llegar a la arquitectura mostrada arriba seg√∫n los resultados.\nEn el paper DCGAN (veremos m√°s adelante), mencionan que el modelo se ve beneficiado al utilizar BatchNorm en todas las capas menos en la primera del Generador y la √∫ltima del Discriminador.\n\n\n\n2. Discriminador\nYa tenemos el modelo que generar√° imagenes y que intentar√° enga√±ar al Discriminador. El Discriminador es bastante m√°s sencillo, s√≥lo debemos pensar que es un clasificador de im√°genes tal y como lo conocemos. Esto es, recibimos una imagen (pixeles), y devolvemos una clase (0 si es fake, 1 si es real)\nEl paper dice lo siguiente del Discriminador D:\n\nWe also define a second multilayer perceptron \\(D(x;\\theta_d)\\) that outputs a single scalar. \\(D(x)\\)represents the probability that \\(x\\) came from the data rather than \\(p_g\\). We train D to maximize the probability of assigning the correct label to both training examples and samples from G.\n\nEn c√∫anto a los detalles, s√≥lo tenemos :\n\nwhile the discriminator net used maxout [10] activations. Dropout [17] was applied in training the discriminator net.\n\nSi lo visualizamos, ser√≠a algo como:\n\n\n\nDiscriminator\n\n\nDiscriminator\nEl proceso es el siguiente:\n\nLa entrada puede ser una imagen real proveniente del dataset (\\(p_{data}\\)) o proveniente del Generador (\\(p_{g}\\))\nLa Imagen entra al Discriminador (NN)\nEl resultado es una probabilidad [0,1], en donde 0 es imagen fake y 1 real.\n\nEs algo as√≠ como el proceso contrario de el Generador. Vamos al c√≥digo!\n#0. Define MaxOut Activation\nclass MaxOut(nn.Module):\n    def __init__(self, num_units, num_pieces):\n        super(MaxOut, self).__init__()\n        self.num_units = num_units\n        self.num_pieces = num_pieces\n        self.fc = nn.Linear(num_units, num_units * num_pieces)\n\n    def forward(self, x):\n        # Reshape the output to separate pieces\n        maxout_output = self.fc(x).view(-1, self.num_pieces, self.num_units)\n        # Take the maximum value across pieces\n        output, _ = torch.max(maxout_output, dim=1)\n        return output\n\n# 1. Imagen o Imagen Generada\nout # Esto viene del Generador que hicimos arriba (o de el dataset original)\n\n# 2. Discriminador\nclass Discriminator(nn.Module):\n    \"\"\"Generator model: In charge of classify\n    images between real and syntetic generated\n    by the generator \n\n    \"\"\"\n    def __init__(self, img_size):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(img_size*img_size, 512), #N x 512\n            MaxOut(512, 4),\n            nn.Linear(512, 256), # N x 256\n            MaxOut(256, 4),\n            nn.Linear(256, 1), # N x 1\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n# 3. Se predice\nD = Discriminator()\np = D(out)\nEl resultado es simple, ser√° algo como 0.495. Algo as√≠ como que es tan probable que sea una imagen real como fake seg√∫n el discriminador. Al comienzo puede ser algo as√≠ ya que no est√° entrenado.\nObservaciones üëÄ\n\nLa entrada es la misma dimensi√≥n que la salida del Generador (784)\nAc√° no utilizamos BatchNorm (aunque si deb√≠ hacerlo seg√∫n el paper DCGAN)\nUtilizamos MaxOut activation como lo indica el paper, √©ste no se encuentra por defecto en Pytorch, por lo que hay que definirlo ‚Äúa mano‚Äù\nOlvid√© utilizar Dropout como lo recomienda el paper, esto podr√≠a mejorar los resultados, al igual que agregar BatchNorm\nAl igual que en el Generador, la arquitectura se fue iterando seg√∫n resultados.\n\n\n\n3. Rutina de Entrenamiento\nUna vez tenemos el Generador y el Discriminador, s√≥lo nos queda entrenar. Seg√∫n el paper (lo que entend√≠), no lo hacen de la forma tradicional (iterando sobre el dataset original). Ellos proponen lo siguiente:\n\n\n\nUntitled\n\n\nProponen, en cada iteraci√≥n, primero optimizar el Discriminador por K pasos:\n\nExtraen un minibatch de imagenes reales\nExtraen un minibatch de imagenes generadas por G\nHacen un update en base a Gradient Ascent\n\nLuego, optimizan el Generador en un s√≥lo paso:\n\nExtrae un minibatch de imagenes generadas por G\nHacen un update de los par√°metros en base a Gradient Descend\n\nSeg√∫n indican en el paper, updatean m√°s veces el Discriminador, ya que buscan que el Discriminador se mantenga cercano al √≥ptimo mientras el Generador se va actualizando lentamente hasta converger.\n\nThis results in D being maintained near its optimal solution, so long as G changes slowly enough.\n\nLo que vemos ac√°, es que el generador actualiza sus par√°metros utilizando informaci√≥n implicita entregada por el Discriminador. Optimizamos el Generador para lograr enga√±ar al Discriminador.\nAlgo importante a poner atenci√≥n, es la funci√≥n objetivo, que tenemos 2, la del discriminador y la del generador, vamos a ver esto con m√°s detalle.\nLoss Functions üìé\nGenerador\nRecordemos que el Discriminador \\(D\\) es un clasificador com√∫n, por lo que podemos utilizar la funci√≥n com√∫n para estos casos (Cross Entropy Loss). Desentra√±emos esto hasta llegar a lo que tienen en el paper:\nQueremos clasificar correctamente los positivos (imagenes reales) y negativos (imagenes generadas), la funci√≥n objetivo Binary Cross Entropy est√° dada por:\n\\[\n-\\dfrac{1}{m} \\sum{y_ilog(\\hat{y_i}) + (1-y_i)log(1-\\hat{y_i})}\n\\]\ndonde \\(y_i\\) es el label (0 o 1) de la imagen i ; \\(\\hat{y_i}\\) es la predicci√≥n de la imagen i, fake o real ; \\(m\\) es el tama√±o del minibatch\nNotemos que\n\ncuando \\(y_i = 1\\), la imagen \\(x_i\\) es real , osea que \\(\\hat{y_i} = D(x_i)\\) y lo de la derecha es 0 en la ecuaci√≥n\ncuando \\(y_i=0\\), la imagen \\(G(z_i)\\) es fake ,osea que \\(\\hat{y_i} = D(G(z_i))\\) y lo de la izquierda es 0 en la ecuaci√≥n\n\nEs por esto que podemos reducir la ecuaci√≥n a:\n\\[\nMin -\\dfrac{1}{m} \\sum{ log(D(x_i)) + log(1-D(G(z_i)))}\n\\]\nSi bien esto se puede traducir a que queremos maximizar\n\\[\nMax \\dfrac{1}{m} \\sum{ log(D(x_i)) + log(1-D(G(z_i)))}\n\\]\ntal como sale en el paper (sin el signo negativo), por conveniencia no lo haremos y optaremos por minimizar el Binary Cross Entropy y asi el c√≥digo se reduce a utilizar la funci√≥n ya hecha en Pytorch BCELoss() quedando algo como\nBCELoss([D(x), D(G(z))], [1, 0])\nOtras soluciones optan por optimizarlas por separado, o por promediar ambos. En mi caso las concaten√© junto con sus targets.\n\nüí° Maximizar una funci√≥n Z es equivalente a Minimizar -Z\n\nGenerador\nPara el generador, es similar, s√≥lo que ahora s√≥lo le entregamos im√°genes falsas y adem√°s queremos que \\(D\\) se equivoque. Por lo tanto, maximizamos la funci√≥n de p√©rdida Binary Cross Entropy (que se equivoque), y s√≥lo observamos cuando \\(y_i = 0\\) (s√≥lo im√°genes falsas), teniendo\n\\[\nMax -\\dfrac{1}{m} \\sum{  log(1-D(G(z_i)))}\n\\]\nAhora lo llevamos a minimizar por conveniencia en c√≥digo (en Pytorch por defecto se busca minimizar la funci√≥n objetivo)\n\\[\nMin \\dfrac{1}{m} \\sum{  log(1-D(G(z_i)))}\n\\]\nobteniendo lo que aparece en el paper. En c√≥digo ser√≠a algo como\n-BCELoss(D(G(z)), 0) # El signo - es porque implicitamente maximizamos BCE\n\nFuaaa, cuanta matem√°tica ü§Øü§Øü§Ø, comprobemos esto de forma intuitiva:\nPara el Discriminador tenemos entonces:\n\\[\nMax \\dfrac{1}{m} \\sum{ log(D(x_i)) + log(1-D(G(z_i)))}\n\\]\nsi lo separamos, queremos entonces:\n\\[\nMax \\dfrac{1}{m} \\sum{ log(D(x_i)) }\n\\]\n\\[\nMax \\dfrac{1}{m} \\sum{ log(1-D(G(z_i)) }\n\\]\nLa primera funci√≥n (olvidando la suma y eso) es \\(ln(x)\\) ‚Üí si, la base utilizada es la exponencial, por lo que es el logaritmo natural. Su gr√°fica es:\n\n\n\nNotes_240225_163331.jpg\n\n\nEl eje horizontal es \\(D(x_i)\\): La probabilidad de que la imagen real \\(x_i\\) sea clasificada como real, con un rango [0,1]. Ya que la imagen es real, queremos que D prediga un valor alto (ojal√° 1), lo que si vemos la gr√°fica es igual a alcanzar el m√°ximo de la funci√≥n (m√°x optimo). Es por esto que maximizamos \\(\\dfrac{1}{m} \\sum{ log(D(x_i)) }\\)\nAhora vamos con \\(log(1-D(G(z_i))\\) , cuya gr√°fica es:\n\n\n\nUntitled\n\n\nAhora la gr√°fica cambia, el eje horizontal est√° dado por \\(D(G(z_i))\\): La probabilidad de que la imagen falsa \\(G(z_i)\\) sea clasificada como real. Vemos que la funci√≥n se maximiza cuando \\(D(G(z_i))\\) se acerca a 0, y esto es lo que queremos porque \\(G(z_i)\\) es una imagen falsa y debe ser clasificada como tal en el Discriminador (ojal√° 0). Es por esto que buscamos maximizar \\(\\dfrac{1}{m} \\sum{ log(1-D(G(z_i)) }\\)\n\nüî• IMPORTANTE! Ac√° vemos la funci√≥n objetivo tal y como est√° en el paper (maximizando), pero en el c√≥digo, por conveniencia, lo llevamos a minimizar la BCELoss, lo cual es equivalente, tal como lo mostramos en la parte matem√°tica. √âsto en los gr√°ficos es parecido s√≥lo que ahora buscamos minimizar ambas partes, dejar√© este ejercicio como tarea para el lector!\n\nPara el Generador, es muy similar a la segunda ecuaci√≥n del Discriminador, s√≥lo que ahora buscamos minimizar (ir hacia a la derecha).\n\n\n\nUntitled\n\n\nVemos que la idea es ir hacia la derecha porque queremos que el Discriminador se equivoque, o sea que aunque le entreguemos una imagen falsa \\(D(G(z_i))\\), el discriminador prediga que es real (se acerque a 1). Si vemos bien, a medida \\(D(G(z_i))\\) se acerca a 1, la funci√≥n \\(\\dfrac{1}{m} \\sum{  log(1-D(G(z_i)))}\\) se acerca a \\(-\\infin\\). Esto puede provocar inestabilidad en la optimizaci√≥n, y es por esto que mucha gente utiliza la equivalencia:\n\\[\nMin \\dfrac{1}{m} \\sum{ log(D(G(z_i))) }\n\\]\ncomo funci√≥n objetivo de el Generador. Ac√° vemos que utilizamos la parte donde \\(y_i = 1\\) , osea que la imagen es real, pero minimizamos haciendo que el Discriminador diga que es falsa. Intuitivamente se pierde un poco el sentido ya que en realidad la imagen entregada no es real, pero matem√°ticamente es equivalente y logra mayor estabilidad y al minimizar el √≥ptimo es 0 y no \\(-\\infin\\) (a√∫n as√≠ yo utilic√© la primera forma)\nEspero esta secci√≥n se haya entendido, es un tanto complicado de escribir con palabras. El proceso de entendimiento de la funci√≥n objetivo, fue un proceso bastante entretenido e interesante, que sin duda me sirvi√≥ para mejorar algunas aptitudes a la hora de leer papers. A√∫n as√≠ mencionar que es importante apoyarse en la literatura (libros, blogs, papers), el objetivo no es comprobar que tu podr√≠as haber llegado a las mismas conclusiones que los autores por tu lado, si no que eres capaz de entender (matem√°tica- o intuitiva- mente) lo que se propone. Invito al lector a iniciar una conversaci√≥n conmigo si le qued√≥ alguna duda, o si not√≥ que pude haberme equivocado en algo.\n\nTrain üèãüèæ\nAhora ya podemos comenzar a entrenar el modelo, primero vamos a inicializar algunos componentes importantes:\n\nConfiguraci√≥n\n\n# CFG\nlatent_size = 100 # noise dimension\nimg_size = 28 # image shape\ndevice = \"cuda\" # GPU\nepochs = 20000 # TRAINING ITERATIONS\nk = 1 # Discriminator steps\n\nModelos: Utilizamos los modelos construidos anteriormente\n\n# Models \nG = Generator(cfg.latent_dim, cfg.img_size)\nD = Discriminator(cfg.img_size)\n\nG.to(cfg.device)\nD.to(cfg.device)\n\nLoss Functions: Recordemos que gracias a que re-definimos la matem√°tica de las funciones objetivos, podemos ocupar la funci√≥n pre-construida en Pytorch BCELoss()\n\n# Losses\nD_LOSS = nn.BCELoss()\nG_LOSS = nn.BCELoss()\n\nOptimizadores: Utilizamos Adam, con un learning rate bastante m√°s peque√±o que el usual y unos betas espec√≠ficos. Esto es uno de los problemas de GAN, es muy sensible a los hyperpar√°metros, cuando normalmente los modelos (Deeplearning) son robustos a estos.\n\n# Optimizers\nd_optimizer = torch.optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))\ng_optimizer = torch.optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n\nDataset: Ya que MNIST es bastante conocido, ya se encuentra disponible en Pytorch para descargar\n\n# Datasets (Images and Noise)\ntransform = transforms.Compose([\n    transforms.Resize((cfg.img_size, cfg.img_size)),\n    transforms.ToTensor(),  # Convert PIL image or numpy array to tensor\n    transforms.Normalize((0.5,), (0.5,))  # Normalize the tensor with mean and standard deviation\n])\n\ndataset = MNIST(root = '', download = True, transform =transform )\nsampler = RandomSampler(dataset) # To get random images each iteration\n\n# DataLoader\noriginal_dl = DataLoader(dataset, batch_size = cfg.batch_size, sampler = sampler, pin_memory=torch.cuda.is_available())\nNotar que ac√° normalizamos las im√°genes para que est√©n en el rango [-1,1] y nos aseguramos que tengan el mismo tama√±o 28x28. Tambi√©n, ya que no iremos avanzando batch por batch, iremos sampleando aleatoriamente el dataset tal como lo dice la rutina de entrenamiento pertenciente al paper.\n\nRutina de entrenamiento: Iremos sentencia por sentencia navegando la rutina de entrenamiento y codeando!\n\n\nfor number of training iterations do\n\nfor epoch in range(cfg.epochs):\n\nfor k steps do\n\nfor epoch in range(cfg.epochs):\n    for k in range(cfg.k):\n\nSample minibatch of m noise samples {z(1), . . . , z(m)} from noise prior \\(p_g(z)\\).\n\nfor epoch in range(cfg.epochs):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n\nSample minibatch of m examples {x(1), . . . , x(m)} from data generating distribution \\(p_{data}(x)\\)\n\nfor epoch in range(cfg.epochs):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n        # original minibatch\n        x, _ = next(iter(original_dl))\n\nUpdate the discriminator by ascending its stochastic gradient\n\nfor epoch in range(cfg.epochs):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n        # original minibatch\n        x, _ = next(iter(original_dl))\n\n        ##############################\n    ## Discriminator Optimization \n    ##############################\n        d_optimizer.zero_grad()\n        G_z = G(z) # Generated Image\n        D_x  = D(x) # real image's probability of being real\n        D_G_z = D(G_z.detach()) # fake image's probability of being real\n        \n        # Concat real and fakes\n        samples = torch.cat([D_G_z, D_x]).to(cfg.device)\n        # Make targets (1 for real, 0 for fakes)\n        targets = torch.cat([torch.zeros(D_G_z.size()[0]), torch.ones(D_x.size()[0])]).to(cfg.device)\n\n        # Discriminator Loss\n        # Loss\n    d_loss = D_LOSS(samples, targets.unsqueeze(-1))\n        d_loss.backward() # backward\n        \n        # Adjust learning weights\n    d_optimizer.step()\n\nEnd for. Sample minibatch of m noise samples {z(1), . . . , z(m)} from noise prior \\(p_g(z)\\).\n\nfor epoch in tqdm(range(cfg.epochs)):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n        # original minibatch\n        x, _ = next(iter(original_dl))\n\n        ##############################\n    ## Discriminator Optimization \n    ##############################\n        d_optimizer.zero_grad()\n        G_z = G(z) # Generated Image\n        D_x  = D(x) # real image's probability of being real\n        D_G_z = D(G_z.detach()) # fake image's probability of being real\n        \n        # Concat real and fakes\n        samples = torch.cat([D_G_z, D_x]).to(cfg.device)\n        # make targets (1 for real, 0 for fakes)\n        targets = torch.cat([torch.zeros(D_G_z.size()[0]), torch.ones(D_x.size()[0])]).to(cfg.device)\n\n        # Discriminator Loss\n        # Loss\n    d_loss = D_LOSS(samples, targets.unsqueeze(-1))\n        d_loss.backward() # backward\n        \n        # Adjust learning weights\n    d_optimizer.step()\n    \n    # Minibatch from noise prior\n    z = torch.randn(batch_size, cfg.latent_dim).to(cfg.device)\n\nUpdate the generator by descending its stochastic gradient:\n\nfor epoch in tqdm(range(cfg.epochs)):\n    for k in range(cfg.k):\n        # noise minibatch \n        z = torch.randn(batch_size, cfg.latent_dim)\n        # original minibatch\n        x, _ = next(iter(original_dl))\n\n        ##############################\n    ## Discriminator Optimization \n    ##############################\n        d_optimizer.zero_grad()\n        G_z = G(z) # Generated Image\n        D_x  = D(x) # real image's probability of being real\n        D_G_z = D(G_z.detach()) # fake image's probability of being real\n        \n        # Concat real and fakes\n        samples = torch.cat([D_G_z, D_x]).to(cfg.device)\n        # make targets (1 for real, 0 for fakes)\n        targets = torch.cat([torch.zeros(D_G_z.size()[0]), torch.ones(D_x.size()[0])]).to(cfg.device)\n\n        # Discriminator Loss\n        # Loss\n    d_loss = D_LOSS(samples, targets.unsqueeze(-1))\n        d_loss.backward() # backward\n        \n        # Adjust learning weights\n    d_optimizer.step()\n    \n    # Minibatch from noise prior\n    z = torch.randn(batch_size, cfg.latent_dim).to(cfg.device)\n\n  ##############################\n  ## Generator Optimization \n  ##############################\n  g_optimizer.zero_grad()\n    G_z = G(z) # Generated Images\n    D_G_z = D(G_z) # fake/generated image's probability of being real\n    # targets\n    targets = torch.zeros(D_G_z.size()[0]).to(cfg.device)\n    # Loss\n  g_loss = -G_LOSS(D_G_z, targets.unsqueeze(-1)) # Max BCE\n  g_loss.backward()\n\n  # Adjust learning weights\n  g_optimizer.step()\nY con eso ya estamos! a correr y a observar los resultados, a continuaci√≥n algunas observaciones:\n\nTambi√©n se puede hacer el entrenamiento com√∫n (minibatch stochastic gradient descend )\nVi que en algunos lados utilizan el mismo noise \\(z\\) para ambas partes de la optimizaci√≥n\nEn la linea D_G_z = D(G_z.detach()) , el .detach() es para no actualizar parametros del Generador en la parte del Discriminador, por lo que quitamos G_z de del grafo computacional.\nEntren√© por 20.000 iteraciones! se demor√≥ aprox 15 minutos. Notemos que no son epochs (pasadas por el training set), si no que s√≥lo iteraciones del algoritmo.\n\n\n‚ö†Ô∏è WARNING: Si quieres utilizar el c√≥digo, te recomiendo verlo directamente desde Github ya que ac√° quit√© algunas cosas para que se pueda entender mejor\n\n\n\n4. Resultados\nLo primero a observar es la curva de aprendizaje, que no es tan bonita\n\n\n\nUntitled\n\n\nPodemos ver lo inestable que es el entrenamiento, aunque si tiende a minimizar ambas funciones. Ahora vemos que im√°genes genera:\n\n\n\nUntitled\n\n\nFuaaa ü§©¬†Parecer ser que el modelo intenta converger y genera algunas im√°genes que si podrian enga√±ar al ojo humano.\nSi te lo est√°s preguntando, la evaluaci√≥n real propuesta en el paper no es simple, en el paper utilizan unas t√©cnicas espec√≠ficas, pero para dejarlo simple, ac√° simplemente evaluaremos con la vista!\n\n\n\nUntitled\n\n\nAlgo importante es que idealmente en generaci√≥n de im√°genes no se aprenda de memoria las caracter√≠sticas de el conjunto de entrenamiento, si no que le agregue un poco de su saz√≥n! e.g al generar caras, en donde el set de entrenamiento no tienen ninguna cara con barba, nos gustar√≠a que pudiese generar caras con barba (esto es una limitante)\nVeamos como va evolucionando el modelo cada 1000 iteraciones!\n\n\n\nIteraci√≥n 0\n\n\nIteraci√≥n 0\n\n\n\nIteraci√≥n 2000\n\n\nIteraci√≥n 2000\n\n\n\nIteraci√≥n 1000\n\n\nIteraci√≥n 1000\n\n\n\nIteraci√≥n 3000\n\n\nIteraci√≥n 3000\n\n\n\nIteraci√≥n 4000\n\n\nIteraci√≥n 4000\n\n\n\nIteraci√≥n 15000\n\n\nIteraci√≥n 15000\nObservamos que ya a partir de la iteraci√≥n 4000 logra hacer un ‚Äú6‚Äù bastante decente."
  },
  {
    "objectID": "posts/llm-recipe/main.html",
    "href": "posts/llm-recipe/main.html",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "",
    "text": "Note\n\n\n\nAntes de comenzar a leer esto, recuerda que yo estoy aprendiendo junto contigo. Si tienes alguna duda, sugerencia, correci√≥n o comentario, no dudes en escribirme a mi LinkedIn."
  },
  {
    "objectID": "posts/llm-recipe/main.html#prompt",
    "href": "posts/llm-recipe/main.html#prompt",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "Prompt:",
    "text": "Prompt:\nImaginemos queremos saber los ingredientes para cocinar una lasagna, entonces escribiremos algo del estilo:\n\n\n\n\n\n\nNoteüë©üèº‚Äçüî¨\n\n\n\n¬øQue necesito para cocinar una Lasagna?\n\n\nEsta pregunta, que es la entrada de un LLM se le denomina prompt. Un t√©rmino bastante conocido hasta ahora, que de hecho se asocia a un rol, es Prompt Engineering. Podemos entender este t√©rmino como el ‚Äúarte‚Äù de escribir el mejor prompt para obtener la respuesta deseada.\n‚ÄúCuida la forma en la que pides las cosas‚Äù me dec√≠a mi mam√° cuando ni√±o al pedirle un favor a alguien. Las mam√°s siempre tienen la raz√≥n, y esta no es la excepci√≥n.\nImaginemos que la respuesta de la LLM es algo como:\n\n\n\n\n\n\nNoteü§ñ\n\n\n\nPara cocinar una lasagna necesitas un horno, un cuchillo, una cocina y los ingredientes.\n\n\nNo es la respuesta que esperabamos! nosotros en realidad quer√≠amos saber los ingredientes, pero nos expresamos mal. Es por esto que com√∫nmente se suele iterar el prompt hasta conseguir la respuesta deseada, imaginemos que luego de iterar un poco llegamos al prompt final:\n\n\n\n\n\n\nNoteüë©üèº‚Äçüî¨\n\n\n\n¬øCuales son los ingredientes que necesito para cocinar una lasagna?\n\n\nProbablemente con un prompt as√≠ obtengamos lo que buscamos. No hay un prompt √≥ptimo, pero si existen muchos prompt que nos conseguir√°n la respuesta que buscamos.\nM√°s adelante veremos algunas t√©cnicas de prompt engineering. Por ahora nos basta con saber que el prompt ser√° elemento importante de nuestra soluci√≥n."
  },
  {
    "objectID": "posts/llm-recipe/main.html#output",
    "href": "posts/llm-recipe/main.html#output",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "Output:",
    "text": "Output:\nAlgo que nos debemos cuestionar es: ¬øComo necesitamos el resultado? algunas de las opciones son: Una lista de ingredientes, un json, un DataFrame, etc.\nEn este caso lo que decid√≠ fue obtener un json, el cual luego convertir√≠a a un DataFrame. Los elementos que tendr√° la respuesta son:\n\nIngredient: Nombre del ingrediente\nQuantity: Cantidad necesaria\nOptional: ‚ÄúYes‚Äù si el ingrediente es opcional, ‚ÄúNo‚Äù si es obligatorio\nEstimated Price: Un precio estimado en d√≥lares del ingrediente (as√≠ podremos calcular alg√∫n valor aproximado de la receta)\nAvailable: Una simulaci√≥n de disponibilidad del ingrediente en el supermercado. ‚ÄúYes‚Äù si est√° disponible, ‚ÄúNo‚Äù si no lo est√°.\n\nAc√° tenemos un ejemplo:\n{\n  \"Spaguetthi With Meat\":\n  [\n    {\n      \"ingredient\": \"Spaguetti\",\n      \"optional\": \"No\",\n      \"quantity\": \"200g\",\n      \"estimated_price\": \"5.00\",\n      \"available\": \"No\"\n    },\n    {\n      \"ingredient\": \"Meat\",\n      \"optional\": \"No\",\n      \"quantity\": \"1kg\",\n      \"estimated_price\": \"10.00\",\n      \"available\": \"Yes\"\n    },\n    {\n      \"ingredient\": \"Pepper\",\n      \"optional\": \"Yes\",\n      \"quantity\": \"at ease\",\n      \"estimated_price\": \"1.00\",\n      \"available\": \"No\"\n    }\n\n  ]\n}\nUna pregunta importante es, ¬øC√≥mo lograremos que el LLM nos estructure el output como lo requerimos?\nSPOILER: Prompt Engineering"
  },
  {
    "objectID": "posts/llm-recipe/main.html#obtener-receta-mediante-llm",
    "href": "posts/llm-recipe/main.html#obtener-receta-mediante-llm",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "1. Obtener receta mediante LLM",
    "text": "1. Obtener receta mediante LLM\n\nPrompt Templates\nEn este punto vale la pena preguntarnos, c√≥mo esperamos que el usuario interact√∫e con nuestra aplicaci√≥n? Queremos que el usuario haga la pregunta completa? Ahora sabemos que esto no es una buena idea por varias razones:\n\nEl usuario podria ingresar incluso preguntas que no est√©n relacionadas con la aplicaci√≥n (comida)\nEl usuario puede preguntar de forma ineficiente\nObtener la estructura json que deseamos ser√≠a imposible\nEl usuario no sabe de Prompt Engineering\n\nLa idea es que el usuario s√≥lo ingrese el nombre de la comida, y por detr√°s nuestra aplicaci√≥n haga el resto. Para esto, Langchain cuenta con una herramienta llamada Prompt Templates, que como su nombre lo indica es una plantilla del prompt.\nEsto es, imaginemos nuestra plantilla es: ‚Äú¬øCuales son los ingredientes que necesito para cocinar {COMIDA}?‚Äù Entonces si la entrada del usuario es ‚ÄúLasagna‚Äù, el prompt quedar√° ‚Äú¬øCuales son los ingredientes que necesito para cocinar Lasagna?‚Äù.\n\n\nüë®üèæ‚Äçüíª¬†Code time!\nPara crear un template, primero debemos definir la estructura:\ntemplate_string = \"\"\"\nGive me a list of ingredients to cook {food}.\n\"\"\"\nNotemos que el input en este caso es ‚Äúfood‚Äù. Luego usamos Langchain\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = PromptTemplate.from_template(template_string)\nLuego simplemente obtenemos el prompt final para entregarle al modelo de la siguiente forma:\nuser_input = 'Lasagna'\nfinal_prompt = prompt.format(food=user_input)\nAs√≠ de simple! Ahora quiero que nos compliquemos un poco m√°s la vida. ChatGPT hizo unos cambios en su API, por lo que en Langchain ahora aparece un nuevo elemento llamado ChatPromptTemplate\nLa idea de este template, que si bien tambi√©n acepta entradas como las vistas anteriormente, ahora permite ingresar mensajes con roles. Existen tres tipos de roles: System, Human, AI. Bien brevemente te explico que deberian ser:\n\nSystem Message: Las instrucciones que se le quiere entregar al modelo\nHuman Message: Las entradas del usuario\nAI Message: Alguna respuesta por parte de el modelo\n\nAc√° te dejo un post del por qu√© de esta implementaci√≥n, que viene de la mano con ChatModels: https://blog.langchain.dev/chat-models/\nDebido a que s√≥lo el paso 1 utiliza input de usuario, s√≥lo lo haremos as√≠ en este paso:\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\n#¬†System template\nfirst_system_template_str = \"\"\"\n    You are a good chef, users need you to bring them recipes from given food.\n\"\"\"\nfirst_system_template = SystemMessagePromptTemplate.from_template(first_system_template_str)\n\n# Human Template\nfirst_human_template_str = \"{food}\"\nfirst_human_template = HumanMessagePromptTemplate.from_template(first_human_template_str)\n\n# First Prompt Template\nfirst_prompt = ChatPromptTemplate.from_messages([first_system_template, first_human_template])\n\n\nLLM Model\nAhora que ya tenemos una entrada para nuestro modelo, necesitamos llamarlo!\nPara poder hacer uso de un modelo de LLM (En este caso ChatOpenAI) primero debemos crearnos una cuenta en openai y crear una nueva API Key. Luego, debemos crear un archivo .env en la ra√≠z del proyecto con el siguiente contenido:\nOPENAI_API_KEY = \"&lt;tu api key&gt;\"\n\n\n\n\n\n\nWarning\n\n\n\nNo debes dejar que nadie vea tu API KEY, asi que no subas tu .env a ning√∫n lugar p√∫blico!\n\n\nSi no, puedes agregarlo directamente utilizando la libreria openai o Langchain.Luego de obtener y entregar tu API KEY, en Langchain basta con hacer algo como:\nfrom langchain.chat_models import ChatOpenAI\n\nchat = ChatOpenAI(temperature=0.0)\nprompt = \"Porfavor hazme la tesis!\"\n\nresult = chat(prompt)\nEn este caso, estaremos utilizando dos LLM, en donde la salida de una es la entrada de otra. Langchain ya tiene algo preparado para esto! se les llama Chains. En este caso, tenemos una cadena simple (Dos LLM secuenciales), pero existen otros tipos de arquitecturas mucho m√°s complejas ‚ò†Ô∏è\nPara poder crear nuestra primera Chain no es tanto m√°s complejo que el ejemplo anterior, s√≥lo debemos agregar unos pasos extras:\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models import ChatOpenAI\n\n# LLM Definition\nllm = ChatOpenAI(temperature=0)\n\n# Chain Step 1\nrecipe_chain = LLMChain(llm=llm, prompt=first_prompt, output_key=\"recipe\")\nEn el c√≥digo anterior estamos utilizando la LLM ChatOpenAI, y creando la primera parte de la cadena. El par√°metro output_key nos servir√° m√°s adelante para comunicarle a la segunda parte cual es el nombre del primer output."
  },
  {
    "objectID": "posts/llm-recipe/main.html#obtener-ingredientes-de-una-receta-mediante-llm",
    "href": "posts/llm-recipe/main.html#obtener-ingredientes-de-una-receta-mediante-llm",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "2. Obtener ingredientes de una receta mediante LLM",
    "text": "2. Obtener ingredientes de una receta mediante LLM\nYa sabemos que la salida del paso anterior ser√° una receta, a la cual deberemos extraer los ingredientes. Adem√°s, √©ste es el √∫ltimo paso, por lo que deberemos preocuparnos de que el output que salga del LLM sea adecuado y simple de ‚Äúparsear‚Äù.\nPara el output, lo que haremos ser√° aplicar un poco del ya conocido prompt engineering.\nAdem√°s, en este caso, ya que no existen tantos riesgos de prompt injection , es que usaremos templates sencillos (no haremos uso de los roles en los mensajes)\nsecond_step_template = \"\"\"I need you to bring me the ingredients contained in the following recipe: \\\nrecipe: {recipe}\n{format_instructions}\n{example_instructions}\"\"\"\nVemos que tenemos tres entradas:\n\nrecipe: Este input ser√° el resultado del paso anterior.\nformat_instructions: Ac√° le comunicaremos al LLM como queremos el formato del output.\nexample_instructions: Ac√° aplicamos un poco de lo que se llama few-shot , que es b√°sicamente darle un par de ejemplos al LLM para que entienda como esperamos el resultado.\n\n# Format Instructions\ncustom_format_instructions = \"\"\"\nThe output should be in a json format, formatted in the following schema:\n{\n  \"food\": List // List of ingredients\n  [\n    {\n      \"ingredient\": string // Name of one ingredient\n      \"quantity\": string  // Quantity of the ingredient \n      \"optional\": string  // Whether or not that ingredient is optional to cook the food. \"Yes\" if the ingredient is not indispensable to cook, \"No\" if is the ingredient is indispensable.\n      \"estimated_price\": string  // The ingredient's estimated price in dolars\n      \"available\": string // Random \"Yes\" or \"No\"\n    }\n  ]\n}\n\"\"\"\n    example_instructions = \"\"\"\nFollow the schema of this example:\n{\n  \"food\":\n  [\n    {\n      \"ingredient\": \"Spaguetti\",\n      \"optional\": \"No\",\n      \"quantity\": \"200g\",\n      \"estimated_price\": \"5.00\",\n      \"available\": \"No\"\n    },\n    {\n      \"ingredient\": \"Meat\",\n      \"optional\": \"No\",\n      \"quantity\": \"1kg\",\n      \"estimated_price\": \"10.00\",\n      \"available\": \"Yes\"\n    },\n    {\n      \"ingredient\": \"Pepper\",\n      \"optional\": \"Yes\",\n      \"quantity\": \"at ease\",\n      \"estimated_price\": \"1.00\",\n      \"available\": \"No\"\n    }\n\n  ]\n}\n\"\"\"\n\n\n\n\n\n\nWarning\n\n\n\n‚úãüèΩ Es importante que sepas que langchain cuenta con un m√≥dulo de Output parser que crea por detr√°s el format_instructions e incluso cuenta con funciones que transforman la salida del LLM(string) en el formato que deseabamos (json, lista, Pydantic, etc). La raz√≥n de porqu√© yo no ocup√© esto fue que no funciona para json nesteados. Pero me bas√© en sus instrucciones para crear custom_format_instructions\n\n\nLuego seguimos como lo vimos anteriormente:\nsecond_prompt = ChatPromptTemplate.from_template(second_step_template)\ningredient_chain = LLMChain(llm=llm, prompt=second_prompt, output_key=\"ingredients\")"
  },
  {
    "objectID": "posts/llm-recipe/main.html#cadena-final",
    "href": "posts/llm-recipe/main.html#cadena-final",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "3. Cadena Final",
    "text": "3. Cadena Final\nLuego necesitamos orquestar las cadenas que creamos anteriormente para obtener la cadena final. En Langchain se hace de la siguiente forma:\noverall_simple_chain = SequentialChain(chains=[recipe_chain, ingredient_chain], verbose=True,\n                                       input_variables=[\"food\", \"format_instructions\", \"example_instructions\"],\n                                       output_variables=[\"recipe\", \"ingredients\"])"
  },
  {
    "objectID": "posts/llm-recipe/main.html#output-parser",
    "href": "posts/llm-recipe/main.html#output-parser",
    "title": "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT",
    "section": "4. Output Parser",
    "text": "4. Output Parser\nFinalmente, necesitamos parsear el resultado obtenido desde la LLM. Esto es string ‚Üí json.\nDebido a la forma en que le pedimos el resultado al LLM, es que se nos hace muy sencillo:\nimport json\nimport pandas as pd\n\nresult = overall_simple_chain(\n        {\n            \"food\": food, # Esto lo entrega el usuario\n            \"format_instructions\": custom_format_instructions, # Esto lo entregamos nosotros\n            \"example_instructions\": example_instructions, # Esto lo entregamos nosotros\n        }\n    )\n\n# String to json(dict en python)\ndict_response = json.loads(result['ingredients']) \n\n# Dict to df\noutput_df = pd.DataFrame(data = dict_response['food'])\nFinalmente, para la entrada ‚ÄúLasagna‚Äù, podemos obtener algo asi:\n\n\n\nOutput 1 as DataFrame"
  },
  {
    "objectID": "posts/birdclef2023/main.html#short-time-fourier-transform",
    "href": "posts/birdclef2023/main.html#short-time-fourier-transform",
    "title": "BirdClef 2023 Competition",
    "section": "Short Time Fourier Transform",
    "text": "Short Time Fourier Transform\nSabiendo ahora que lo que necesitamos es obtener espectogramas, ahora necesitamos saber como. Para tener un espectograma, necesitamos calcular las variaciones de frecuencias a trav√©s del tiempo, por lo que necesitamos calcular frecuencias. Una metodolog√≠a muy conocida en el √°rea de matem√°ticas que se utiliza para esto es la transformada de fourier. Lamentablemente existen unos problemas al intentar aplicarla a todo un audio, es por esto que se utiliza una t√©cnica llamada windowing, que lo que busca es dividir el audio en frames y aplicar la transformada de fourier a cada frame. Esta t√©cnica se conoce como Short-time Fourier Transform (STFT)\nVeamos como calcularlo:\nLa transformada de Fourier de tiempo corto (Short Time Fourier Transform o STFT) es una t√©cnica ampliamente utilizada en el procesamiento de audio y se utiliza com√∫nmente en aplicaciones de aprendizaje autom√°tico. Perm√≠teme explicarte c√≥mo se calcula y los conceptos clave asociados.\nLa STFT es una forma de analizar se√±ales de audio en dominio de frecuencia a lo largo del tiempo. Permite descomponer una se√±al de audio en sus componentes de frecuencia en intervalos de tiempo peque√±os y consecutivos. Esto es √∫til para capturar informaci√≥n detallada sobre c√≥mo cambian las frecuencias a lo largo del tiempo en una se√±al de audio.\nA continuaci√≥n, describir√© los conceptos clave involucrados en el c√°lculo de la STFT:\n\nVentaneo (Windowing): El ventaneo es una t√©cnica utilizada para evitar artefactos en la transformada de Fourier al considerar solo una secci√≥n finita de la se√±al a la vez. La se√±al de audio se multiplica por una funci√≥n de ventana, que es b√°sicamente una funci√≥n que se desvanece hacia los extremos. Al multiplicar la se√±al por la ventana, se reduce la cantidad de energ√≠a ‚Äúderramada‚Äù en las frecuencias adyacentes, lo que mejora la resoluci√≥n en frecuencia. Las funciones de ventana comunes incluyen la ventana rectangular, la ventana Hamming, la ventana Blackman, entre otras.\nTama√±o de ventana (Window Size o Frame Size): El tama√±o de ventana se refiere al n√∫mero de muestras de la se√±al de audio que se consideran para calcular la transformada de Fourier en un momento dado. Espec√≠ficamente, la se√±al se divide en segmentos superpuestos de tama√±o de ventana y se aplica la funci√≥n de ventana a cada segmento. Un tama√±o de ventana m√°s grande proporciona una mejor resoluci√≥n en frecuencia, pero reduce la resoluci√≥n en el tiempo, mientras que un tama√±o de ventana m√°s peque√±o tiene el efecto contrario.\nSalto de ventana (Hop Size): El salto de ventana, tambi√©n conocido como desplazamiento o hop size, se refiere a la cantidad de muestras que se desplaza la ventana despu√©s de calcular la transformada de Fourier para un segmento. Un salto de ventana m√°s peque√±o resulta en una mayor superposici√≥n entre los segmentos adyacentes y, por lo tanto, proporciona una mejor resoluci√≥n en el tiempo. Sin embargo, tambi√©n implica un mayor costo computacional. Un salto de ventana m√°s grande reduce la superposici√≥n y aumenta la resoluci√≥n en frecuencia.\n\nEl proceso general para calcular la STFT se realiza de la siguiente manera:\n\nDivide la se√±al de audio en segmentos superpuestos de tama√±o de ventana.\nAplica una funci√≥n de ventana a cada segmento para reducir el efecto de los extremos.\nCalcula la transformada de Fourier de cada segmento.\nRepite los pasos anteriores desplazando la ventana por el salto de ventana hasta que se haya cubierto toda la se√±al de audio.\n\nEl resultado de la STFT es una matriz 2D que representa la magnitud o la fase de las componentes de frecuencia a lo largo del tiempo. Esta matriz se puede utilizar posteriormente en t√©cnicas de aprendizaje autom√°tico, como redes neuronales, para extraer caracter√≠sticas o para realizar tareas espec√≠ficas, como clasificaci√≥n de audio, detecci√≥n de eventos,entre otras.\nEn resumen, la STFT es una herramienta poderosa en el procesamiento de audio y el aprendizaje autom√°tico. Permite analizar las caracter√≠sticas de frecuencia que cambian en el tiempo en una se√±al de audio y se calcula mediante el ventaneo de la se√±al, el c√°lculo de la transformada de Fourier en cada segmento y el desplazamiento de la ventana a lo largo de la se√±al.\n\n# Seteamos el tama√±o del frame (en cantidad de samples) y el tama√±o del HOP\nFRAME_SIZE = 1024\nHOP_SIZE = FRAME_SIZE//2\n\n\n# Utilizamos librosa para aplicar stft\ns_audio = librosa.stft(audio_data, n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n\n\ns_audio.shape #(frequency bins, frames) # Complex numbers\n\n(513, 216)\n\n\n\nFRAME_SIZE//2 + 1, (n_samples + FRAME_SIZE)//HOP_SIZE + 1\n\n(513, 218)\n\n\nLa t√©cnica STFT nos entrega n√∫meros complejos, para poder entenderlos e ingresarlos a un modelo de machine learning, calculamos su magnitud\n\n# We get the magnitude \nm_audio = np.abs(s_audio)**2\n\n¬øComo observamos de otra forma un audio?\nUn espectrograma es una representaci√≥n visual que muestra c√≥mo var√≠a la energ√≠a espectral de una se√±al a lo largo del tiempo. Muestra c√≥mo se distribuyen las diferentes frecuencias en una se√±al en funci√≥n del tiempo. En un espectrograma convencional, la amplitud de las frecuencias se representa mediante colores o tonos de grises, donde los colores m√°s brillantes o m√°s claros indican una mayor amplitud y los colores m√°s oscuros indican una menor amplitud.\nCon esto, ahora podemos observar los siguientes espectogramas:\n\n# Plot Spectogram\ndef plot_spectrogram(Y, sr, hop_length, y_axis=\"linear\"):\n    \"\"\"\n    Plot a spectrogram of an audio signal.\n\n    Parameters:\n    - Y (ndarray): 2D array representing the spectrogram.\n    - sr (int): Sampling rate of the audio signal.\n    - hop_length (int): Number of samples between successive frames.\n    - y_axis (str, optional): Scale of the frequency axis. Default is \"linear\".\n\n    Returns:\n    None\n\n    This function plots a spectrogram of the audio signal using the provided parameters. The spectrogram represents\n    the distribution of frequencies over time.\n\n    Example usage:\n    &gt;&gt;&gt; plot_spectrogram(Y, sr=22050, hop_length=512, y_axis=\"log\")\n\n    \"\"\"\n    plt.figure(figsize=(25, 10))\n    librosa.display.specshow(Y, \n                             sr=sr, \n                             hop_length=hop_length, \n                             x_axis=\"time\", \n                             y_axis=y_axis)\n    plt.colorbar(format=\"%+2.f\")\n\n\nplot_spectrogram(m_audio, sampling_rate, HOP_SIZE)\n\n\n\n\n\n\n\n\nPodemos notar que las frecuencias son muy bajas, para poder observarlo de mejor forma, calculamos el logaritmo, lo que equivale a transformar el sonido a decibeles (algo que los humanos podemos entender)."
  },
  {
    "objectID": "posts/birdclef2023/main.html#log-amplitud-spectogram",
    "href": "posts/birdclef2023/main.html#log-amplitud-spectogram",
    "title": "BirdClef 2023 Competition",
    "section": "Log-Amplitud Spectogram",
    "text": "Log-Amplitud Spectogram\nPor otro lado, un espectrograma de amplitud logar√≠tmica (Log-Amplitude Spectrogram) aplica una transformaci√≥n logar√≠tmica a la amplitud de las frecuencias antes de su representaci√≥n visual. En lugar de mostrar la amplitud directamente, se muestra el logaritmo de la amplitud. Esto se hace para mejorar la interpretaci√≥n visual y resaltar mejor las caracter√≠sticas de baja amplitud en la se√±al.\nExisten varias razones por las que se utiliza un espectrograma de amplitud logar√≠tmica en lugar de un espectrograma normal:\nEscala perceptual: La audici√≥n humana no percibe la amplitud de manera lineal, sino de forma logar√≠tmica. Al aplicar la transformaci√≥n logar√≠tmica, el espectrograma de amplitud logar√≠tmica se ajusta m√°s a c√≥mo percibimos el sonido, lo que facilita su interpretaci√≥n.\nRango din√°mico: El uso de una escala logar√≠tmica comprime el rango din√°mico del espectrograma. Esto significa que las diferencias de amplitud m√°s peque√±as se vuelven m√°s visibles y f√°ciles de distinguir, lo que permite apreciar mejor las caracter√≠sticas de baja amplitud en la se√±al.\nReducci√≥n de ruido: Al aplicar una transformaci√≥n logar√≠tmica, las partes m√°s ruidosas de la se√±al se aten√∫an, lo que ayuda a reducir la presencia visual del ruido y mejora la legibilidad del espectrograma.\n\n# To a better visualization, we obtain logs (transform to decibels)\n\nY_log_scale = librosa.power_to_db(m_audio)\nplot_spectrogram(Y_log_scale, sampling_rate, HOP_SIZE)"
  },
  {
    "objectID": "posts/birdclef2023/main.html#log-frequency-spectogram",
    "href": "posts/birdclef2023/main.html#log-frequency-spectogram",
    "title": "BirdClef 2023 Competition",
    "section": "Log-Frequency Spectogram",
    "text": "Log-Frequency Spectogram\n\n# Now we also put the log in the Y-axis (frequency)\nplot_spectrogram(Y_log_scale, sampling_rate, HOP_SIZE, y_axis=\"log\")\n\n\n\n\n\n\n\n\n¬ø Como se obtienen los espectogramas?\nEl c√°lculo de un espectrograma implica varios pasos:\n\nPreprocesamiento de la se√±al: Si la se√±al de audio no est√° en el dominio del tiempo discreto, se debe convertir a ese formato. Adem√°s, es posible que desees aplicar t√©cnicas de preprocesamiento adicionales, como normalizaci√≥n o eliminaci√≥n de ruido, para mejorar la calidad del espectrograma.\nDivisi√≥n de la se√±al en segmentos: La se√±al de audio se divide en segmentos superpuestos de longitud fija. La longitud de estos segmentos se denomina ‚Äúventana‚Äù y es un par√°metro importante en el c√°lculo del espectrograma. La elecci√≥n de la longitud de la ventana depende de la resoluci√≥n temporal y frecuencial deseada. Una ventana t√≠picamente utilizada es la ventana de Hamming.\nAplicaci√≥n de la funci√≥n de ventana: A cada segmento de la se√±al se le aplica una funci√≥n de ventana, como la ventana de Hamming mencionada anteriormente. La funci√≥n de ventana reduce las discontinuidades en los extremos de los segmentos y ayuda a evitar el sangrado espectral (spectral leakage), que puede introducir artefactos no deseados en el espectrograma.\nC√°lculo de la Transformada de Fourier de tiempo corto (STFT): Para cada segmento de la se√±al, se calcula la Transformada de Fourier de tiempo corto (STFT). La STFT se obtiene aplicando la Transformada de Fourier a cada segmento de la se√±al despu√©s de aplicar la funci√≥n de ventana. La STFT proporciona informaci√≥n sobre la distribuci√≥n de frecuencias en cada segmento de tiempo.\nC√°lculo del espectrograma: El espectrograma se crea a partir de la magnitud cuadrada de los valores obtenidos en la STFT. La magnitud cuadrada se calcula para resaltar las amplitudes y eliminar la informaci√≥n de fase. Luego, se aplica una escala de colores o una escala de grises para representar la amplitud de cada frecuencia en funci√≥n del tiempo. Los valores m√°s altos de amplitud se suelen representar con colores m√°s brillantes o tonos de grises m√°s claros, mientras que los valores m√°s bajos se representan con colores m√°s oscuros o tonos de grises m√°s oscuros.\n\nEs importante tener en cuenta que los par√°metros utilizados en el c√°lculo del espectrograma, como el tama√±o de la ventana y el solapamiento de los segmentos, pueden afectar la resoluci√≥n temporal y frecuencial del espectrograma resultante. Por lo tanto, es posible que debas ajustar estos par√°metros seg√∫n las necesidades espec√≠ficas de tu aplicaci√≥n."
  },
  {
    "objectID": "posts/birdclef2023/main.html#train-val-split",
    "href": "posts/birdclef2023/main.html#train-val-split",
    "title": "BirdClef 2023 Competition",
    "section": "Train, Val SPlit",
    "text": "Train, Val SPlit\nExiste un concepto en Machine Learning muy importante llamado ‚Äúoverfitting‚Äù, que b√°sicamente es cuando los modelos comienzan a aprenderse de memoria los datos en vez de buscar patrones que puedan funcionar bien para predecir a futuro. Es por esto que una de las acciones que se toman para evitar el overfitting es un ‚Äúsplit‚Äù de la data. Esto es, simplemente dividimos la data en dos partes (o en 3), una parte llamada de entrenamiento y otra llamada de prueba. As√≠ entrenamos nuestro modelo en la data de entrenamiento y luego calculamos m√©tricas en la parte de prueba para as√≠ tener con mayor certeza una aproximaci√≥n de como nuestro modelo lo har√° para datos que no fueron vistos a la hora de entrenar. El splitting es mucho m√°s complicado que esto, pero con esto se tiene una idea.\nYa que dividimos los audios, tenemos que asegurarnos que todos los splits de un mismo audio esten o en train o en test, para no generar leakages.\n\nOriginal Metadata\n\noriginal_metadata\n\n\n\n\n\n\n\n\nprimary_label\nsecondary_labels\ntype\nlatitude\nlongitude\nscientific_name\ncommon_name\nauthor\nlicense\nrating\nurl\nfilename\n\n\n\n\n0\nabethr1\n[]\n['song']\n4.3906\n38.2788\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nRolf A. de By\nCreative Commons Attribution-NonCommercial-Sha...\n4.0\nhttps://www.xeno-canto.org/128013\nabethr1/XC128013.ogg\n\n\n1\nabethr1\n[]\n['call']\n-2.9524\n38.2921\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nJames Bradley\nCreative Commons Attribution-NonCommercial-Sha...\n3.5\nhttps://www.xeno-canto.org/363501\nabethr1/XC363501.ogg\n\n\n2\nabethr1\n[]\n['song']\n-2.9524\n38.2921\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nJames Bradley\nCreative Commons Attribution-NonCommercial-Sha...\n3.5\nhttps://www.xeno-canto.org/363502\nabethr1/XC363502.ogg\n\n\n3\nabethr1\n[]\n['song']\n-2.9524\n38.2921\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nJames Bradley\nCreative Commons Attribution-NonCommercial-Sha...\n5.0\nhttps://www.xeno-canto.org/363503\nabethr1/XC363503.ogg\n\n\n4\nabethr1\n[]\n['call', 'song']\n-2.9524\n38.2921\nTurdus tephronotus\nAfrican Bare-eyed Thrush\nJames Bradley\nCreative Commons Attribution-NonCommercial-Sha...\n4.5\nhttps://www.xeno-canto.org/363504\nabethr1/XC363504.ogg\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16936\nyewgre1\n[]\n['']\n-1.2502\n29.7971\nEurillas latirostris\nYellow-whiskered Greenbul\nAndr√°s Schmidt\nCreative Commons Attribution-NonCommercial-Sha...\n3.0\nhttps://xeno-canto.org/703472\nyewgre1/XC703472.ogg\n\n\n16937\nyewgre1\n[]\n['']\n-1.2489\n29.7923\nEurillas latirostris\nYellow-whiskered Greenbul\nAndr√°s Schmidt\nCreative Commons Attribution-NonCommercial-Sha...\n4.0\nhttps://xeno-canto.org/703485\nyewgre1/XC703485.ogg\n\n\n16938\nyewgre1\n[]\n['']\n-1.2433\n29.7844\nEurillas latirostris\nYellow-whiskered Greenbul\nAndr√°s Schmidt\nCreative Commons Attribution-NonCommercial-Sha...\n4.0\nhttps://xeno-canto.org/704433\nyewgre1/XC704433.ogg\n\n\n16939\nyewgre1\n[]\n['']\n0.0452\n36.3699\nEurillas latirostris\nYellow-whiskered Greenbul\nLars Lachmann\nCreative Commons Attribution-NonCommercial-Sha...\n4.0\nhttps://xeno-canto.org/752974\nyewgre1/XC752974.ogg\n\n\n16940\nyewgre1\n[]\n['']\n-0.3986\n37.3087\nEurillas latirostris\nYellow-whiskered Greenbul\nLars Lachmann\nCreative Commons Attribution-NonCommercial-Sha...\n4.5\nhttps://xeno-canto.org/753190\nyewgre1/XC753190.ogg\n\n\n\n\n16941 rows √ó 12 columns\n\n\n\n\n# Contamos los audios para cada pajaro\nm_count = original_metadata.primary_label.value_counts()\none_class = list(m_count[m_count &lt;= 1].index)\n# Aseguramos aquellos pajaros que si o si estar√°n en el entrenamiento (s√≥lo tienen un dato)\nto_train = original_metadata[original_metadata.primary_label.isin(one_class)].copy()\nmetadata = original_metadata[~original_metadata.primary_label.isin(one_class)].copy()\n\n\n# Hacemos el split\nfrom sklearn.model_selection import train_test_split\nmetadata_train, metadata_test = train_test_split(metadata, train_size = CFG.TRAIN_SIZE, random_state=CFG.seed, stratify = metadata['primary_label'])\n\n\n# Le agregamos aquellos pajaros que deben ir en el entrenamiento\nmetadata_train = pd.concat([metadata_train, to_train], axis = 0)\n\n\n# Guardamos los datos\nmetadata_train.to_csv('train_metadata.csv', index = False)\nmetadata_test.to_csv('val_metadata.csv', index = False)"
  },
  {
    "objectID": "posts/birdclef2023/main.html#melspectograms-efficientnet",
    "href": "posts/birdclef2023/main.html#melspectograms-efficientnet",
    "title": "BirdClef 2023 Competition",
    "section": "MelSpectograms + EfficientNet",
    "text": "MelSpectograms + EfficientNet\nComo mencionamos anteriormente, la metodolog√≠a m√°s comun para clasificar audios es tratar el problema como un de visi√≥n. En donde la im√°gen que queremos procesar est√° dada por un espectograma. En este caso utilizaremos una variaci√≥n llamada MelSpectogram.\nEl modelo clasificador, ser√° el modelo ya pre-entrenado llamado EfficientNet. El motivo de esto es que seg√∫n comentarios de otros competidores, este modelo funciona bien en este tipo de data (Audios de p√°jaros). Existen distintos repositorios en donde podemos encontrar estos modelos pre-entrenados (Tensorflow Hub, Pytorch Hub, timm, etc.). En este caso usar√© timm ¬øPor qu√©? Simplemente porque hab√≠a o√≠do de el y quice probarlo.\n\nTimm\nEl framework de aprendizaje autom√°tico timm (Transfer Image Models) es una biblioteca de c√≥digo abierto que proporciona una amplia colecci√≥n de modelos preentrenados y herramientas para la visi√≥n por computadora\n\n\nEfficientNet\nEfficientNet es un modelo de red neuronal convolucional desarrollado por Google que destaca por su eficiencia y rendimiento sobresaliente en la clasificaci√≥n de im√°genes. Utiliza un enfoque de escalado compuesto, ajustando la profundidad, el ancho y la resoluci√≥n de la imagen de entrada de manera √≥ptima. Esto le permite lograr un equilibrio entre el tama√±o de la red y el rendimiento, superando a modelos anteriores y siendo altamente eficiente en el uso de recursos computacionales y memoria.\n\n\nMelSpectogram\nUn mel spectrograma, o melspectrograma, es una representaci√≥n visual que muestra c√≥mo var√≠a la energ√≠a espectral de una se√±al de audio en funci√≥n del tiempo y de las bandas de frecuencia mel. El melspectrograma es una versi√≥n modificada del espectrograma convencional que utiliza una escala de frecuencia perceptualmente m√°s relevante, conocida como escala mel.\nLa escala mel es una escala no lineal que se basa en la percepci√≥n humana de las frecuencias de sonido. Esta escala asigna m√°s resoluci√≥n a las frecuencias m√°s bajas, donde nuestro sistema auditivo es m√°s sensible, y menos resoluci√≥n a las frecuencias m√°s altas.\nEl proceso de c√°lculo de un melspectrograma implica los siguientes pasos:\n\nPreprocesamiento de la se√±al: Al igual que con el c√°lculo de un espectrograma, se pueden aplicar t√©cnicas de preprocesamiento a la se√±al de audio, como conversi√≥n al dominio del tiempo discreto y normalizaci√≥n.\nDivisi√≥n de la se√±al en segmentos: La se√±al de audio se divide en segmentos superpuestos de longitud fija, al igual que en el c√°lculo del espectrograma.\nAplicaci√≥n de la funci√≥n de ventana: A cada segmento de la se√±al se le aplica una funci√≥n de ventana, como la ventana de Hamming, para reducir las discontinuidades en los extremos de los segmentos y evitar el sangrado espectral.\nC√°lculo de la Transformada de Fourier de tiempo corto (STFT): Se calcula la STFT para cada segmento de la se√±al despu√©s de aplicar la funci√≥n de ventana. La STFT proporciona informaci√≥n sobre la distribuci√≥n de frecuencias en cada segmento de tiempo.\nConversi√≥n a la escala mel: Los coeficientes de frecuencia obtenidos de la STFT se convierten a la escala mel utilizando una transformaci√≥n no lineal. Esta transformaci√≥n se basa en filtros mel, que se superponen en el dominio de la frecuencia para representar las bandas de frecuencia mel.\nC√°lculo del melspectrograma: El melspectrograma se crea a partir de la magnitud cuadrada de los valores obtenidos en el paso anterior. Luego, se aplica una escala de colores o una escala de grises para representar la amplitud de cada banda de frecuencia mel en funci√≥n del tiempo.\n\nEn resumen, un melspectrograma es una representaci√≥n visual que muestra c√≥mo var√≠a la energ√≠a espectral de una se√±al de audio en funci√≥n del tiempo y de las bandas de frecuencia mel. Se calcula mediante la conversi√≥n de los coeficientes de frecuencia obtenidos de la STFT a la escala mel y representa la amplitud de cada banda de frecuencia mel en funci√≥n del tiempo utilizando una escala de colores o de grises.\n\ndef get_sample():\n    audio_samples = glob.glob('/kaggle/input/birdclef-2023/train_audio/*/*.ogg')\n    random_audio = np.random.choice(audio_samples)\n    bird_class = random_audio.split('/')[-2]\n    #signal, sampling_rate = librosa.load(random_audio)\n    signal, sampling_rate  = torchaudio.load(random_audio)\n    return signal, sampling_rate\n\ndef plot_waveform(signal, sr):\n    # Plot the waveform\n    plt.figure(figsize=(15, 5))\n    lid.waveshow(signal.numpy(), sr=sr)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Amplitude')\n    plt.show()\n    \ndef display_audio(signal, sr):\n    return Audio(signal, rate = sr)\n\ndef get_mel_spectogram(signal, sr):\n    mel_t = T.MelSpectrogram(sample_rate = sr, n_fft = CFG.FRAME_SIZE,\n                            hop_length = CFG.HOP_SIZE, n_mels = CFG.N_MELS)\n    mel = mel_t(signal).squeeze(0)\n    return mel\n\n# Plot Spectogram\ndef plot_spectrogram(mel, sr, hop_length, scaled = False):\n    if not scaled:\n        # Convert to dB scale (log-scale)\n        mel = librosa.power_to_db(mel, ref=np.max)\n        \n    # Plot the mel spectrogram\n    plt.figure(figsize=(10, 4))\n    librosa.display.specshow(mel, x_axis='time',\n                             y_axis='mel', sr=sr, hop_length=CFG.HOP_SIZE)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Mel spectrogram')\n    plt.tight_layout()\n    plt.show()\n\n    \n    \n\n\nsample, sr = get_sample()\n\n\nplot_waveform(sample, sr)\n\n\n\n\n\n\n\n\n\nmel = get_mel_spectogram(sample, sr)\n\n\nplot_spectrogram(mel, sr, CFG.HOP_SIZE, scaled = False)"
  },
  {
    "objectID": "posts/birdclef2023/main.html#data-augmentation",
    "href": "posts/birdclef2023/main.html#data-augmentation",
    "title": "BirdClef 2023 Competition",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nOtro concepto importante que ser√≠a bueno conocer, esta vez en DeepLearning, es Data Augmentation. Como su nombre lo dice, este busca aumentar los datos de forma sint√©tinca.\n¬øPor qu√©? En ML siempre es bueno tener m√°s datos, podemos ver los datos como experiencia, a mayor experiencia, mayor conocimiento tenemos.\n¬øC√≥mo? En visi√≥n existen distintos tipos de Data Augmentation, por ejemplo, tomar algunos de los datos que ya tenemos y girarlos aleatoriamente, o quiz√° cortalos, o quiz√°s hacerles un zoom, etc. En este caso, las t√©cnicas tradicionales de Data Augmentation en visi√≥n no son √∫tiles porque no tienen sentido o no son interpretables ¬ø Que significa que haga un zoom a un espectograma? O que lo gire 90 grados?\nEn este caso aplicaremos dos tipos de Data Augmentation:\n\nSignal Data Augmentation: Buscamos aplicarle ciertos efectos al audio en si\nSpectogram Augmentation: Utilizaremos algunas t√©cnicas que si tengan sentido cuando trabajamos con Audio.\n\n\nMixUp\nM√°s adelante veremos otra t√©cnica llamada MixUp.\n\nclass ComposeTransform:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, audio_data):\n        for t in self.transforms:\n            audio_data = t(audio_data)\n        return audio_data\n\n\n\n\nSignal Augmentations\n\n\nGaussian Noise:\nGaussian Noise en el procesamiento de audio es un tipo de ruido aleatorio que se utiliza para simular ruidos ambientales y evaluaciones de calidad del sonido. Se genera agregando valores aleatorios extra√≠dos de una distribuci√≥n normal a la se√±al de audio original. Adem√°s, se utiliza como t√©cnica de regularizaci√≥n en el entrenamiento de modelos de aprendizaje autom√°tico. Su aplicaci√≥n implica agregar variabilidad y evaluar el impacto del ruido en la se√±al de audio.\n\nimport random\n\n\nclass GaussianNoise:\n    def __init__(self, mean = 0, sigma = 'auto', p = 0.5):\n        self.mean = mean\n        self.sigma = sigma\n        self.p = p\n        \n    def __call__(self, audio):\n        random_f = torch.rand([]).item()\n        \n        if self.sigma == 'auto':\n            sigma = audio.std()\n        else:\n            sigma = self.sigma\n    \n        if self.p &lt; random_f:\n\n            std_dev = random.uniform(0, sigma)\n            noise = torch.randn_like(audio) * std_dev + self.mean\n\n            # Add the noise to the audio signal\n            audio = audio + noise\n    \n        return audio\n        \n\n\ngaussian_noise = GaussianNoise()\n\n\nnoise_sample = gaussian_noise(sample)\n\n\nplot_waveform(noise_sample, sr)\n\n\n\n\n\n\n\n\n\ndisplay_audio(sample, sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndisplay_audio(noise_sample, sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nsignal_augmentations = ComposeTransform([\n    GaussianNoise()\n])\n\n\n\nSpectogram Augmentations\n\n\nFrequency Masking\nFrequencyMasking es una t√©cnica utilizada en el procesamiento de audio para eliminar o enmascarar informaci√≥n de frecuencia espec√≠fica en una se√±al de sonido. Funciona al ‚Äúmascarar‚Äù o bloquear un rango de frecuencias en el espectro de audio. Esto se logra aplicando un filtro que aten√∫a o elimina selectivamente las componentes de frecuencia dentro del rango especificado. Al enmascarar ciertas frecuencias, se pueden ocultar caracter√≠sticas no deseadas o mejorar la detecci√≥n de otras caracter√≠sticas importantes en la se√±al de audio. Esta t√©cnica es com√∫nmente utilizada en tareas de reconocimiento de voz, separaci√≥n de fuentes de audio y reducci√≥n de ruido para mejorar la calidad y el rendimiento de los algoritmos de procesamiento de audio.\n\n\nTimeMasking\nTimeMasking es una t√©cnica utilizada en el procesamiento de audio para enmascarar o bloquear segmentos de tiempo espec√≠ficos en una se√±al de sonido. Consiste en ocultar o atenuar temporalmente ciertas partes de la se√±al de audio, creando una especie de ‚Äúm√°scara‚Äù en el dominio del tiempo. Al aplicar TimeMasking, se bloquea temporalmente la informaci√≥n de sonido dentro de un intervalo de tiempo determinado. Esta t√©cnica es √∫til para ocultar partes no deseadas de la se√±al, mejorar la privacidad de la informaci√≥n o enfocar la atenci√≥n en partes espec√≠ficas del audio. Es ampliamente utilizado en tareas como el procesamiento de voz, reconocimiento de habla y an√°lisis de se√±ales de audio en general.\n\n\nData Augmentation\nB√°sicamente puedes imaginar un masking de forma horizontal y vertical si lo vemos en un espectograma\n\nspec_augmentations = nn.Sequential(\n    T.FrequencyMasking(CFG.FREQ_MASK_PARAM),\n    T.TimeMasking(time_mask_param = CFG.TIME_MASK_PARAM, p = CFG.P_TIME_MASK)\n)\n\n\naugm_spec = spec_augmentations(mel[None, ...])\n\n\nplot_spectrogram(augm_spec.squeeze(0), sr, CFG.HOP_SIZE, scaled = False)"
  },
  {
    "objectID": "posts/birdclef2023/main.html#load-data",
    "href": "posts/birdclef2023/main.html#load-data",
    "title": "BirdClef 2023 Competition",
    "section": "Load Data",
    "text": "Load Data\nEn este caso de uso, haremos uso de Pytorch Lighting, que es un framework basado en Pytorch. En cualquier framework que decidamos utilizar se debe crear un script para cargar los datos. En el caso de Pytorch creamos una clase que debe tener ciertos requisitos:\n\n__len__\n__getitem__\n\nEn este caso, incorpor√© la creaci√≥n de MelSpectograms, para crear cada instancia entonces realizo: 1. Obtener el path del audio correspondiente 2. Leer el audio utilizando torchaudio 3. Resample: la idea es que todos los sonidos tengan la misma ‚Äúresoluci√≥n‚Äù, por esto resampleamos cada audio 4. Mix Down: Dejamos el audio con un s√≥lo canal: stereo -&gt; mono 5. Cut: Ya que existen audios que tienen diferentes largos, cortamos 5 segundos aleatorios de cada uno. En caso de ser validaci√≥n, tomamos los primeros 5 segundos del audio, esto debido a que la data de validaci√≥n debe ser la misma en cada epoch. 6. Signal Augmentations: Aplicamos Data Augmentation para se√±ales 7. MelSpectograms: Obtenemos los MelSpectogramas 8. Pasamos la escala a decibeles, lo hace m√°s ‚Äúentendible‚Äù 9. Spectograms Augmentations: Aplicamos Data Augmentation para spectogramas 10. Para crear imagenes con 3 canales, repetimos el spectograma 11. Target Transform: ya que el target debe ser num√©rico, hacemos esta transformaci√≥n.\n\nclass BirdDataset(Dataset):\n     \"\"\"\n    Dataset class for bird audio classification.\n\n    Parameters:\n    - metadata_path (str): Path to the metadata CSV file.\n    - audio_path (str): Path to the directory containing the audio files.\n    - sr (int, optional): Sampling rate of the audio files. Default is CFG.SAMPLE_RATE.\n    - framesize (int, optional): Size of the audio frames. Default is CFG.FRAME_SIZE.\n    - hop (int, optional): Number of samples between successive frames. Default is CFG.HOP_SIZE.\n    - n_mels (int, optional): Number of mel bands in the mel spectrogram. Default is CFG.N_MELS.\n    - n_samples (int, optional): Number of samples to be used from each audio file. Default is CFG.N_SAMPLES.\n    - signal_augmentations (callable, optional): Function or transform to apply signal augmentations. Default is None.\n    - spec_augmentations (callable, optional): Function or transform to apply spectrogram augmentations. Default is None.\n    - train (bool, optional): Whether the dataset is for training or not. Default is True.\n\n    Methods:\n    - __len__(): Returns the number of samples in the dataset.\n    - __getitem__(idx): Retrieves a sample from the dataset.\n\n    \"\"\"\n    def __init__(self, metadata_path, audio_path, sr = CFG.SAMPLE_RATE,\n                 framesize = CFG.FRAME_SIZE, hop = CFG.HOP_SIZE,\n                 n_mels = CFG.N_MELS, n_samples = CFG.N_SAMPLES, \n                 signal_augmentations = None, spec_augmentations = None, train = True):\n        \"\"\"\n        Initialize the BirdDataset.\n\n        Loads the metadata CSV file, sets the audio paths and parameters, and defines augmentations.\n\n        \"\"\"\n        \n        self.metadata = pd.read_csv(metadata_path)\n        self.audio_path = audio_path\n        self.framesize = framesize\n        self.hop = hop\n        self.sr = sr\n        self.n_mels = n_mels\n        self.num_samples = n_samples\n        self.signal_augmentations = signal_augmentations\n        self.spec_augmentations = spec_augmentations\n        self.train = train\n        \n    \n        \n    \n    def __len__(self):\n        \"\"\"\n        Return the number of samples in the dataset.\n\n        Returns:\n        - int: Number of samples.\n\n        \"\"\"\n        return len(self.metadata)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieve a sample from the dataset.\n\n        Parameters:\n        - idx (int): Index of the sample to retrieve.\n\n        Returns:\n        - tuple: A tuple containing the mel spectrogram and the target label.\n\n        \"\"\"\n        # 1. Get image path and label\n        path, label = self.metadata.loc[idx, [\"filename\", \"primary_label\"]]\n        full_path = os.path.join(self.audio_path, path)\n        \n        # 2. Load audio\n        waveform, sample_rate = torchaudio.load(full_path)\n        \n        \n        \n        # 3. & 4. preprocess\n        waveform = self._resample(waveform, sample_rate)\n        waveform = self._mix_down(waveform)\n        \n        #5. Cut or take first\n        if not(CFG.SPLIT_AUDIO):\n            if self.train:\n                waveform = self._random_cut(waveform) # Get random split if &gt; duration\n            else:\n                waveform = self._take_first(waveform) # Get the first split (Validation should not be random)\n\n            waveform = self._pad(waveform)\n        \n        #6. Signal Augmentations\n        if self.signal_augmentations != None:\n            waveform = self.signal_augmentations(waveform)\n            \n\n        \n        #7. MelSpectogram\n        mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate = self.sr, n_fft = self.framesize,\n                                                             hop_length = self.hop, n_mels = self.n_mels)\n        mels = mel_transform(waveform)\n        \n        #8. To decibels\n        to_db = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n        mels = to_db(mels)\n        \n        #9. Spectogram Augmentations\n        if self.spec_augmentations != None:\n            mels = self.spec_augmentations(mels)\n            \n        # ride off the batch\n        mels = mels.squeeze(0)\n    \n        #10. Stack (Add channels in based in efffnet requirements)\n        if CFG.CHANNELS==3:\n            mels = torch.stack([mels,mels, mels], dim=0)\n        else:\n            mels = mels[None, :, :]\n        \n       \n        \n        # 11. Label transform (OHE)\n        num_label = CFG.name2label[label]\n        target = F.one_hot(torch.tensor(num_label), num_classes = CFG.N_CLASSES)\n        target = target.type(torch.FloatTensor)\n        \n        return mels, target\n    \n    \n    #### Helper Functions #####\n    \n    def _random_cut(self, signal):\n        \"\"\"\n        Randomly cut a segment from the given signal.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n\n        Returns:\n        - Tensor: Cut segment of the signal.\n\n        \"\"\"\n        if signal.shape[1] &gt; self.num_samples:\n            #signal = signal[:, :self.num_samples]\n            diff_len = signal.shape[1] - self.num_samples \n            idx = int(torch.rand([]).item()*diff_len)\n            signal = signal[:, idx: (idx + self.num_samples)]\n        return signal\n    \n    \n    def _take_first(self, signal):\n        \"\"\"\n        Take the first segment from the given signal.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n\n        Returns:\n        - Tensor: First segment of the signal.\n\n        \"\"\"\n        if signal.shape[1] &gt; self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n            \n    \n    def _resample(self, signal, sr):\n        \"\"\"\n        Resample the signal to the target sampling rate.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n        - sr (int): Original sampling rate of the signal.\n\n        Returns:\n        - Tensor: Resampled signal.\n\n        \"\"\"\n        if sr != self.sr:\n            resampler = torchaudio.transforms.Resample(sr, self.sr)\n            signal = resampler(signal)\n        return signal\n    \n    def _pad(self, signal):\n        \"\"\"\n        Pad the signal with zeros if its length is shorter than the desired number of samples.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n\n        Returns:\n        - Tensor: Padded signal.\n\n        \"\"\"\n        length_signal = signal.shape[1]\n        if length_signal &lt; self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n    \n    def _mix_down(self, signal):\n        \"\"\"\n        Mix down a multi-channel signal to a single channel.\n\n        Parameters:\n        - signal (Tensor): Input signal.\n\n        Returns:\n        - Tensor: Mixed down signal.\n\n        \"\"\"\n        if signal.shape[0] &gt; 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n    \n            \n    \n    \n\n        \n        \n        \n\n\nDataloader\nFinalmente, creamos los datasets y el dataloader.\nLos DataLoaders en PyTorch son clases que facilitan la carga eficiente de conjuntos de datos para entrenar modelos de aprendizaje autom√°tico. Proporcionan funciones como la divisi√≥n en lotes, la mezcla de datos y la aplicaci√≥n de transformaciones, lo que simplifica el proceso de entrenamiento. Los DataLoaders permiten cargar y procesar los datos en paralelo, lo que acelera el entrenamiento y garantiza que los datos se utilicen de manera eficiente. Adem√°s, ofrecen una interfaz sencilla para iterar sobre los datos durante el entrenamiento del modelo. En resumen, los DataLoaders son una herramienta clave en PyTorch para cargar y preparar los datos de manera eficiente antes de entrenar un modelo de aprendizaje autom√°tico.\n\nif CFG.SPLIT_AUDIO:\n    train_dataset = BirdDataset('train_metadata.csv', CFG.CUT_AUDIOS_PATH, train = True, signal_augmentations = signal_augmentations, spec_augmentations= spec_augmentations)\n    val_dataset = BirdDataset('val_metadata.csv', CFG.CUT_AUDIOS_PATH, train = False)\nelse:\n    train_dataset = BirdDataset('train_metadata.csv', CFG.AUDIOS_PATH, train = True, signal_augmentations = signal_augmentations, spec_augmentations= spec_augmentations)\n    val_dataset = BirdDataset('val_metadata.csv', CFG.AUDIOS_PATH, train = False)\n\n\ndl_train = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE , shuffle=True, num_workers = 2)\ndl_val = DataLoader(val_dataset, batch_size=CFG.BATCH_SIZE , shuffle=False, num_workers = 2)\n\n\n\nShow Data\nVeamos como es la data generada\n\ndef show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n    fig = plt.figure(figsize=(12, 6))    \n    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n    for index, img_index in enumerate(img_index):  # list first 9 images\n        img, lb = img_ds[img_index]    \n        #img = img[0][0]\n        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n        if isinstance(img, torch.Tensor):\n            img = img.detach().numpy()\n        if isinstance(img, np.ndarray):\n            img = img.transpose(1, 2, 0)\n            ax.imshow(img)        \n        title = f\"Spec {CFG.label2name[lb.argmax().item()]}\"\n        ax.set_title(title)\n\n\nshow_batch(train_dataset, 8, 2, 4)\n\n\nshow_batch(val_dataset, 8, 2, 4)\n\n\n# Aunque son similares, vemos que no son los mismos! Entonces cada vez que dataloader extrea\n# El item, le entrega un sample distinto!"
  },
  {
    "objectID": "posts/birdclef2023/main.html#modeling",
    "href": "posts/birdclef2023/main.html#modeling",
    "title": "BirdClef 2023 Competition",
    "section": "Modeling !",
    "text": "Modeling !\nPara la fase de modelado, recordar que utilizamos Pytorch Lighting\n\nimport timm\nimport pytorch_lightning as pl\n\n\nOptimizer\nNormalmente, podemos utilizar los optimizadores que Pytorch trae por defecto, en este caso utilizamos Adam pero agregandole un schedular al learning rate.\n\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau, OneCycleLR\n\ndef get_optimizer(lr, params):\n    model_optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, params), \n            lr=lr,\n            weight_decay=CFG.WEIGHT_DECAY\n        )\n    interval = \"epoch\"\n    \n    lr_scheduler = CosineAnnealingWarmRestarts(\n                            model_optimizer, \n                            T_0=CFG.EPOCHS, \n                            T_mult=1, \n                            eta_min=1e-6, \n                            last_epoch=-1\n                        )\n\n    return {\n        \"optimizer\": model_optimizer, \n        \"lr_scheduler\": {\n            \"scheduler\": lr_scheduler,\n            \"interval\": interval,\n            \"monitor\": \"val_loss\",\n            \"frequency\": 1\n        }\n    }\n\n\n\nMetrics\nNormalmente las m√©tricas utilizadas en Kaggle son particulares, este caso no es distinto, es por esto que utilizamos funciones creadas por algunos participantes para poder ingresar en los distintos frameworks que utilizamos.\n\nimport sklearn.metrics\n\ndef padded_cmap(solution, submission, padding_factor=5):\n    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n    new_rows = []\n    for i in range(padding_factor):\n        new_rows.append([1 for i in range(len(solution.columns))])\n    new_rows = pd.DataFrame(new_rows)\n    new_rows.columns = solution.columns\n    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n    score = sklearn.metrics.average_precision_score(\n        padded_solution.values,\n        padded_submission.values,\n        average='macro',\n    )\n    return score\n\ndef map_score(solution, submission):\n    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n    score = sklearn.metrics.average_precision_score(\n        solution.values,\n        submission.values,\n        average='micro',\n    )\n    return score\n\n\n\nMix Up\nAprendiendo del desarrollo de otros Kagglers, encontr√© una t√©cnica bastante utilizada llamada Mixup. MixUp es una t√©cnica utilizada en el procesamiento de audio que combina dos o m√°s se√±ales de audio para crear una nueva se√±al. Consiste en mezclar las formas de onda de diferentes se√±ales con ponderaciones espec√≠ficas. El proceso implica tomar una combinaci√≥n lineal de las formas de onda originales y sus correspondientes etiquetas o caracter√≠sticas asociadas. El objetivo principal de MixUp es aumentar la diversidad y la cantidad de datos de entrenamiento al crear nuevas instancias de se√±ales de audio mediante la interpolaci√≥n entre muestras existentes. Esta t√©cnica promueve la generalizaci√≥n del modelo al exponerlo a diferentes combinaciones de se√±ales, mejorando as√≠ su capacidad para reconocer y clasificar diferentes patrones en los datos de audio. MixUp es particularmente √∫til en tareas de clasificaci√≥n de audio, como el reconocimiento de g√©nero o emociones, y puede ayudar a mejorar el rendimiento y la robustez de los modelos de aprendizaje autom√°tico en el procesamiento de audio.\n\n# Copied and edited from https://www.kaggle.com/code/riadalmadani/fastai-effb0-base-model-birdclef2023\ndef mixup(data, targets, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    new_data = data * lam + shuffled_data * (1 - lam)\n    new_targets = [targets, shuffled_targets, lam]\n    return new_data, new_targets\n\n\n# Copied and edited from https://www.kaggle.com/code/riadalmadani/fastai-effb0-base-model-birdclef2023\ndef mixup_criterion(criterion, preds, targets):\n    targets1, targets2, lam = targets[0], targets[1], targets[2]\n    #criterion = nn.CrossEntropyLoss()\n    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n\n\n\nModel\nPara el caso del modelo, es un poco similar a la creaci√≥n del Datasets, necesitamos tener algunos m√©todos particulares, en este caso tenemos:\n\nForward\nConfigure Optimizers\nTraining Step\nValidation Step\nValidation Epoch End\n\n\nclass BirdModel(pl.LightningModule):\n    def __init__(self, model_name = CFG.MODEL_NAME, num_classes=CFG.N_CLASSES):\n        \"\"\"\n        BirdModel class for audio classification using a pretrained model.\n\n        Args:\n        - model_name (str): Name of the pretrained model architecture.\n        - num_classes (int): Number of output classes.\n\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.model = timm.create_model(model_name, pretrained = True, num_classes = CFG.N_CLASSES, in_chans = CFG.CHANNELS)\n        self.loss_function = nn.CrossEntropyLoss()\n        self.batch_norm = nn.BatchNorm2d(num_features=CFG.CHANNELS)\n        \n        #self.backbone = timm.create_model(model_name, pretrained=True)\n        #self.in_features = self.backbone.classifier.in_features\n        #self.backbone.classifier = nn.Sequential(\n        #        nn.Linear(self.in_features, num_classes)\n        #        )\n        \n        #self.loss_function = nn.BCEWithLogitsLoss() \n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n        - x (torch.Tensor): Input tensor.\n\n        Returns:\n        - torch.Tensor: Output logits.\n\n        \"\"\"\n        x = self.batch_norm(x)\n        logits = self.model(x)\n        return logits\n    \n    def configure_optimizers(self):\n        \"\"\"\n        Configure the optimizer for training.\n\n        Returns:\n        - torch.optim.Optimizer: Optimizer object.\n\n        \"\"\"\n        return get_optimizer(lr=CFG.LR, params=self.parameters())\n        #optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer \n    \n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Training step.\n\n        Args:\n        - batch (tuple): Input batch (image, target).\n        - batch_idx (int): Batch index.\n\n        Returns:\n        - torch.Tensor: Loss value.\n\n        \"\"\"\n        image, target = batch     \n        \n        r_float = torch.rand([]).item()\n\n        \n        if CFG.MIXUP and CFG.P_MIXUP &lt; r_float:\n            image, targets = mixup(image, target, CFG.MIXUP_LAMBDA)\n            y_pred = self.forward(image)\n            loss = mixup_criterion(self.loss_function, y_pred, targets)\n        else:\n            y_pred = self.forward(image)\n            target = torch.argmax(target, dim=1)\n            loss = self.loss_function(y_pred,target)\n\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n        return loss  \n    \n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Validation step.\n\n        Args:\n        - batch (tuple): Input batch (image, target).\n        - batch_idx (int): Batch index.\n\n        Returns:\n        - dict: Dictionary containing validation loss, logits, and targets.\n\n        \"\"\"\n        image, target = batch    \n        return_target = target\n        target = torch.argmax(target, dim=1)\n        y_pred = self.forward(image)\n        loss = self.loss_function(y_pred,target)\n\n        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n        return {\"val_loss\": loss, \"logits\": y_pred, \"targets\": return_target}\n    \n    def validation_epoch_end(self, outputs):\n        \"\"\"\n        Validation epoch end.\n\n        Args:\n        - outputs (list): List of dictionaries containing validation loss, logits, and targets.\n\n        Returns:\n        - dict: Dictionary containing validation loss and C-MAP score.\n\n        \"\"\"\n        \n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        output_val = torch.cat([x['logits'] for x in outputs],dim=0).sigmoid().cpu().detach().numpy()\n        target_val = torch.cat([x['targets'] for x in outputs],dim=0).cpu().detach().numpy()\n        \n        # print(output_val.shape)\n        val_df = pd.DataFrame(target_val, columns = list(CFG.label2name.values()))\n        pred_df = pd.DataFrame(output_val, columns = list(CFG.label2name.values()))\n        \n        avg_score = padded_cmap(val_df, pred_df, padding_factor = 5)\n        ap_score = sklearn.metrics.label_ranking_average_precision_score(target_val,output_val)\n        \n#         competition_metrics(output_val,target_val)\n        print(f'epoch {self.current_epoch} validation loss {avg_loss}')\n        print(f'epoch {self.current_epoch} validation C-MAP score pad 5 {avg_score}')\n        print(f'epoch {self.current_epoch} validation AP score {ap_score}')\n        \n\n        self.log('val_map_score', avg_score)\n        self.log('val_ap_score', ap_score)\n        \n        return {'val_loss': avg_loss,'val_cmap':avg_score}\n        \n\n\n\nWANDB Logging\nPara el monitoreo del entrenamiento utilzamos Weight And Biases. Es importante se√±alar que esta herramienta sirve para mucho m√°s que el monitoreo del entrenamiento.\nWeight and Bias (W&B) es un framework utilizado para el seguimiento y la visualizaci√≥n de experimentos de aprendizaje autom√°tico. Proporciona herramientas para registrar y monitorear m√©tricas, hiperpar√°metros, gr√°ficos y visualizaciones en tiempo real durante el entrenamiento y evaluaci√≥n de modelos. W&B permite el registro de experimentos de manera f√°cil e integrada con diferentes bibliotecas y frameworks de aprendizaje autom√°tico. Adem√°s, ofrece caracter√≠sticas como la comparaci√≥n de experimentos, la colaboraci√≥n en equipo y la reproducci√≥n de resultados anteriores, lo que lo convierte en una herramienta valiosa para el desarrollo y la iteraci√≥n eficiente en proyectos de aprendizaje autom√°tico.\n\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\n\nPara utilizarlo debes ingresar tu KEY de WandB. En este caso la almacenamos en los secrets de Kaggle, pero puedes hacerlo de la forma que quieras.\n\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nsecret_label = \"WANDB_KEY\"\nkey = UserSecretsClient().get_secret(secret_label)\nwandb.login(key = key)\n\n\n\nTraining\nAhora s√≥lo queda entrenar!\nLa funci√≥n run_training() configura y ejecuta el proceso de entrenamiento para un modelo BirdModel. Comienza imprimiendo un mensaje que indica que se est√° ejecutando el entrenamiento. Luego, crea un objeto WandbLogger para registrar y visualizar el proceso de entrenamiento en la plataforma Weights and Biases.\nA continuaci√≥n, inicializa un objeto BirdModel, que es el modelo a ser entrenado.\nLa funci√≥n tambi√©n configura dos callbacks: EarlyStopping y ModelCheckpoint. El callback EarlyStopping monitorea la p√©rdida de validaci√≥n y detiene el entrenamiento si no hay mejoras durante un cierto n√∫mero de √©pocas, mientras que el callback ModelCheckpoint guarda el modelo con la menor p√©rdida de validaci√≥n.\nDespu√©s de configurar los callbacks necesarios, la funci√≥n crea un objeto Trainer del framework PyTorch Lightning. El Trainer se encarga de administrar el proceso de entrenamiento. Especifica el n√∫mero de √©pocas, el logger, los callbacks y otras configuraciones, como el uso de una GPU para aceleraci√≥n.\nFinalmente, se llama al m√©todo trainer.fit() para iniciar el proceso de entrenamiento real, utilizando audio_model como el modelo a entrenar y dl_train y dl_val como los dataloaders de entrenamiento y validaci√≥n, respectivamente. Una vez que se completa el entrenamiento, se libera la memoria llamando a gc.collect() y torch.cuda.empty_cache().\nEn resumen, esta funci√≥n encapsula el proceso de entrenamiento para un modelo BirdModel, incluyendo la inicializaci√≥n del modelo, el registro de datos, los callbacks y el bucle de entrenamiento utilizando el Trainer de PyTorch Lightning.\n\nimport gc\n\ndef run_training():\n    print(f\"Running training...\")\n    \n    wandb_logger = WandbLogger(project=\"birdclef23\", entity = 'diegulio', name = CFG.EXP_NAME)\n    \n    \n    audio_model = BirdModel()\n\n    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=3, verbose= True, mode=\"min\")\n    checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n                                          dirpath= \"/kaggle/working/exp1/\",\n                                      save_top_k=1,\n                                      save_last= True,\n                                      save_weights_only=True,\n                                      filename= f'./{CFG.MODEL_NAME}_loss',\n                                      verbose= True,\n                                      mode='min')\n    \n    callbacks_to_use = [checkpoint_callback,early_stop_callback]\n\n\n    trainer = pl.Trainer(\n        val_check_interval=CFG.VAL_CHECK_INTERVAL,\n        #deterministic=True,\n        max_epochs=CFG.EPOCHS,\n        logger=wandb_logger,\n        #auto_lr_find=True,    \n        callbacks=callbacks_to_use,\n        #precision=16, \n        accelerator=\"gpu\",\n        #num_nodes = 2,\n        strategy = 'ddp_notebook'\n        \n    )\n\n    print(\"Running trainer.fit\")\n    trainer.fit(audio_model, train_dataloaders = dl_train, val_dataloaders = dl_val)                \n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n\nrun_training()"
  },
  {
    "objectID": "posts/mcp/main.html",
    "href": "posts/mcp/main.html",
    "title": "Leveraging MCPs with LLMs",
    "section": "",
    "text": "Hoy quiero hablarte de una ‚Äúherramienta‚Äù que en el √∫ltimo tiempo se ha vuelto bastante famosa. No, no es un nuevo modelo de lenguaje, pero s√≠ algo que hace que las aplicaciones que usan estos modelos sean mucho m√°s f√°ciles de construir para los desarrolladores. Y digo desarrolladores, porque en apariencia no tiene un impacto directo para el usuario final.\nLa idea de este post es acortar esa brecha y mostrar c√≥mo podemos aprovechar sus beneficios incluso sin tener que desarrollar desde cero una aplicaci√≥n con LLMs. Y, para quienes son m√°s computines ü§ñ, tambi√©n veremos c√≥mo se conecta todo esto desde el c√≥digo.\nVolvamos un poco atr√°s. En los primeros d√≠as de los Large Language Models (pensemos en el primer ChatGPT que todos recordamos), las respuestas se basaban √∫nicamente en lo aprendido durante su entrenamiento: miles y miles de p√°ginas de internet. Eso significaba que su conocimiento estaba limitado y, en la mayor√≠a de los casos, desactualizado. Por ejemplo, la primera versi√≥n de ChatGPT lanzada a fines de 2022 solo sab√≠a hasta septiembre de 2021. Por eso, preguntas como ‚Äú¬øc√≥mo est√° hoy la inflaci√≥n en Chile?‚Äù, ‚Äú¬øqu√© pas√≥ con el precio del Bitcoin esta semana?‚Äù o ‚Äú¬øqu√© cambios trae Python 3.11?‚Äù eran imposibles de responder.\nLa primera soluci√≥n a este problema fue lo que se llam√≥ Prompt Engineering: agregar contexto directamente en la pregunta. Al principio, la gente copiaba y pegaba fragmentos de texto de otras fuentes, pero pronto ese contexto empez√≥ a incluir bloques m√°s largos, im√°genes, sonidos o incluso conexiones a APIs. Con el tiempo, lo que empez√≥ como Prompt Engineering evolucion√≥ al concepto m√°s amplio de Context Engineering.\nEl gran desaf√≠o era que no hab√≠a un est√°ndar. Cada persona que quer√≠a conectar un sistema a un LLM deb√≠a crear sus propias funciones personalizadas: para extraer transcripciones de YouTube, leer correos de Gmail, scrapear datos de la web, etc. B√°sicamente, todos reinventaban la rueda una y otra vez, leyendo documentaci√≥n distinta para cada API.\nAh√≠ es donde entra en juego Model Context Protocol (MCP): un est√°ndar que permite conectar asistentes de inteligencia artificial (como chatbots) con distintos sistemas, de forma ordenada y consistente.\n\nEn este post utilizaremos servidores MCPs ya definidos, crearemos un servidor MCP, y lo conectaremos a Claude Sonnet 4 dentro de Claude Desktop (Host MCP). Finalmente haremos el ejercicio de como ser√≠a conectar cualquier LLM a estos servidores sin utilizar un Host como Claude Desktop."
  },
  {
    "objectID": "posts/mcp/main.html#youtube-transcriptions",
    "href": "posts/mcp/main.html#youtube-transcriptions",
    "title": "Leveraging MCPs with LLMs",
    "section": "Youtube Transcriptions",
    "text": "Youtube Transcriptions\n\nQueremos extraer la transcripci√≥n de la clases para ingresarla al LLM como contexto. En mi caso, las clases de periodos anteriores est√°n en Youtube, pero en otros casos de uso podr√≠an utilizarse distintas herramientas de transcripci√≥n.\nPara el caso de youtube, existe una librer√≠a espec√≠fica para obtener transcripciones, esto nos va a facilitar la vida:\n\n\n\nsource: https://pypi.org/project/youtube-transcript-api\n\n\nsource: https://pypi.org/project/youtube-transcript-api\nSu uso es bastante sencillo, s√≥lo se necesita el ID del video, el cual encontraremos de forma sencilla en el url de este:\n\ny para obtener la transcripci√≥n s√≥lo debemos hacer:\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\ntranscript = YouTubeTranscriptApi().fetch(video_id)\nEn mi caso, yo queria insertar directamente el url del video de youtube, por lo que agregu√© una funci√≥n auxiliar, resultando en:\n# Funci√≥n auxiliar para obtener el video_id desde la url\ndef get_video_id(url):\n    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n    match = re.search(pattern, url)\n    return match.group(1) if match else None\n\n# Funci√≥n para obtener la transcripci√≥n del video\ndef get_yt_transcript(video_url: str) -&gt; str:\n    video_id = get_video_id(video_url)\n    transcript = YouTubeTranscriptApi().fetch(video_id)\n\n    text = ' '.join(snippet.text for snippet in transcript)\n\n    return text\nEsto es todo lo que necesito para mi caso de uso, pero imagina que podriamos crear otras funciones m√°s como: Un lector de comentarios, Obtener descripciones de los videos, b√∫squeda de titulos, etc."
  },
  {
    "objectID": "posts/mcp/main.html#mcp-server-youtube",
    "href": "posts/mcp/main.html#mcp-server-youtube",
    "title": "Leveraging MCPs with LLMs",
    "section": "MCP Server: Youtube",
    "text": "MCP Server: Youtube\nAhora, como llevamos esto a que sea un Servidor MCP? Gracias a que Anthropic cre√≥ un SDK en variados lenguajes (en nuestro caso usaremos el de python), la tarea es muy sencilla.\nAntes, quiero detenerme un poco para explicar unos conceptos previos. Existen 3 elementos bases que los servidores MCPs pueden proveer:\n\nüìÅ¬†Resources: Estos son archivos que pueden ser leidos por los clientes (imaginemos un .txt o algo por el estilo)\nüõ†Ô∏è¬†Tools: Este elemento debe ser el m√°s conocido y utilizado, son b√°sicamente funciones que pueden ser llamadas por los LLM.\nüí¨¬†Prompts: Estos son templates pre-hechos que pueden ayudar a los usuarios a ejecutar de mejor manera algunas tareas.\n\nEn nuestro caso, lo que queremos incluir en el servidor es una funci√≥n de Python, por lo que encaja con el elemento Tool. Como mencion√© anteriormente, gracias al SDK (Software Development Kit) de Python el esqueleto para crearlo es algo como:\n# Se importan librerias\nfrom mcp.server.fastmcp import FastMCP\n\n# Se inicializa MCP Server\nmcp = FastMCP(\"youtube\")\n\n# Se definen elementos (resources, tools, prompts)\n...\n\n# Se corre el servidor\nif __name__ == \"__main__\":\n    mcp.run(transport='stdio')\nS√≥lo nos falta definir el tool , y esto es muy sencillo, basta con agregar el decorador @mcp.tool() en la funci√≥n que queremos transformar, resultando:\n# Funci√≥n auxiliar para obtener el video_id desde la url\ndef get_video_id(url):\n    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n    match = re.search(pattern, url)\n    return match.group(1) if match else None\n\n# Funci√≥n para obtener la transcripci√≥n del video\n@mcp.tool() # &lt;--- Agregamos esto\ndef get_yt_transcript(video_url: str) -&gt; str:\n    video_id = get_video_id(video_url)\n    transcript = YouTubeTranscriptApi().fetch(video_id)\n\n    text = ' '.join(snippet.text for snippet in transcript)\n\n    return text\nSin embargo, recordemos que esta informaci√≥n es entregada por el MCP como contexto. Una pregunta v√°lida es ¬øComo sabe el modelo qu√© hace la funci√≥n? podriamos quiz√°s pensar que lee el c√≥digo para entender bien de que trata, pero en realidad es mas directo que esto.\nEl MCP server le entrega metadata como los tipos de entrada, salida y descripci√≥n de la funci√≥n. Por esto √∫ltimo es que es muy importante comentar de forma adecuada nuestro tool:\n# Funci√≥n auxiliar para obtener el video_id desde la url\ndef get_video_id(url):\n    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n    match = re.search(pattern, url)\n    return match.group(1) if match else None\n\n# Funci√≥n para obtener la transcripci√≥n del video\n@mcp.tool()\ndef get_yt_transcript(video_url: **str**) -&gt; **str**:\n        **\"\"\"Fetches the transcript of a YouTube video.\n    Args:\n        video_url (str): The URL of the YouTube video.\n    Returns:\n        str: The transcript of the video.\"\"\"**\n    video_id = get_video_id(video_url)\n    transcript = YouTubeTranscriptApi().fetch(video_id)\n\n    text = ' '.join(snippet.text for snippet in transcript)\n\n    return text\nFinalmente, nuestro main.py o server.py resulta:\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom mcp.server.fastmcp import FastMCP\nimport re\n\n# Initialize FastMCP server\nmcp = FastMCP(\"youtube\")\n\ndef get_video_id(url):\n    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n    match = re.search(pattern, url)\n    return match.group(1) if match else None\n\n@mcp.tool()\ndef get_yt_transcript(video_url: str) -&gt; str:\n    \"\"\"Fetches the transcript of a YouTube video.\n    Args:\n        video_url (str): The URL of the YouTube video.\n    Returns:\n        str: The transcript of the video.\"\"\"\n    video_id = get_video_id(video_url)\n    transcript = YouTubeTranscriptApi().fetch(video_id)\n\n    text = ' '.join(snippet.text for snippet in transcript)\n\n    return text\n\nif __name__ == \"__main__\":\n    mcp.run(transport='stdio')\n\nTest MCP Server\nAhora que hemos definido la configuraci√≥n de nuestro MCP Server de forma sencilla (existen un mont√≥n de otras configuraciones m√°s complejas posibles), podemos utilizar la herramienta Inspector para probar el servidor, para esto debemos correr en nuestra terminal:\nnpx @modelcontextprotocol/inspector \\\n  uv \\\n  --directory path/to/server \\\n  run \\\n  package-name \\\n  args...\nEn mi caso, corr√≠:\nnpx @modelcontextprotocol/inspector \\\n  uv \\\n  --directory path/to/server \\\n  run \\\n  main.py\nEsto nos abrir√° una interfaz donde podremos interactuar con nuestro servidor. Ver alguno de los elementos que hayamos definido, ejecutar herramientas, entre otros.\n\n\n\nMCP Inspector\n\n\nComo vemos en el video, seleccionamos la √∫nica herramienta disponible y le ingresamos un video_url v√°lido, obteniendo as√≠ un resultado success con la transcripci√≥n del video. Es bastante importante el manejo de Exceptions/errores en las funciones, pero esto se escapa del scope del post."
  },
  {
    "objectID": "posts/mcp/main.html#mcp-server-notion",
    "href": "posts/mcp/main.html#mcp-server-notion",
    "title": "Leveraging MCPs with LLMs",
    "section": "MCP Server: Notion",
    "text": "MCP Server: Notion\nUna vez ya tenemos el MCP de Youtube, que ayudar√° a nuestro LLM a obtener contexto sobre la clase, necesitamos conectarlo con Notion, que es el lugar donde queremos dejar el resumen de las notas discutidas con el LLM.\nLa parte buena de esto, es que Notion ya tiene definido su propio Notion MCP Server. A continuaci√≥n definiremos el paso a paso para configurar este servidor, ya que requiere ciertos par√°metros; como a que p√°ginas les daremos accesos, y que tipo de accesos. Cabe destacar que el requisito m√≠nimo es tener una cuenta en Notion.\n\nPasos configuraci√≥n Notion MCP\n\nCrear integraci√≥n en Notion: Notion requiere crear integraciones para cualquier conexi√≥n a sus APIs, o en este caso, para el MCP. Esto se hace ingresando a https://www.notion.so/profile/integrations\n\n\nAc√° podemos seleccionar que accesos tendr√° el MCP Client\n\n\n\n\n\n\n\nNoteüîë\n\n\n\nAl crear esta integraci√≥n, Notion nos entregar√° un token, este token nos permitir√° hacer la conexi√≥n a nuestra cuenta.\n\n\n\nSeleccionamos a que secciones o paginas de Notion queremos dar acceso a el MCP Client (Claude Desktop)\n\n\n\ny ya est√°! no es nada m√°s complicado que eso. Ahora tenemos los MCP Server (Youtube y Notion) necesarios para conectarlos al MCP Client mediante el MCP Host (Claude Desktop en nuestro caso)\nPara ver estos pasos de forma m√°s detallada, puedes visitar: https://github.com/makenotion/notion-mcp-server"
  },
  {
    "objectID": "posts/mcp/main.html#conectando-mcp-servers-a-claude-desktop",
    "href": "posts/mcp/main.html#conectando-mcp-servers-a-claude-desktop",
    "title": "Leveraging MCPs with LLMs",
    "section": "Conectando MCP Servers a Claude Desktop",
    "text": "Conectando MCP Servers a Claude Desktop\nUna vez que ya tenemos los servidores creados o configurados, nos toca conectarlos a el Host MCP (En este caso, Claude Desktop). Necesitamos informarle al Host sobre los servidores MCP para que √©ste inserte apropiadamente la informaci√≥n como contexto al LLM (Claude Sonnet).\nClaude Desktop cuenta con una conexi√≥n sencilla cuando se trata de conectar a servidores remotos (como lo es el caso de Notion MCP). Simplemente debemos ir a Settings ‚Üí Connectors ‚Üí Browse Connectors\n\nEn este lugar podremos encontrar un mont√≥n de herramientas, aunque s√≥lo se permite si eres usuario Pro üí∏¬†üò¢. Felizmente existe una forma gratuita de conectar los servidores MCPs a Claude Desktop, agregando la informaci√≥n del servidor a un archivo .json de configuraci√≥n, el cual se encuentra ingresando a Settings ‚Üí Developer ‚Üí Edit Config. Esto te dirigir√° al archivo claude_desktop_config.json el cual debemos abrir con alg√∫n editor de texto; dentro ver√°s el archivo con unas llaves {} . Ac√° debemos ingresar la informaci√≥n de nuestros MCP Servers para que Claude Desktop pueda instanciarlos.\nSiguiendo la documentaci√≥n del Notion MCP, debemos agregar algo como:\n\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n      }\n    }\n \nEn donde debemos reemplazar ntn_**** con el token de Notion que obtuvimos cuando creamos la integraci√≥n. Notemos que esto son s√≥lo instrucciones para lograr inicializar el servidor, con sus respectivos argumentos y/o variables de ambiente.\nEs por esto que para el caso de nuestro Youtube MCP Server la cosa no es muy diferente:\n\"youtube\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Path/to/server/\",\n        \"run\",\n        \"server.py\"\n      ]\n   }\nPara notion utilizamos npx para correr el MCP Server, en el caso de Youtube utilizamos uv (es lo recomendado por la documentaci√≥n de MCP).\nEstas son las configuraciones por separado, para agregarlo correctamente a claude_desktop_config.json queda:\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_***\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n      }\n    },\n    \"youtube\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Path/to/server/\",\n        \"run\",\n        \"server.py\"\n      ]\n   }\n }\n}\nPodr√°s imaginar que para seguir agregando MCP Servers debemos ir a√±adiendolos ac√° y listo. Ahora s√≥lo queda guardar üíæ¬† el archivo y reiniciar Claude Desktop ! üí•¬†Deberias poder ver las conexiones cuando seleccionas ‚ÄúSearch and Tools‚Äù , o incluso en Settings ‚Üí Connectors\n\n\n\nMCP Servers en el Chat\n\n\n\n\n\nMCP Servers en Settings ‚Üí Connectors\n\n\n\n\n\n\n\n\nNote‚ö†Ô∏è\n\n\n\nPara que Claude Desktop pueda inicializar los servidores localmente, necesitamos installar nodejs (para el uso de npx) y uv.\n\n\n# Puedes usar tu administrador de paquetes favorito (o seguir las instrucciones\n# de la web de cada uno). En mi caso, utilizo brew \n\nbrew install node@22\n\nbrew install uv\n\n# Si no te funciona con brew, prueba descargando el instalador directamente desde la web"
  },
  {
    "objectID": "posts/mcp/main.html#voil√†-vamos-a-testear",
    "href": "posts/mcp/main.html#voil√†-vamos-a-testear",
    "title": "Leveraging MCPs with LLMs",
    "section": "ü•ñ¬†Voil√† ! Vamos a testear",
    "text": "ü•ñ¬†Voil√† ! Vamos a testear\nProbemos primero solicitando que nos hable de que trata alg√∫n video random de youtube, en mi caso probar√© directamente con una de las clases:\n\nFunciona!! Ahora podemos darle ese contexto a Claude Sonnet a la hora de preguntarle respecto a la claseüìù. Notemos que Claude Sonnet utiliz√≥ correctamente la funci√≥n get_yt_transcript que creamos anteriormente.\n\n\n\n\n\n\nNoteüëÄ\n\n\n\nNotar√°s que Claude Desktop te pedir√° autorizaci√≥n cada vez que quiera ejecutar una tool de alguno de tus servidores. Esto le agrega un componente importante llamado human-in-the-loop y nos permite checkear que el LLM no est√© ejecutando acciones indebidas o que no estemos usando una herramienta peligrosa para nuestro sistema.\n\n\n\nVe√°mos como nos va con el MCP Server de Notion. Recordemos que s√≥lo tiene acceso a la p√°gina que le permitimos en la creaci√≥n de la integraci√≥n:\n\nEn Notion, vemos:\n\nAgreg√≥ la p√°gina e incluso a√±adi√≥ material introductorio dentro üí•¬†Ahora ya tenemos todo listo para poder estudiar eficientemente! Alguna de las solicitudes podrian ser tales como:\n\nPorfavor resume los puntos importantes de la clase y agregalas en una clase nueva en Notion, elige el titulo que prefieras!\n\n\nComo lleg√≥ a ese resultado utilizando la regla de la cadena? Escribe el paso a paso en la secci√≥n ‚Äútroubleshooting‚Äù de mi p√°gina en notion\n\n\nPor qu√© la regla de la cadena se aplica de esa forma en los LSTM? ‚Ä¶. (luego de una discusi√≥n intensa)‚Ä¶ perfecto, deja los puntos que aclaramos en la pagina de la clase correspondiente en Notion"
  },
  {
    "objectID": "posts/mcp/main.html#que-sucede-tras-bambalinas",
    "href": "posts/mcp/main.html#que-sucede-tras-bambalinas",
    "title": "Leveraging MCPs with LLMs",
    "section": "üì∏¬†Que sucede tras bambalinas?",
    "text": "üì∏¬†Que sucede tras bambalinas?\n\n(Previo a la pregunta) El Client MCP (Claude Desktop) le da el contexto de los elementos disponibles para utilizar al LLM (Claude Sonnet).\nSe env√≠a la pregunta a Claude Sonnet\nClaude Sonnet analiza las herramientas disponibles y decide si es necesario o no utilizar alguna\nEl MCP Client ejecuta la herramienta seleccionada (si es que seleccion√≥ una) mediante el MCP Server\nLos resultados son enviados a Claude Sonnet (y agregados al contexto)\nClaude Sonnet formula la respuesta a enviar al usuario (nosotros) mediante el MCP Host (Claude Desktop)"
  },
  {
    "objectID": "posts/mcp/main.html#troubleshooting",
    "href": "posts/mcp/main.html#troubleshooting",
    "title": "Leveraging MCPs with LLMs",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nUn problema que estuve teniendo en el uso de este sistema que acabamos de crear, es que Claude Sonnet estaba teniendo problemas para escribir los titulos o formulas en Notion. Para solucionar esto, utilic√© el elemento prompting de los MCPs Servers. Para iterar r√°pido, esto lo agregu√© en el MCP Youtube que creamos üòÖ. Claramente lo ideal ser√≠a que estuviese en el MCP de Notion.\n## server.py\n\n@mcp.prompt()\ndef notion_formatting() -&gt; str:\n    \"\"\" \n    This prompt give useful information about how to use the notion tools\n    when you want to use headlines or formulas.\n    If you want to write titles/headlines or formulas, you mus include this prompt\n    \"\"\"\n\n    return \"\"\"\n    This is the schema for writing formula blocks in Notion:\n      {\n        \"children\": [\n            {\n                \"object\": \"block\",\n                \"type\": \"equation\",\n                \"equation\": {\n                    \"expression\": \"a^2 + b^2 = c^2\"\n                }\n            }\n        ]\n    }\n\n    This is the schema for writing headlines/titles in Notion:\n    {\n        \"children\": [\n            {\n                \"object\": \"block\",\n                \"type\": \"heading_1\",\n                \"heading_1\": {\n                    \"rich_text\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": {\n                                \"content\": \"Your Headline Text Here\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    }\n    \"\"\"\nAhora las f√≥rmulas y headlines son escritos de forma apropiada!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/main.html",
    "href": "posts/kaggle_nlp_disaster/main.html",
    "title": "Identificando desastres en Twitter con NLP",
    "section": "",
    "text": "Goal\nI‚Äôm learning NLP. So to do that I decided pass trough diverse NLP models, study the teory and code them! I think that it is a good method to learn Machine Learning things. So, Disclaimer: All the content on the notebooks is what I understood from diverse references (I will put the links), somethings could be wrong, If you find any mistake please let me know, so I can learn of it. Also, if you have some doubt, it will be a pleasure to me to answer it (as long as I have the answer).\nAt the end, I achieve a score of 0.843 in the LB. Is beatifull to see how you are improving the solutions step by step!\nSo, this will be the embedding Notebook, I will put the link to each specific notebook here (So it will be more readable).\nMethodologies & Notebooks:\n\n\n\nModel Notebook\nScore\n\n\n\n\nSimple Neural Network ( View on kaggle)\n0.56\n\n\nEmbeddings ( View on kaggle)\n0.797\n\n\nRecurrent Neural Networks ( View on kaggle)\n0.809\n\n\nBERT & HuggingFace ( View on kaggle)\n0.824\n\n\nMyBestSolution ( View on kaggle)\n0.843\n\n\n\n\n\nEDA\nHere I will do some preprocessing and split the data. I will use that data to each notebook!\nI think there is a lot of notebooks with a beatifull EDA, So I won‚Äôt take to much around this.\n\nLibraries\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\nData\n\ntrain_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntrain_df.head()\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\n\n\n\n\n0\n1\nNaN\nNaN\nOur Deeds are the Reason of this #earthquake M...\n1\n\n\n1\n4\nNaN\nNaN\nForest fire near La Ronge Sask. Canada\n1\n\n\n2\n5\nNaN\nNaN\nAll residents asked to 'shelter in place' are ...\n1\n\n\n3\n6\nNaN\nNaN\n13,000 people receive #wildfires evacuation or...\n1\n\n\n4\n7\nNaN\nNaN\nJust got sent this photo from Ruby #Alaska as ...\n1\n\n\n\n\n\n\n\n\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntest_df.head()\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\n\n\n\n\n0\n0\nNaN\nNaN\nJust happened a terrible car crash\n\n\n1\n2\nNaN\nNaN\nHeard about #earthquake is different cities, s...\n\n\n2\n3\nNaN\nNaN\nthere is a forest fire at spot pond, geese are...\n\n\n3\n9\nNaN\nNaN\nApocalypse lighting. #Spokane #wildfires\n\n\n4\n11\nNaN\nNaN\nTyphoon Soudelor kills 28 in China and Taiwan\n\n\n\n\n\n\n\n\n\nEDA\n\n# Target Proportion\nsns.countplot(data=train_df, x = \"target\")\n\n\n\n\n\n\n\n\nI think it is balanced!\n\n# Random example of disaster tweet\ntrain_df[train_df.target == 1].sample(1).text.values[0]\n\n'California Bush fires please evacuate affected areas ASAP when california govts advised you to do so http://t.co/ubVEVUuAch'\n\n\n\n# Random example of NO disaster tweet\ntrain_df[train_df.target == 0].sample(1).text.values[0]\n\n'Someone split a mudslide w me when I get off work'\n\n\n\n\nPre-processing\nI will do some preprocessing with Tensorflow!\n\n# Input Tensor Data\ntext = tf.data.Dataset.from_tensor_slices(train_df.text)\ntext\n\n2022-12-11 15:20:58.687517: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: (), types: tf.string&gt;\n\n\nNote that Im reading data from memory! If it would huge data I would be in troubles!\nOne advantage of initialize a Tensorflow dataset is that I will be able to create a data pipeline (batch, fetch, shuffle, etc.)\n\n# some samples\nlist(text.take(2).as_numpy_iterator())\n\n[b'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n b'Forest fire near La Ronge Sask. Canada']\n\n\nWe need to know that models don‚Äôt understand text by itself! Just numbers! For this, we vectorize the sentences. I don‚Äôt plan to use a model now, I would like to observe wich words are more present by target! (Also I don‚Äôt want to consider stopwords)\nI will use the tensorflow layer: Text Vectorization. from behind, it apply lowercase and delete punctuation. I also wants to remove stopwords, so I will build a custom standarization that do: 1. lowercase 2. strip punctuation 3. remove stop words!\nClick here if you don‚Äôt know what are stop words\n\n#### COUNT WORDS BY TARGET\n\ndef custom_standardization(inputs):\n    \"\"\"\n    Apply: lowercase, remove punctuation and stopwords\n    \"\"\"\n    PUNCTUATION = r'[!\"#$%&()\\*\\+,-\\./:;&lt;=&gt;?@\\[\\\\\\]^_`{|}~\\']'\n    lowercase = tf.strings.lower(inputs) # lowercase\n    strip = tf.strings.regex_replace(lowercase, PUNCTUATION, '') # strip punctuation\n    stopwrd = tf.strings.regex_replace(strip, r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*', '')\n    return stopwrd\n    \n\n# model to apply vectorize_layer with custom standardization\nvectorize_layer = tf.keras.layers.TextVectorization(output_mode = 'multi_hot', standardize = custom_standardization)\nvectorize_layer.adapt(text)\n\n# model to vectorize\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.Input(shape=(1,), dtype=tf.string))\nmodel.add(vectorize_layer)\n\n# make counter\ntrain_count = model.predict(text.batch(batch_size = len(text))) # predict to count \ntoken_counts = pd.DataFrame(columns = vectorize_layer.get_vocabulary(), data = train_count) # df with tokens and count\ntrain_df.rename(columns = {\"target\":\"disaster_target\"}, inplace = True) # rename target because there is a word target in data\ncount_df = pd.concat([train_df, token_counts], axis = 1) #concat\ngroup_count = count_df.iloc[:,4:].groupby(\"disaster_target\", as_index = False).sum() # count token for each target\nmelt_count = pd.melt(group_count, id_vars=[\"disaster_target\"], value_name = \"count\") # each token to row\nmelt_count.sort_values(by=[\"count\"], ascending = False).head(30)\n\n2022-12-11 15:20:58.997911: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: FutureWarning: This dataframe has a column name that matches the 'value_name' column name of the resulting Dataframe. In the future this will raise an error, please set the 'value_name' parameter of DataFrame.melt to a unique name.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\n\n\n\n\n\n\ndisaster_target\nvariable\ncount\n\n\n\n\n2\n0\nlike\n239.0\n\n\n4\n0\nim\n221.0\n\n\n6\n0\namp\n174.0\n\n\n12\n0\nnew\n163.0\n\n\n9\n1\nfire\n162.0\n\n\n10\n0\nget\n158.0\n\n\n22\n0\ndont\n136.0\n\n\n21\n1\nnews\n130.0\n\n\n18\n0\none\n122.0\n\n\n15\n1\nvia\n121.0\n\n\n42\n0\nbody\n110.0\n\n\n51\n1\ncalifornia\n108.0\n\n\n53\n1\nsuicide\n104.0\n\n\n17\n1\npeople\n101.0\n\n\n37\n1\npolice\n97.0\n\n\n35\n1\ndisaster\n96.0\n\n\n14\n0\nvia\n96.0\n\n\n7\n1\namp\n95.0\n\n\n38\n0\nwould\n93.0\n\n\n95\n1\nkilled\n90.0\n\n\n24\n0\nvideo\n90.0\n\n\n16\n0\npeople\n90.0\n\n\n3\n1\nlike\n88.0\n\n\n117\n1\nhiroshima\n84.0\n\n\n87\n1\nfires\n82.0\n\n\n62\n0\nknow\n82.0\n\n\n28\n0\n2\n81.0\n\n\n104\n0\nfull\n81.0\n\n\n84\n0\nlove\n81.0\n\n\n58\n0\ntime\n80.0\n\n\n\n\n\n\n\nAfter I build it I realized that it is not necessary to instantiate a model to use layers! üòÖ\n\n\n\nSplit Data\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(train_df[[col for col in train_df.columns if col != 'disaster_target']], train_df.disaster_target, test_size=0.2, random_state=13)\n\n\n# To csv (In some notebooks I will use this data)\npd.concat([X_train, y_train], axis = 1).to_csv('df_train.csv',index = False)\npd.concat([X_test, y_test], axis = 1).to_csv('df_test.csv',index = False)\n\nThis dataset is here: https://www.kaggle.com/datasets/diegomachado/df-split\n\n# Delete it from memory\ndel train_df, test_df, X_train, X_test, y_train, y_test\ngc.collect()\n\n671"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "As we see in the previous notebooks. Embeddings solve the problem of have a good representation of text! But we still having some other problems:\nThe input of the Dense layers could vary in size! There are sequences of differents size! (Remember that we solve it with padding and truncation) Due to sequences could be large, there is a lot of computational costs! Layers are not sharing information! (They have different weighths) So we are not taking into account the order of the words, the context, or the words around To take care of this points, we will apply Recurrent Neural Networks in this notebook! üí•üí•üí•üí•\nI recommend spend time in understand what is happen inside RNN. For this I see a lot of videos and read some blogs. I will let you some of them here:\n\nhttps://www.youtube.com/watch?v=Y2wfIKQyd1I\nhttps://www.tensorflow.org/text/tutorials/text_classification_rnn\nhttps://www.youtube.com/watch?v=AsNTP8Kwu80\n\n\n\n\nNLP-Embedding-RNN.png\n\n\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')\n\n\n\n\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 02:13:04.769598: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 40 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 02:13:04.955961: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\n\n\n\nNow we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\nAs Twitter Embedding is the best so far. We will continue use it!\nIn this case we will use the SimpleRNN Tensorflow Layer. Furthermore, we also going to use the Bidirectional Layer. This is basically to apply two RNN, one that process the sequence from left to right, and another that process the sequence from right to left. It makes sense because we will know all the sequence input at the moment we want to predict. Also, I prove with and without Bidirectional, and with Bidirectional improve a lot wit respect without it. I think that in timeseries is not a good idea because we don‚Äôt know the future at the moment we want predict.\nClick here to read about Bidirectional RNN\nüëÄ Also, note that now we will define explicitly the activation functions in the NN. Also we will apply the sigmoid function at the end. So know the loss functions should has: from_logits=False or let it by default. (I just do it because I want to prove both ways)\n\n# GloVe Twitter Embedding\nwv = KeyedVectors.load_word2vec_format('../input/twitter-word2vecs-wordvecs-from-godin/word2vec_twitter_tokens.bin', \n                                       binary=True,\n                                       unicode_errors='ignore')\n\n\n# Build embedding matrix \n\nvoc = vectorization_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\n# We have to construct the embedding matrix with weigths from our own vocabulary\n# shape embedding matrix : (vocab_size, embedding_dim)\nnum_tokens = len(voc)\nembedding_dim = 400 # we download glove 100 dimension\nhits = []\nmisses = []\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    if word in wv:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_vector = wv[word]\n        embedding_matrix[i] = embedding_vector\n        hits.append(word)\n    else:\n        misses.append(word)\nprint(\"Converted %d words (%d misses)\" % (len(hits), len(misses)))\n\nConverted 7873 words (2127 misses)\n\n\n\n# Model \nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False\n    ),\n    layers.SpatialDropout1D(0.3),\n    layers.Bidirectional(layers.SimpleRNN(64, dropout = 0.2, recurrent_dropout = 0.2)),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(16, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 40)                0         \n_________________________________________________________________\nembedding (Embedding)        (None, 40, 400)           4000000   \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 40, 400)           0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128)               59520     \n_________________________________________________________________\ndense (Dense)                (None, 32)                4128      \n_________________________________________________________________\ndropout (Dropout)            (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                528       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 4,064,193\nTrainable params: 64,193\nNon-trainable params: 4,000,000\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\n\n\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback]\n)\n\nEpoch 1/100\n191/191 [==============================] - 16s 70ms/step - loss: 0.6465 - binary_accuracy: 0.6245 - val_loss: 0.5218 - val_binary_accuracy: 0.7610\nEpoch 2/100\n191/191 [==============================] - 13s 67ms/step - loss: 0.5322 - binary_accuracy: 0.7484 - val_loss: 0.4914 - val_binary_accuracy: 0.7722\nEpoch 3/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.5156 - binary_accuracy: 0.7675 - val_loss: 0.4749 - val_binary_accuracy: 0.7846\nEpoch 4/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.5046 - binary_accuracy: 0.7768 - val_loss: 0.4774 - val_binary_accuracy: 0.7912\nEpoch 5/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4972 - binary_accuracy: 0.7811 - val_loss: 0.4751 - val_binary_accuracy: 0.7827\nEpoch 6/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4820 - binary_accuracy: 0.7806 - val_loss: 0.4750 - val_binary_accuracy: 0.7774\nEpoch 7/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4786 - binary_accuracy: 0.7836 - val_loss: 0.4745 - val_binary_accuracy: 0.7859\nEpoch 8/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4677 - binary_accuracy: 0.7870 - val_loss: 0.4880 - val_binary_accuracy: 0.7748\nEpoch 9/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4674 - binary_accuracy: 0.7885 - val_loss: 0.4692 - val_binary_accuracy: 0.7859\nEpoch 10/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4619 - binary_accuracy: 0.7941 - val_loss: 0.4784 - val_binary_accuracy: 0.7761\nEpoch 11/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4604 - binary_accuracy: 0.7957 - val_loss: 0.4947 - val_binary_accuracy: 0.7715\nEpoch 12/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4576 - binary_accuracy: 0.7995 - val_loss: 0.4750 - val_binary_accuracy: 0.7827\nEpoch 13/100\n191/191 [==============================] - 13s 68ms/step - loss: 0.4503 - binary_accuracy: 0.7972 - val_loss: 0.4668 - val_binary_accuracy: 0.7925\nEpoch 14/100\n191/191 [==============================] - 13s 71ms/step - loss: 0.4568 - binary_accuracy: 0.7979 - val_loss: 0.4693 - val_binary_accuracy: 0.7794\nEpoch 15/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4446 - binary_accuracy: 0.8015 - val_loss: 0.4820 - val_binary_accuracy: 0.7807\nEpoch 16/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4411 - binary_accuracy: 0.8048 - val_loss: 0.4815 - val_binary_accuracy: 0.7676\nEpoch 17/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4406 - binary_accuracy: 0.8061 - val_loss: 0.4686 - val_binary_accuracy: 0.7840\nEpoch 18/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4396 - binary_accuracy: 0.8003 - val_loss: 0.4630 - val_binary_accuracy: 0.7912\nEpoch 19/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4364 - binary_accuracy: 0.8080 - val_loss: 0.4975 - val_binary_accuracy: 0.7761\nEpoch 20/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4346 - binary_accuracy: 0.8025 - val_loss: 0.4834 - val_binary_accuracy: 0.7820\nEpoch 21/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4325 - binary_accuracy: 0.8026 - val_loss: 0.4844 - val_binary_accuracy: 0.7741\nEpoch 22/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4780 - binary_accuracy: 0.7767 - val_loss: 0.4947 - val_binary_accuracy: 0.7702\nEpoch 23/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4338 - binary_accuracy: 0.8046 - val_loss: 0.4666 - val_binary_accuracy: 0.7873\n\n\n\n\n\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"RNN_submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\n\n\n\nRNN has some benefits. But also some disadvantages (See the references). One of them, maybe the most named is the Exploding-Vanishing gradient problem!\n\nTo improve our solution, maybe we could try adding more RNN layers. Note that for this we need to set up return_sequences = True in the previous LSTM layers. Thats because by default the output of a RNN or LSTM layer is the last hidden state, but to feed to another LSTM we need a sequence (and this sequence is given thanks to return_sequences = True). And of course we could try hyperparameter optimization\n\nSo there are other arquitectures that take care of it. We will explore those now:\n\nLSTM\nGRU"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#libraries",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#libraries",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Lambda\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n#import tensorflow_hub as hub\n#import tensorflow_text as text # Bert preprocess uses this \nfrom tensorflow.keras.optimizers import Adam\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.models import KeyedVectors\n\n#nltk.download('stopwords')"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "# Load data\ntrain = pd.read_csv(\"/kaggle/input/df-split/df_split/df_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/df-split/df_split/df_test.csv\")\n\nX_train = train[[col for col in train.columns if col != 'target']].copy()\ny_train = train['target'].copy()\n\nX_test = test[[col for col in test.columns if col != 'target']].copy()\ny_test = test['target'].copy()\n\n\n# Tensorflow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.text, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.text, y_test))\ntrain_ds\n\n2022-12-12 02:13:04.769598: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n\n\n&lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)&gt;\n\n\n\n# Vectorization Layer\n\nmax_features = 10000 # Vocabulary (TensorFlow select the most frequent tokens)\nsequence_length = 40 # It will pad or truncate sequences\nvectorization_layer = TextVectorization(\n    max_tokens = max_features,\n    output_sequence_length = sequence_length,\n)\n\n# Adapt is to compute metrics (In this case the vocabulary)\nvectorization_layer.adapt(X_train.text)\n\n2022-12-12 02:13:04.955961: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data-pipeline",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#data-pipeline",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "Now we need to prepare the data pipeline:\nbatch -&gt; cache -&gt; prefetch\n\nBatch : Create a set of samples (Those will be processed together in the model)\nCache: The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\nPrefetch : This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n\nOptional: You can do it another steps like shuffle\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#model",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#model",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "As Twitter Embedding is the best so far. We will continue use it!\nIn this case we will use the SimpleRNN Tensorflow Layer. Furthermore, we also going to use the Bidirectional Layer. This is basically to apply two RNN, one that process the sequence from left to right, and another that process the sequence from right to left. It makes sense because we will know all the sequence input at the moment we want to predict. Also, I prove with and without Bidirectional, and with Bidirectional improve a lot wit respect without it. I think that in timeseries is not a good idea because we don‚Äôt know the future at the moment we want predict.\nClick here to read about Bidirectional RNN\nüëÄ Also, note that now we will define explicitly the activation functions in the NN. Also we will apply the sigmoid function at the end. So know the loss functions should has: from_logits=False or let it by default. (I just do it because I want to prove both ways)\n\n# GloVe Twitter Embedding\nwv = KeyedVectors.load_word2vec_format('../input/twitter-word2vecs-wordvecs-from-godin/word2vec_twitter_tokens.bin', \n                                       binary=True,\n                                       unicode_errors='ignore')\n\n\n# Build embedding matrix \n\nvoc = vectorization_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\n# We have to construct the embedding matrix with weigths from our own vocabulary\n# shape embedding matrix : (vocab_size, embedding_dim)\nnum_tokens = len(voc)\nembedding_dim = 400 # we download glove 100 dimension\nhits = []\nmisses = []\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    if word in wv:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_vector = wv[word]\n        embedding_matrix[i] = embedding_vector\n        hits.append(word)\n    else:\n        misses.append(word)\nprint(\"Converted %d words (%d misses)\" % (len(hits), len(misses)))\n\nConverted 7873 words (2127 misses)\n\n\n\n# Model \nmodel = tf.keras.Sequential([\n    layers.Input(shape=(1,), dtype=tf.string),\n    vectorization_layer,\n    layers.Embedding(num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False\n    ),\n    layers.SpatialDropout1D(0.3),\n    layers.Bidirectional(layers.SimpleRNN(64, dropout = 0.2, recurrent_dropout = 0.2)),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(16, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, 40)                0         \n_________________________________________________________________\nembedding (Embedding)        (None, 40, 400)           4000000   \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 40, 400)           0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128)               59520     \n_________________________________________________________________\ndense (Dense)                (None, 32)                4128      \n_________________________________________________________________\ndropout (Dropout)            (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                528       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 4,064,193\nTrainable params: 64,193\nNon-trainable params: 4,000,000\n_________________________________________________________________\n\n\n\nplot_model(model, show_shapes=True)\n\n\n\n\n\n\n\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nepochs = 100\nhistory = model.fit(\n    train_ds,\n    validation_data = test_ds,\n    epochs=epochs,\n    callbacks = [early_stop_callback]\n)\n\nEpoch 1/100\n191/191 [==============================] - 16s 70ms/step - loss: 0.6465 - binary_accuracy: 0.6245 - val_loss: 0.5218 - val_binary_accuracy: 0.7610\nEpoch 2/100\n191/191 [==============================] - 13s 67ms/step - loss: 0.5322 - binary_accuracy: 0.7484 - val_loss: 0.4914 - val_binary_accuracy: 0.7722\nEpoch 3/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.5156 - binary_accuracy: 0.7675 - val_loss: 0.4749 - val_binary_accuracy: 0.7846\nEpoch 4/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.5046 - binary_accuracy: 0.7768 - val_loss: 0.4774 - val_binary_accuracy: 0.7912\nEpoch 5/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4972 - binary_accuracy: 0.7811 - val_loss: 0.4751 - val_binary_accuracy: 0.7827\nEpoch 6/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4820 - binary_accuracy: 0.7806 - val_loss: 0.4750 - val_binary_accuracy: 0.7774\nEpoch 7/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4786 - binary_accuracy: 0.7836 - val_loss: 0.4745 - val_binary_accuracy: 0.7859\nEpoch 8/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4677 - binary_accuracy: 0.7870 - val_loss: 0.4880 - val_binary_accuracy: 0.7748\nEpoch 9/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4674 - binary_accuracy: 0.7885 - val_loss: 0.4692 - val_binary_accuracy: 0.7859\nEpoch 10/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4619 - binary_accuracy: 0.7941 - val_loss: 0.4784 - val_binary_accuracy: 0.7761\nEpoch 11/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4604 - binary_accuracy: 0.7957 - val_loss: 0.4947 - val_binary_accuracy: 0.7715\nEpoch 12/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4576 - binary_accuracy: 0.7995 - val_loss: 0.4750 - val_binary_accuracy: 0.7827\nEpoch 13/100\n191/191 [==============================] - 13s 68ms/step - loss: 0.4503 - binary_accuracy: 0.7972 - val_loss: 0.4668 - val_binary_accuracy: 0.7925\nEpoch 14/100\n191/191 [==============================] - 13s 71ms/step - loss: 0.4568 - binary_accuracy: 0.7979 - val_loss: 0.4693 - val_binary_accuracy: 0.7794\nEpoch 15/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4446 - binary_accuracy: 0.8015 - val_loss: 0.4820 - val_binary_accuracy: 0.7807\nEpoch 16/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4411 - binary_accuracy: 0.8048 - val_loss: 0.4815 - val_binary_accuracy: 0.7676\nEpoch 17/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4406 - binary_accuracy: 0.8061 - val_loss: 0.4686 - val_binary_accuracy: 0.7840\nEpoch 18/100\n191/191 [==============================] - 14s 72ms/step - loss: 0.4396 - binary_accuracy: 0.8003 - val_loss: 0.4630 - val_binary_accuracy: 0.7912\nEpoch 19/100\n191/191 [==============================] - 14s 71ms/step - loss: 0.4364 - binary_accuracy: 0.8080 - val_loss: 0.4975 - val_binary_accuracy: 0.7761\nEpoch 20/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4346 - binary_accuracy: 0.8025 - val_loss: 0.4834 - val_binary_accuracy: 0.7820\nEpoch 21/100\n191/191 [==============================] - 14s 73ms/step - loss: 0.4325 - binary_accuracy: 0.8026 - val_loss: 0.4844 - val_binary_accuracy: 0.7741\nEpoch 22/100\n191/191 [==============================] - 13s 70ms/step - loss: 0.4780 - binary_accuracy: 0.7767 - val_loss: 0.4947 - val_binary_accuracy: 0.7702\nEpoch 23/100\n191/191 [==============================] - 13s 69ms/step - loss: 0.4338 - binary_accuracy: 0.8046 - val_loss: 0.4666 - val_binary_accuracy: 0.7873"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"RNN_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations",
    "title": "3. Recurrent Neural Networks",
    "section": "",
    "text": "RNN has some benefits. But also some disadvantages (See the references). One of them, maybe the most named is the Exploding-Vanishing gradient problem!\n\nTo improve our solution, maybe we could try adding more RNN layers. Note that for this we need to set up return_sequences = True in the previous LSTM layers. Thats because by default the output of a RNN or LSTM layer is the last hidden state, but to feed to another LSTM we need a sequence (and this sequence is given thanks to return_sequences = True). And of course we could try hyperparameter optimization\n\nSo there are other arquitectures that take care of it. We will explore those now:\n\nLSTM\nGRU"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-1",
    "title": "3. Recurrent Neural Networks",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\npreds.shape\n\n(3263, 1)\n\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"LSTM_submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#observations-1",
    "title": "3. Recurrent Neural Networks",
    "section": "Observations:",
    "text": "Observations:\nNice, we find our better solution so far!\n\nTo improve our solution, maybe we could try adding more LSTM. Note that for this we need to set up return_sequences = True in the previous LSTM layers. Thats because by default the output of a RNN or LSTM layer is the last hidden state, but to feed to another LSTM we need a sequence (and this sequence is given thanks to return_sequences = True). And of course we could try hyperparameter optimization\n\nNow we will try GRU Recurrent Neural Network"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-2",
    "href": "posts/kaggle_nlp_disaster/seqclass-3-rnn.html#predict-test-2",
    "title": "3. Recurrent Neural Networks",
    "section": "Predict Test",
    "text": "Predict Test\n\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n\ntopred_ds = tf.data.Dataset.from_tensor_slices(test_df.text)\nAUTOTUNE = tf.data.AUTOTUNE\ntopred_ds = topred_ds.batch(32).cache().prefetch(buffer_size=AUTOTUNE)\n\n\npreds = model.predict(topred_ds)\n\n\n# Now we not apply sigmoid function here because or activation function\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\ntest_df[\"target\"] = tf.round(preds)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n1\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\n#sub.to_csv(\"GRU_submission.csv\", index = False)\nsub.to_csv(\"submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nA little better than LSTM!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html",
    "title": "4. BERT",
    "section": "",
    "text": "As we see in the previous notebook, RNN improve the results and give us some benefits. But they are expensive to train, mainly because they are not paralellizable! Now we are in the transformers era. Transformers are models that leverage the mechanism of self-attention.\nIn this specific case we will apply BERT. I will let you with some references that help me to understand what is happend behind scenes:\n\nhttp://nlp.seas.harvard.edu/annotated-transformer/\nhttp://jalammar.github.io/illustrated-transformer/\nhttps://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\nhttps://www.youtube.com/watch?v=xI0HHN5XKDo&t=2s&pp=ugMICgJlcxABGAE%3D\nhttps://www.youtube.com/watch?v=7kLi8u2dJz0\nhttps://www.youtube.com/watch?v=TQQlZhbC5ps&list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE&index=1\n\n\nRemember that this belong to a NLP Notebook series where I am learning and testing different NLP approachs in this competition. Like NN, Embedding, RNN, Transformers, HuggingFace, etc.\n\n\nTo see the other notebooks visit: https://www.kaggle.com/code/diegomachado/seqclass-nn-embed-rnn-lstm-gru-bert-hf"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#hf-pipelines---zero-shot-classification",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#hf-pipelines---zero-shot-classification",
    "title": "4. BERT",
    "section": "HF ü§ó Pipelines - Zero Shot Classification",
    "text": "HF ü§ó Pipelines - Zero Shot Classification\nAs you can see in the hf documentation. HuggingFace has pipelines, where we can develop a variety of nlp (and more) applications! (Text generation, classification, summarization, etc)\nNow we will do Zero shot classification. That means that we will predict classes that wasn‚Äôt observe during traning. It is a difficult task\n\nclassifier = pipeline(\"zero-shot-classification\",\n                      model=\"facebook/bart-large-mnli\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# predict\ncandidate_labels = ['disaster']\ntest_df['target'] = [x['scores'][0] for x in classifier(test_df.text.to_list(), candidate_labels)]\n\n\ntest_df[\"target\"] = tf.round(test_df.target)\ntest_df[\"target\"] = test_df[\"target\"].astype(int)\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"HF_ZS_Submission.csv\", index = False)\n\n\n\n\nimage.png\n\n\nMake sense because is a zero shot classification!"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#fine-tune",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#fine-tune",
    "title": "4. BERT",
    "section": "Fine-Tune",
    "text": "Fine-Tune\nNow we will download a model from HuggingFace models and finetune with our data\n\n# Model name to fine tune\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n\nfrom transformers import AutoTokenizer\n\n# First we need to use the tokenizer (Is similar we did in the Tensorflow hub with the preprocessing layer)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"])\n\n\n\n\n\n\n\n\n\n\n\n# We need to load our data, for this we use HF datasets (read directly from the disk, so save us ram)\nfrom datasets import load_dataset\ndata_files = {\"train\": \"/kaggle/input/df-split/df_split/df_train.csv\",\n             \"test\":\"/kaggle/input/df-split/df_split/df_test.csv\"}\ndataset = load_dataset(\"csv\", data_files = data_files, usecols = ['text', 'target'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1860883215d5579d/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n\n\n\n\n\n\n\n\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1860883215d5579d/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'target'],\n        num_rows: 6090\n    })\n    test: Dataset({\n        features: ['text', 'target'],\n        num_rows: 1523\n    })\n})\n\n\n\n# preprocess\ndataset = dataset.map(tokenize_dataset)\n\n\n\n\n\n\n\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n\n# we transform the HF dataset into TF dataset to fine tune\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_train_dataset = train_dataset.to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True)\n\ntf_val_dataset = test_dataset.to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    label_cols=[\"target\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True\n    )"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#model-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#model-1",
    "title": "4. BERT",
    "section": "Model",
    "text": "Model\n\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(3e-5),metrics = ['accuracy'])\n\n\nearly_stop_callback = EarlyStopping(patience = 5)\nmodel.fit(tf_train_dataset,\n         validation_data = tf_val_dataset,\n          epochs = 20,\n          callbacks = [early_stop_callback]\n         )\n\n\n\n\n2022-12-12 03:54:13.603300: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\nSome layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_20']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nEpoch 1/20\n380/380 [==============================] - 44s 87ms/step - loss: 0.4815 - accuracy: 0.7913 - val_loss: 0.3999 - val_accuracy: 0.8270\nEpoch 2/20\n380/380 [==============================] - 31s 83ms/step - loss: 0.3179 - accuracy: 0.8750 - val_loss: 0.4339 - val_accuracy: 0.8164\nEpoch 3/20\n380/380 [==============================] - 31s 83ms/step - loss: 0.2035 - accuracy: 0.9238 - val_loss: 0.4884 - val_accuracy: 0.8053\nEpoch 4/20\n380/380 [==============================] - 31s 83ms/step - loss: 0.1147 - accuracy: 0.9572 - val_loss: 0.5929 - val_accuracy: 0.7967\nEpoch 5/20\n380/380 [==============================] - 32s 83ms/step - loss: 0.0829 - accuracy: 0.9729 - val_loss: 0.7295 - val_accuracy: 0.7717\nEpoch 6/20\n380/380 [==============================] - 31s 82ms/step - loss: 0.0579 - accuracy: 0.9760 - val_loss: 0.9569 - val_accuracy: 0.7855\n\n\n&lt;keras.callbacks.History at 0x7f06a863c950&gt;\n\n\n\nPredict Test\n\npred_dataset = load_dataset(\"csv\", data_files = \"/kaggle/input/nlp-getting-started/test.csv\", usecols = ['text'])\n\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n\n\n\n\n\n\n\n\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e12e02ba714b9048/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n\n\n\n\n\n\n# preprocess\npred_dataset = pred_dataset.map(tokenize_dataset)\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\ntf_pred_dataset = pred_dataset['train'].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\"],\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=False)\n\n\npreds = model.predict(tf_pred_dataset)\n\n\npreds = np.argmax(preds['logits'], axis = 1)\n\n\ntest_df[\"target\"] = preds\n\n\nsub = test_df[[\"id\", \"target\"]]\nsub\n\n\n\n\n\n\n\n\nid\ntarget\n\n\n\n\n0\n0\n1\n\n\n1\n2\n1\n\n\n2\n3\n1\n\n\n3\n9\n1\n\n\n4\n11\n1\n\n\n...\n...\n...\n\n\n3258\n10861\n0\n\n\n3259\n10865\n1\n\n\n3260\n10868\n1\n\n\n3261\n10874\n1\n\n\n3262\n10875\n1\n\n\n\n\n3263 rows √ó 2 columns\n\n\n\n\nsub.to_csv(\"DistilBERT_FT_Submission.csv\", index = False)\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#observations-1",
    "href": "posts/kaggle_nlp_disaster/seqclass-4-bert-tensorflow-huggingface.html#observations-1",
    "title": "4. BERT",
    "section": "Observations",
    "text": "Observations\nNot bad! Note that we don‚Äôt tune much this model, so probably we could improve it easily!\nThe important point is how easy is use hub models from huggingface and finetune. If you look the website, you can see that there are a lot of datasets and models made by de comunnity!\nIn the next notebook I will do my best combining all the stuff that we saw in this notebooks NLP series to get a better result. Also I will pre-process the data before feed it into the model"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Diegulio‚Äôs Blog ",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLeveraging MCPs with LLMs\n\n\nDesarrollamos un framework para el estudio eficiente utilizando MCPs\n\n\n\nmcp\n\nllm\n\napplication\n\n\n\n\n\n\nDiegulio\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\nPaper Implementation Series: Generative Adversarial Networks\n\n\nImplementamos el paper Generative Adversarial Networks\n\n\n\npython\n\npytorch\n\npaper\n\n\n\n\n\n\nDiegulio\n\n27 min\n\n\n\n\n\n\n\n\n\n\n\nImage Classification with Pytorch Lightning\n\n\nClassify images of pets using Pytorch Lightning\n\n\n\nPytorch\n\nLightning\n\nWandb\n\nTimm\n\nGradio\n\n\n\n\n\n\nDiegulio\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\nData ‚ù§Ô∏è Chat: Chatea con tu Curriculum\n\n\nCreamos una aplicaci√≥n web que te permite chatear con tu CV!\n\n\n\npython\n\nllm\n\npalm\n\nstreamlit\n\nlangchain\n\n\n\n\n\n\nDiegulio\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\nIniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT\n\n\nCocina tu comida favorita con la ayuda de LLM\n\n\n\npython\n\nllm\n\nchatgpt\n\nlangchain\n\ngradio\n\n\n\n\n\n\nDiegulio\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\nBirdClef 2023 Competition\n\n\nClasificando Sonidos de P√°jaros con Redes Neuronales\n\n\n\nAudio\n\ncode\n\nKaggle\n\nPytorch\n\n\n\n\n\n\nDiegulio\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\nBack To Sit\n\n\nA library to notify you when your code is done!\n\n\n\npython\n\nlibrary\n\n\n\n\n\n\nDiegulio\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nIdentificando desastres en Twitter con NLP\n\n\nAprende a usar Natural Language Processing (NLP) para identificar desastres en Twitter\n\n\n\nNLP\n\ncode\n\nKaggle\n\n\n\n\n\n\nDiegulio\n\n3 min\n\n\n\n\nNo matching items"
  }
]