<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.333">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Diegulio">
<meta name="dcterms.date" content="2023-07-05">

<title>diegulio’s blog🎯 - Iniciando en LLM: Crea tu primera aplicación con LangChain y ChatGPT</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../raccoon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">diegulio’s blog🎯</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Iniciando en LLM: Crea tu primera aplicación con LangChain y ChatGPT</h1>
            <p class="subtitle lead">Cocina tu comida favorita con la ayuda de LLM</p>
                                <div class="quarto-categories">
                <div class="quarto-category">python</div>
                <div class="quarto-category">llm</div>
                <div class="quarto-category">chatgpt</div>
                <div class="quarto-category">langchain</div>
                <div class="quarto-category">gradio</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Diegulio </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 5, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#llm-recipe" id="toc-llm-recipe" class="nav-link active" data-scroll-target="#llm-recipe">LLM Recipe</a></li>
  <li><a href="#tópico-large-language-models" id="toc-tópico-large-language-models" class="nav-link" data-scroll-target="#tópico-large-language-models">📚&nbsp;Tópico: Large Language Models</a></li>
  <li><a href="#motivación-de-compras-con-llm" id="toc-motivación-de-compras-con-llm" class="nav-link" data-scroll-target="#motivación-de-compras-con-llm">🛒&nbsp;Motivación: De compras con LLM</a></li>
  <li><a href="#tool-path-que-utilizaremos" id="toc-tool-path-que-utilizaremos" class="nav-link" data-scroll-target="#tool-path-que-utilizaremos">🔨&nbsp;Tool Path: Que utilizaremos</a></li>
  <li><a href="#concept-path-que-aprenderemos" id="toc-concept-path-que-aprenderemos" class="nav-link" data-scroll-target="#concept-path-que-aprenderemos">💭&nbsp;Concept Path: Que aprenderemos</a></li>
  <li><a href="#estrategia-como-abordamos" id="toc-estrategia-como-abordamos" class="nav-link" data-scroll-target="#estrategia-como-abordamos">♟️&nbsp;Estrategia: Como abordamos</a>
  <ul class="collapse">
  <li><a href="#prompt" id="toc-prompt" class="nav-link" data-scroll-target="#prompt">Prompt:</a></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output">Output:</a></li>
  </ul></li>
  <li><a href="#tools" id="toc-tools" class="nav-link" data-scroll-target="#tools">🔨&nbsp;Tools</a></li>
  <li><a href="#prototyping" id="toc-prototyping" class="nav-link" data-scroll-target="#prototyping">🧠&nbsp;Prototyping</a>
  <ul class="collapse">
  <li><a href="#obtener-receta-mediante-llm" id="toc-obtener-receta-mediante-llm" class="nav-link" data-scroll-target="#obtener-receta-mediante-llm">1. Obtener receta mediante LLM</a>
  <ul class="collapse">
  <li><a href="#prompt-templates" id="toc-prompt-templates" class="nav-link" data-scroll-target="#prompt-templates">Prompt Templates</a></li>
  <li><a href="#code-time" id="toc-code-time" class="nav-link" data-scroll-target="#code-time">👨🏾‍💻&nbsp;Code time!</a></li>
  <li><a href="#llm-model" id="toc-llm-model" class="nav-link" data-scroll-target="#llm-model">LLM Model</a></li>
  </ul></li>
  <li><a href="#obtener-ingredientes-de-una-receta-mediante-llm" id="toc-obtener-ingredientes-de-una-receta-mediante-llm" class="nav-link" data-scroll-target="#obtener-ingredientes-de-una-receta-mediante-llm">2. Obtener ingredientes de una receta mediante LLM</a></li>
  <li><a href="#cadena-final" id="toc-cadena-final" class="nav-link" data-scroll-target="#cadena-final">3. Cadena Final</a></li>
  <li><a href="#output-parser" id="toc-output-parser" class="nav-link" data-scroll-target="#output-parser">4. Output Parser</a></li>
  </ul></li>
  <li><a href="#front-end" id="toc-front-end" class="nav-link" data-scroll-target="#front-end">🧐&nbsp;Front-End</a></li>
  <li><a href="#próximos-pasos" id="toc-próximos-pasos" class="nav-link" data-scroll-target="#próximos-pasos">🚀&nbsp;Próximos Pasos</a></li>
  <li><a href="#conclusión" id="toc-conclusión" class="nav-link" data-scroll-target="#conclusión">🥳&nbsp;Conclusión</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="llm-recipe" class="level1">
<h1>LLM Recipe</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://github.com/diegulio/llm-recipe"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Antes de comenzar a leer esto, recuerda que yo estoy aprendiendo junto contigo. Si tienes alguna duda, sugerencia, correción o comentario, no dudes en escribirme a mi <a href="https://www.linkedin.com/in/dieguliomachado/">LinkedIn</a>.</p>
</div>
</div>
</section>
<section id="tópico-large-language-models" class="level1">
<h1>📚&nbsp;Tópico: Large Language Models</h1>
<p>Estoy seguro que alguna vez haz escuchado el término ChatGPT. Yep! ese robot 🤖&nbsp;que te hace hasta la tesis! ChatGPT es un LLM (Large Language Model), lo que en otras palabras significa que es un modelo matemático que se alimenta de grandes volumenes de texto y es capaz de generar lenguaje humano de manera muuy avanzada. Y si que lo hemos visto en acción, yo lo ocupo en mi dia a dia y me facilita un montón mi trabajo.</p>
<p>Ahora ¿ Que tal si dejamos que un LLM nos describa que es un LLM?</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
🤖
</div>
</div>
<div class="callout-body-container callout-body">
<p>Un large language model (modelo de lenguaje amplio) es un tipo de sistema de inteligencia artificial diseñado para comprender y generar lenguaje humano de manera avanzada. Estos modelos están entrenados en grandes cantidades de texto y utilizan técnicas de aprendizaje automático para aprender patrones y estructuras lingüísticas. Un large language model, como GPT-3.5, puede responder preguntas, redactar textos, generar código, traducir idiomas y realizar una variedad de tareas relacionadas con el lenguaje natural. Estos modelos son capaces de capturar la complejidad y sutileza del lenguaje humano, y pueden adaptarse a diferentes contextos y estilos de escritura.</p>
</div>
</div>
<p>En este post <strong>no</strong> hablaremos de las técnicas utilizadas para construir un LLM, ya que en internet hay muy buenas fuentes para aprender sobre esto, acá te dejo una:</p>
<p><a href="https://docs.cohere.com/docs/llmu">LLM University (LLMU) | Cohere</a></p>
<p>Lo que haremos en este post, será utilizar los LLM en el bien de la humanidad ! 🦸🏽‍♀️&nbsp;o quizá sólo en nuestro bien.</p>
</section>
<section id="motivación-de-compras-con-llm" class="level1">
<h1>🛒&nbsp;Motivación: De compras con LLM</h1>
<p>El otro dia queria cocinar lasagna, pero no sabia si tenia los ingredientes en casa. En realidad, no queria gastar ningún ingrediente en casa, por lo que me metí a la website de mi supermercado favorito para comprar los ingredientes. El problema es que nunca he cocinado lasagna por lo que no sé que ingredientes lleva! me dió tanta flojera googlear una receta, extraer los ingredientes y ponerlos uno por uno en el carrito de compras que terminé comiendo cereales con leche 🥣.</p>
<p>Ahí fue cuando pensé que sería bueno que el supermercado tuviese integrado un algoritmo que agregue automáticamente los ingredientes de una comida específica en el carrito de compras! 🛍️</p>
<p>🧠&nbsp;<strong>Solución: Utilizar LLM para obtener recetas y extraer los ingredientes de forma estructurada.</strong></p>
<p>La verdad esto fue una excusa para comenzar a aprender a utilizar los LLM, el mundo se está moviendo en torno a esto y no pienso quedarme atrás.</p>
</section>
<section id="tool-path-que-utilizaremos" class="level1">
<h1>🔨&nbsp;Tool Path: Que utilizaremos</h1>
<p>A continuación les dejo las herramientas que utilizaremos en este post:</p>
<ol type="1">
<li><strong>ChatGPT API</strong>: Para poder acceder a los poderes de ChatGPT</li>
<li><strong>LangChain</strong>: Para poder comunicarme de manera fácil con la API de ChatGPT, además de aprovechar un montón de los facilitadores que tiene para construir herramientas basadas en LLM</li>
<li><strong>Python</strong>: Lenguaje de programación</li>
<li><strong>Gradio</strong>: Para crear una interfaz simple de uso</li>
</ol>
</section>
<section id="concept-path-que-aprenderemos" class="level1">
<h1>💭&nbsp;Concept Path: Que aprenderemos</h1>
<p>A continuación algunos de los conceptos que aprenderemos:</p>
<ol type="1">
<li>LLM: Ya hablamos un poco sobre esto</li>
<li>Prompt Engineering</li>
<li>Prompt Templates</li>
<li>Chains</li>
<li>Environments</li>
<li>Framework</li>
</ol>
<p>La verdad esto es un primer paso para aprender LLM, hay muchos conceptos más allá que aún queda por explorar, y lo peor es que esto sigue avanzando a pasos agigantados. 🦶🏽</p>
</section>
<section id="estrategia-como-abordamos" class="level1">
<h1>♟️&nbsp;Estrategia: Como abordamos</h1>
<p>La solución es bastante directa, pedirle a algún LLM que nos entregue los ingredientes de una comida especifica</p>
<section id="prompt" class="level2">
<h2 class="anchored" data-anchor-id="prompt">Prompt:</h2>
<p>Imaginemos queremos saber los ingredientes para cocinar una lasagna, entonces escribiremos algo del estilo:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
👩🏼‍🔬
</div>
</div>
<div class="callout-body-container callout-body">
<p>¿Que necesito para cocinar una Lasagna?</p>
</div>
</div>
<p>Esta pregunta, que es la entrada de un LLM se le denomina <code>prompt.</code> Un término bastante conocido hasta ahora, que de hecho se asocia a un rol, es <code>Prompt Engineering</code>. Podemos entender este término como el “arte” de escribir el mejor prompt para obtener la respuesta deseada.</p>
<p>“Cuida la forma en la que pides las cosas” me decía mi mamá cuando niño al pedirle un favor a alguien. Las mamás siempre tienen la razón, y esta no es la excepción.</p>
<p>Imaginemos que la respuesta de la LLM es algo como:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
🤖
</div>
</div>
<div class="callout-body-container callout-body">
<p>Para cocinar una lasagna necesitas un horno, un cuchillo, una cocina y los ingredientes.</p>
</div>
</div>
<p>No es la respuesta que esperabamos! nosotros en realidad queríamos saber los ingredientes, pero nos expresamos mal. Es por esto que comúnmente se suele iterar el prompt hasta conseguir la respuesta deseada, imaginemos que luego de iterar un poco llegamos al prompt final:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
👩🏼‍🔬
</div>
</div>
<div class="callout-body-container callout-body">
<p>¿Cuales son los ingredientes que necesito para cocinar una lasagna?</p>
</div>
</div>
<p>Probablemente con un prompt así obtengamos lo que buscamos. No hay un prompt óptimo, pero si existen muchos prompt que nos conseguirán la respuesta que buscamos.</p>
<p>Más adelante veremos algunas técnicas de prompt engineering. Por ahora nos basta con saber que el prompt será elemento importante de nuestra solución.</p>
</section>
<section id="output" class="level2">
<h2 class="anchored" data-anchor-id="output">Output:</h2>
<p>Algo que nos debemos cuestionar es: ¿Como necesitamos el resultado? algunas de las opciones son: Una lista de ingredientes, un json, un DataFrame, etc.</p>
<p>En este caso lo que decidí fue obtener un json, el cual luego convertiría a un DataFrame. Los elementos que tendrá la respuesta son:</p>
<ul>
<li><strong>Ingredient</strong>: Nombre del ingrediente</li>
<li><strong>Quantity</strong>: Cantidad necesaria</li>
<li><strong>Optional</strong>: “Yes” si el ingrediente es opcional, “No” si es obligatorio</li>
<li><strong>Estimated Price</strong>: Un precio estimado en dólares del ingrediente (así podremos calcular algún valor aproximado de la receta)</li>
<li><strong>Available</strong>: Una simulación de disponibilidad del ingrediente en el supermercado. “Yes” si está disponible, “No” si no lo está.</li>
</ul>
<p>Acá tenemos un ejemplo:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"Spaguetthi With Meat"</span><span class="fu">:</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="ot">[</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"ingredient"</span><span class="fu">:</span> <span class="st">"Spaguetti"</span><span class="fu">,</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"optional"</span><span class="fu">:</span> <span class="st">"No"</span><span class="fu">,</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"quantity"</span><span class="fu">:</span> <span class="st">"200g"</span><span class="fu">,</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"estimated_price"</span><span class="fu">:</span> <span class="st">"5.00"</span><span class="fu">,</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"available"</span><span class="fu">:</span> <span class="st">"No"</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"ingredient"</span><span class="fu">:</span> <span class="st">"Meat"</span><span class="fu">,</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"optional"</span><span class="fu">:</span> <span class="st">"No"</span><span class="fu">,</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"quantity"</span><span class="fu">:</span> <span class="st">"1kg"</span><span class="fu">,</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"estimated_price"</span><span class="fu">:</span> <span class="st">"10.00"</span><span class="fu">,</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"available"</span><span class="fu">:</span> <span class="st">"Yes"</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"ingredient"</span><span class="fu">:</span> <span class="st">"Pepper"</span><span class="fu">,</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"optional"</span><span class="fu">:</span> <span class="st">"Yes"</span><span class="fu">,</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"quantity"</span><span class="fu">:</span> <span class="st">"at ease"</span><span class="fu">,</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"estimated_price"</span><span class="fu">:</span> <span class="st">"1.00"</span><span class="fu">,</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"available"</span><span class="fu">:</span> <span class="st">"No"</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  <span class="ot">]</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Una pregunta importante es, ¿Cómo lograremos que el LLM nos estructure el output como lo requerimos?</p>
<p><strong>SPOILER: Prompt Engineering</strong></p>
</section>
</section>
<section id="tools" class="level1">
<h1>🔨&nbsp;Tools</h1>
<p>En este post no busco explicar a fondo las herramientas que utilizaré, si no que mostrar su uso. Para entender más sobre ellas te dejaré enlaces a sus propias documentaciones (probablemente mucho mejor explicado como lo haría yo).</p>
<p>A continuación mencionaré un poco sobre las herramientas que utilizaremos en las siguientes secciones:</p>
<ul>
<li><strong>Langchain</strong>: LangChain es un framework diseñado para simplificar la creación de aplicaciones utilizando modelos de lenguaje grandes (LLM). Como framework de integración del modelo de lenguaje, los casos de uso de LangChain se superponen en gran medida con los de los modelos de lenguaje en general, incluidos el análisis y resumen de documentos, los chatbots y el análisis de código.</li>
<li><strong>ChatGPT API</strong>: ChatGPT API es una interfaz de programación de aplicaciones (API) que permite a los desarrolladores interactuar con el modelo de lenguaje ChatGPT de OpenAI. Esta API permite enviar solicitudes a través de una conexión de red para obtener respuestas generadas por el modelo en tiempo real. Al utilizar la API, los desarrolladores pueden integrar fácilmente la funcionalidad de ChatGPT en sus propias aplicaciones, productos o servicios. Proporciona una manera conveniente de aprovechar la potencia de ChatGPT para tareas como conversación, generación de texto y respuestas a preguntas en una amplia gama de aplicaciones y escenarios.</li>
</ul>
</section>
<section id="prototyping" class="level1">
<h1>🧠&nbsp;Prototyping</h1>
<p>La forma final en la que plantearemos la solución será:</p>
<ol type="1">
<li>Usar un LLM que obtenga la receta según la comida</li>
<li>Usar un LLM que obtenga los ingredientes de la receta obtenida del paso 1</li>
<li>Crear la cadena final de LLM. Esto es, unir los resultados de los pasos 1 y 2.</li>
<li>Parsear los resultados. Esto es, estructurarlos.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>👀 Inicialmente habia pensado en que un LLM directamente me entregue los ingredientes desde una comida especificada, pero luego de unas cuantas iteraciones llegué a que de la forma en base a chains conseguía mejores resultados</p>
</div>
</div>
<section id="obtener-receta-mediante-llm" class="level2">
<h2 class="anchored" data-anchor-id="obtener-receta-mediante-llm">1. Obtener receta mediante LLM</h2>
<section id="prompt-templates" class="level3">
<h3 class="anchored" data-anchor-id="prompt-templates">Prompt Templates</h3>
<p>En este punto vale la pena preguntarnos, cómo esperamos que el usuario interactúe con nuestra aplicación? Queremos que el usuario haga la pregunta completa? Ahora sabemos que esto no es una buena idea por varias razones:</p>
<ol type="1">
<li>El usuario podria ingresar incluso preguntas que no estén relacionadas con la aplicación (comida)</li>
<li>El usuario puede preguntar de forma ineficiente</li>
<li>Obtener la estructura json que deseamos sería imposible</li>
<li>El usuario no sabe de <code>Prompt Engineering</code></li>
</ol>
<p>La idea es que el usuario sólo ingrese el nombre de la comida, y por detrás nuestra aplicación haga el resto. Para esto, Langchain cuenta con una herramienta llamada Prompt Templates, que como su nombre lo indica es una plantilla del prompt.</p>
<p>Esto es, imaginemos nuestra plantilla es: “¿Cuales son los ingredientes que necesito para cocinar {COMIDA}?” Entonces si la entrada del usuario es “Lasagna”, el prompt quedará “¿Cuales son los ingredientes que necesito para cocinar <em>Lasagna</em>?”.</p>
</section>
<section id="code-time" class="level3">
<h3 class="anchored" data-anchor-id="code-time">👨🏾‍💻&nbsp;Code time!</h3>
<p>Para crear un template, primero debemos definir la estructura:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>template_string <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="st">Give me a list of ingredients to cook </span><span class="sc">{food}</span><span class="st">.</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Notemos que el input en este caso es “food”. Luego usamos Langchain</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.prompts <span class="im">import</span> PromptTemplate</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>prompt_template <span class="op">=</span> PromptTemplate.from_template(template_string)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Luego simplemente obtenemos el prompt final para entregarle al modelo de la siguiente forma:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>user_input <span class="op">=</span> <span class="st">'Lasagna'</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>final_prompt <span class="op">=</span> prompt.<span class="bu">format</span>(food<span class="op">=</span>user_input)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Así de simple! Ahora quiero que nos compliquemos un poco más la vida. ChatGPT hizo unos cambios en su API, por lo que en Langchain ahora aparece un nuevo elemento llamado <code>ChatPromptTemplate</code></p>
<p>La idea de este template, que si bien también acepta entradas como las vistas anteriormente, ahora permite ingresar mensajes con <code>roles</code>. Existen tres tipos de roles: System, Human, AI. Bien brevemente te explico que deberian ser:</p>
<ul>
<li>System Message: Las instrucciones que se le quiere entregar al modelo</li>
<li>Human Message: Las entradas del usuario</li>
<li>AI Message: Alguna respuesta por parte de el modelo</li>
</ul>
<p>Acá te dejo un post del por qué de esta implementación, que viene de la mano con <code>ChatModels</code>: <a href="https://blog.langchain.dev/chat-models/">https://blog.langchain.dev/chat-models/</a></p>
<p>Debido a que sólo el paso 1 utiliza input de usuario, sólo lo haremos así en este paso:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.prompts <span class="im">import</span> (</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    ChatPromptTemplate,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    PromptTemplate,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    SystemMessagePromptTemplate,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    AIMessagePromptTemplate,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    HumanMessagePromptTemplate,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;System template</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>first_system_template_str <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="st">    You are a good chef, users need you to bring them recipes from given food.</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>first_system_template <span class="op">=</span> SystemMessagePromptTemplate.from_template(first_system_template_str)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Human Template</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>first_human_template_str <span class="op">=</span> <span class="st">"</span><span class="sc">{food}</span><span class="st">"</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>first_human_template <span class="op">=</span> HumanMessagePromptTemplate.from_template(first_human_template_str)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># First Prompt Template</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>first_prompt <span class="op">=</span> ChatPromptTemplate.from_messages([first_system_template, first_human_template])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="llm-model" class="level3">
<h3 class="anchored" data-anchor-id="llm-model">LLM Model</h3>
<p>Ahora que ya tenemos una entrada para nuestro modelo, necesitamos llamarlo!</p>
<p>Para poder hacer uso de un modelo de LLM (En este caso ChatOpenAI) primero debemos crearnos una cuenta en <a href="https://openai.com/blog/openai-api">openai</a> y crear una nueva API Key. Luego, debemos crear un archivo <code>.env</code> en la raíz del proyecto con el siguiente contenido:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">OPENAI_API_KEY</span> = <span class="st">"&lt;tu api key&gt;"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>No debes dejar que nadie vea tu API KEY, asi que no subas tu .env a ningún lugar público!</p>
</div>
</div>
<p>Si no, puedes agregarlo directamente utilizando la libreria openai o Langchain.Luego de obtener y entregar tu API KEY, en Langchain basta con hacer algo como:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chat_models <span class="im">import</span> ChatOpenAI</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>chat <span class="op">=</span> ChatOpenAI(temperature<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Porfavor hazme la tesis!"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> chat(prompt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>En este caso, estaremos utilizando dos LLM, en donde la salida de una es la entrada de otra. Langchain ya tiene algo preparado para esto! se les llama Chains. En este caso, tenemos una cadena simple (Dos LLM secuenciales), pero existen otros tipos de arquitecturas mucho más complejas ☠️</p>
<p>Para poder crear nuestra primera Chain no es tanto más complejo que el ejemplo anterior, sólo debemos agregar unos pasos extras:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chains <span class="im">import</span> LLMChain</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chat_models <span class="im">import</span> ChatOpenAI</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># LLM Definition</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> ChatOpenAI(temperature<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Chain Step 1</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>recipe_chain <span class="op">=</span> LLMChain(llm<span class="op">=</span>llm, prompt<span class="op">=</span>first_prompt, output_key<span class="op">=</span><span class="st">"recipe"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>En el código anterior estamos utilizando la LLM ChatOpenAI, y creando la primera parte de la cadena. El parámetro <code>output_key</code> nos servirá más adelante para comunicarle a la segunda parte cual es el nombre del primer output.</p>
</section>
</section>
<section id="obtener-ingredientes-de-una-receta-mediante-llm" class="level2">
<h2 class="anchored" data-anchor-id="obtener-ingredientes-de-una-receta-mediante-llm">2. Obtener ingredientes de una receta mediante LLM</h2>
<p>Ya sabemos que la salida del paso anterior será una receta, a la cual deberemos extraer los ingredientes. Además, éste es el último paso, por lo que deberemos preocuparnos de que el output que salga del LLM sea adecuado y simple de “parsear”.</p>
<p>Para el output, lo que haremos será aplicar un poco del ya conocido <code>prompt engineering</code>.</p>
<p>Además, en este caso, ya que no existen tantos riesgos de <code>prompt injection</code> , es que usaremos templates sencillos (no haremos uso de los roles en los mensajes)</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>second_step_template <span class="op">=</span> <span class="st">"""I need you to bring me the ingredients contained in the following recipe: </span><span class="ch">\</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="st">recipe: </span><span class="sc">{recipe}</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="sc">{format_instructions}</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="sc">{example_instructions}</span><span class="st">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Vemos que tenemos tres entradas:</p>
<ul>
<li>recipe: Este input será el resultado del paso anterior.</li>
<li>format_instructions: Acá le comunicaremos al LLM como queremos el formato del output.</li>
<li>example_instructions: Acá aplicamos un poco de lo que se llama <code>few-shot</code> , que es básicamente darle un par de ejemplos al LLM para que entienda como esperamos el resultado.</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Format Instructions</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>custom_format_instructions <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="st">The output should be in a json format, formatted in the following schema:</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="st">{</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="st">  "food": List // List of ingredients</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="st">  [</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="st">    {</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="st">      "ingredient": string // Name of one ingredient</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="st">      "quantity": string  // Quantity of the ingredient </span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="st">      "optional": string  // Whether or not that ingredient is optional to cook the food. "Yes" if the ingredient is not indispensable to cook, "No" if is the ingredient is indispensable.</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="st">      "estimated_price": string  // The ingredient's estimated price in dolars</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="st">      "available": string // Random "Yes" or "No"</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="st">  ]</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>    example_instructions <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="st">Follow the schema of this example:</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="st">{</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="st">  "food":</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="st">  [</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="st">    {</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="st">      "ingredient": "Spaguetti",</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="st">      "optional": "No",</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="st">      "quantity": "200g",</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="st">      "estimated_price": "5.00",</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="st">      "available": "No"</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="st">    },</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="st">    {</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="st">      "ingredient": "Meat",</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="st">      "optional": "No",</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="st">      "quantity": "1kg",</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="st">      "estimated_price": "10.00",</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="st">      "available": "Yes"</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="st">    },</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="st">    {</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="st">      "ingredient": "Pepper",</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="st">      "optional": "Yes",</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="st">      "quantity": "at ease",</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="st">      "estimated_price": "1.00",</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="st">      "available": "No"</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="st">  ]</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>✋🏽 Es importante que sepas que langchain cuenta con un módulo de Output parser que crea por detrás el format_instructions e incluso cuenta con funciones que transforman la salida del LLM(string) en el formato que deseabamos (json, lista, Pydantic, etc). La razón de porqué yo no ocupé esto fue que no funciona para json nesteados. Pero me basé en sus instrucciones para crear <code>custom_format_instructions</code></p>
</div>
</div>
<p>Luego seguimos como lo vimos anteriormente:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>second_prompt <span class="op">=</span> ChatPromptTemplate.from_template(second_step_template)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>ingredient_chain <span class="op">=</span> LLMChain(llm<span class="op">=</span>llm, prompt<span class="op">=</span>second_prompt, output_key<span class="op">=</span><span class="st">"ingredients"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="cadena-final" class="level2">
<h2 class="anchored" data-anchor-id="cadena-final">3. Cadena Final</h2>
<p>Luego necesitamos orquestar las cadenas que creamos anteriormente para obtener la cadena final. En Langchain se hace de la siguiente forma:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>overall_simple_chain <span class="op">=</span> SequentialChain(chains<span class="op">=</span>[recipe_chain, ingredient_chain], verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>                                       input_variables<span class="op">=</span>[<span class="st">"food"</span>, <span class="st">"format_instructions"</span>, <span class="st">"example_instructions"</span>],</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                                       output_variables<span class="op">=</span>[<span class="st">"recipe"</span>, <span class="st">"ingredients"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="output-parser" class="level2">
<h2 class="anchored" data-anchor-id="output-parser">4. Output Parser</h2>
<p>Finalmente, necesitamos parsear el resultado obtenido desde la LLM. Esto es <strong><em>string → json.</em></strong></p>
<p>Debido a la forma en que le pedimos el resultado al LLM, es que se nos hace muy sencillo:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> overall_simple_chain(</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">"food"</span>: food, <span class="co"># Esto lo entrega el usuario</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"format_instructions"</span>: custom_format_instructions, <span class="co"># Esto lo entregamos nosotros</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"example_instructions"</span>: example_instructions, <span class="co"># Esto lo entregamos nosotros</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co"># String to json(dict en python)</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>dict_response <span class="op">=</span> json.loads(result[<span class="st">'ingredients'</span>]) </span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Dict to df</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>output_df <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> dict_response[<span class="st">'food'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Finalmente, para la entrada “Lasagna”, podemos obtener algo asi:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM Recipe ee36a92d74c24a5d986142d48f3b51c8/Untitled.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Output 1 as DataFrame</figcaption>
</figure>
</div>
</section>
</section>
<section id="front-end" class="level1">
<h1>🧐&nbsp;Front-End</h1>
<p>Nuestra aplicación no puede quedarse sólo en código, es por esto que creamos un pequeño front-end para los usuarios. A continuación te dejo una imagen, pero lo mejor es que corras el código por ti mismo y los pruebes! Te invito a mejorar el algoritmo!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM Recipe ee36a92d74c24a5d986142d48f3b51c8/Untitled 1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Input and Output 0</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM Recipe ee36a92d74c24a5d986142d48f3b51c8/Untitled 2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Output 1</figcaption>
</figure>
</div>
<p>Notar que en el front-end se le entrega al usuario tanto la receta como el resumen de los ingredientes. Algo interesante de gradio es el botón <code>Avisar</code>. Este botón sirve para recibir feedback de los usuarios en caso de que el resultado no haya sido satisfactorio, y así puedes mejorar tu producto.</p>
</section>
<section id="próximos-pasos" class="level1">
<h1>🚀&nbsp;Próximos Pasos</h1>
<p>Esto es sólo una solución inicial, podemos mejorarla de muchas formas, algunas que se me ocurren por ahora son:</p>
<ul>
<li>Prompt Optimization: Mejorar más los prompt. Quizá agregarle el hecho de que una persona puede ingresar algo que no es una comida. Podemos simplemente decirle “Si piensas que no es una comida, entrega este resultado..”</li>
<li>Fine-Tuning: Podriamos entrenar un LLM con datos de recetarios</li>
<li>Document: Podriamos hacer que el LLM considere un libro de recetas en particular a la hora de crear su respuesta. Quizá también agregarle productos propios del supermercado para incentivar su compra. Esto es desafiante debido a que un documento tiene muchos caracteres y los LLM tienen un limite llamado <code>context_length</code>. Afortunadamente ya existen metodologías para sobrellevar esto.</li>
</ul>
</section>
<section id="conclusión" class="level1">
<h1>🥳&nbsp;Conclusión</h1>
<p>En este blog exploramos el mundo de los Large Language Models (LLM) y su aplicación en la vida cotidiana. El LLM más conocido, ChatGPT, es un modelo matemático capaz de comprender y generar lenguaje humano de manera avanzada. Descubrimos cómo utilizar los LLM para obtener recetas y extraer los ingredientes de forma estructurada, utilizando técnicas de Prompt Engineering.</p>
<p>Mediante el uso de herramientas como LangChain, Python y Gradio, pudimos construir una solución que permite al usuario ingresar el nombre de una comida y obtener automáticamente los ingredientes necesarios para cocinarla. Utilizando Prompt Templates y Chains, logramos interactuar con el modelo de manera eficiente y obtener respuestas precisas.</p>
<p>Aunque esta solución es solo un primer paso en el mundo de los LLM, demuestra el potencial de estos modelos para simplificar tareas y mejorar nuestra vida diaria. A medida que la tecnología avance, seguiremos explorando nuevas aplicaciones y técnicas para aprovechar al máximo los Large Language Models en beneficio de la humanidad.</p>
<p>Recuerda que en github podrás encontrar el código utilizado en este proyecto.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>