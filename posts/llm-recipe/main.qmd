---
title: "Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT"
date: "2023-07-05"
image: "llm_recipe2.png"
author: "Diegulio"
categories: [python, llm, chatgpt, langchain, gradio]
subtitle: "Cocina tu comida favorita con la ayuda de LLM"
---

# LLM Recipe

[![](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/diegulio/llm-recipe)


::: {.callout-note icon=false}
Antes de comenzar a leer esto, recuerda que yo estoy aprendiendo junto contigo. Si tienes alguna duda, sugerencia, correci√≥n o comentario, no dudes en escribirme a mi [LinkedIn](https://www.linkedin.com/in/dieguliomachado/).
:::


# üìö¬†T√≥pico: Large Language Models

Estoy seguro que alguna vez haz escuchado el t√©rmino ChatGPT. Yep! ese robot ü§ñ¬†que te hace hasta la tesis!  ChatGPT es un LLM (Large Language Model), lo que en otras palabras significa que es un modelo matem√°tico que se alimenta de grandes volumenes de texto y es capaz de generar lenguaje humano de manera muuy avanzada. Y si que lo hemos visto en acci√≥n, yo lo ocupo en mi dia a dia y me facilita un mont√≥n mi trabajo.

Ahora ¬ø Que tal si dejamos que un LLM nos describa que es un LLM? 

::: {.callout-note icon=false}
## ü§ñ
Un large language model (modelo de lenguaje amplio) es un tipo de sistema de inteligencia artificial dise√±ado para comprender y generar lenguaje humano de manera avanzada. Estos modelos est√°n entrenados en grandes cantidades de texto y utilizan t√©cnicas de aprendizaje autom√°tico para aprender patrones y estructuras ling√º√≠sticas. Un large language model, como GPT-3.5, puede responder preguntas, redactar textos, generar c√≥digo, traducir idiomas y realizar una variedad de tareas relacionadas con el lenguaje natural. Estos modelos son capaces de capturar la complejidad y sutileza del lenguaje humano, y pueden adaptarse a diferentes contextos y estilos de escritura.

:::

En este post **no** hablaremos de las t√©cnicas utilizadas para construir un LLM, ya que en internet hay muy buenas fuentes para aprender sobre esto, ac√° te dejo una: 

[LLM University (LLMU) | Cohere](https://docs.cohere.com/docs/llmu)

Lo que haremos en este post, ser√° utilizar los LLM en el bien de la humanidad ! ü¶∏üèΩ‚Äç‚ôÄÔ∏è¬†o quiz√° s√≥lo en nuestro bien. 

# üõí¬†Motivaci√≥n: De compras con LLM

El otro dia queria cocinar lasagna, pero no sabia si tenia los ingredientes en casa. En realidad, no queria gastar ning√∫n ingrediente en casa, por lo que me met√≠ a la website de mi supermercado favorito para comprar los ingredientes. El problema es que nunca he cocinado lasagna por lo que no s√© que ingredientes lleva! me di√≥ tanta flojera googlear una receta, extraer los ingredientes y ponerlos uno por uno en el carrito de compras que termin√© comiendo cereales con leche ü•£. 

Ah√≠ fue cuando pens√© que ser√≠a bueno que el supermercado tuviese integrado un algoritmo que agregue autom√°ticamente los ingredientes de una comida espec√≠fica en el carrito de compras! üõçÔ∏è

üß†¬†**Soluci√≥n: Utilizar LLM para obtener recetas y extraer los ingredientes de forma estructurada.** 

La verdad esto fue una excusa para comenzar a aprender a utilizar los LLM, el mundo se est√° moviendo en torno a esto y no pienso quedarme atr√°s.

# üî®¬†Tool Path: Que utilizaremos

A continuaci√≥n les dejo las herramientas que utilizaremos en este post:

1. **ChatGPT API**: Para poder acceder a los poderes de ChatGPT
2. **LangChain**: Para poder comunicarme de manera f√°cil con la API de ChatGPT, adem√°s de aprovechar un mont√≥n de los facilitadores que tiene para construir herramientas basadas en LLM
3. **Python**: Lenguaje de programaci√≥n
4. **Gradio**: Para crear una interfaz simple de uso 

# üí≠¬†Concept Path: Que aprenderemos

A continuaci√≥n algunos de los conceptos que aprenderemos: 

1. LLM: Ya hablamos un poco sobre esto
2. Prompt Engineering
3. Prompt Templates
4. Chains
5. Environments
6. Framework

La verdad esto es un primer paso para aprender LLM, hay muchos conceptos m√°s all√° que a√∫n queda por explorar, y lo peor es que esto sigue avanzando a pasos agigantados. ü¶∂üèΩ

# ‚ôüÔ∏è¬†Estrategia: Como abordamos

La soluci√≥n es bastante directa, pedirle a alg√∫n LLM que nos entregue los ingredientes de una comida especifica

## Prompt:

Imaginemos queremos saber los ingredientes para cocinar una lasagna, entonces escribiremos algo del estilo:

::: {.callout-note icon=false}
## üë©üèº‚Äçüî¨ 
¬øQue necesito para cocinar una Lasagna?
:::

Esta pregunta, que es la entrada de un LLM se le denomina `prompt.` Un t√©rmino bastante conocido hasta ahora, que de hecho se asocia a un rol, es `Prompt Engineering`. Podemos entender este t√©rmino como el ‚Äúarte‚Äù de escribir el mejor prompt para obtener la respuesta deseada. 

‚ÄúCuida la forma en la que pides las cosas‚Äù me dec√≠a mi mam√° cuando ni√±o al pedirle un favor a alguien. Las mam√°s siempre tienen la raz√≥n, y esta no es la excepci√≥n.  

Imaginemos que la respuesta de la LLM es algo como:

::: {.callout-note icon=false}
## ü§ñ 
Para cocinar una lasagna necesitas un horno, un cuchillo, una cocina y los ingredientes.

:::

No es la respuesta que esperabamos! nosotros en realidad quer√≠amos saber los ingredientes, pero nos expresamos mal. Es por esto que com√∫nmente se suele iterar el prompt hasta conseguir la respuesta deseada, imaginemos que luego de iterar un poco llegamos al prompt final:

::: {.callout-note icon=false}
## üë©üèº‚Äçüî¨ 
¬øCuales son los ingredientes que necesito para cocinar una lasagna?

:::

Probablemente con un prompt as√≠ obtengamos lo que buscamos. No hay un prompt √≥ptimo, pero si existen muchos prompt que nos conseguir√°n la respuesta que buscamos.

M√°s adelante veremos algunas t√©cnicas de prompt engineering. Por ahora nos basta con saber que el prompt ser√° elemento importante de nuestra soluci√≥n.

## Output:

Algo que nos debemos cuestionar es: ¬øComo necesitamos el resultado? algunas de las opciones son: Una lista de ingredientes, un json, un DataFrame, etc. 

En este caso lo que decid√≠ fue obtener un json, el cual luego convertir√≠a a un DataFrame. Los elementos que tendr√° la respuesta son:

- **Ingredient**: Nombre del ingrediente
- **Quantity**: Cantidad necesaria
- **Optional**: ‚ÄúYes‚Äù si el ingrediente es opcional, ‚ÄúNo‚Äù si es obligatorio
- **Estimated Price**: Un precio estimado en d√≥lares del ingrediente (as√≠ podremos calcular alg√∫n valor aproximado de la receta)
- **Available**: Una simulaci√≥n de disponibilidad del ingrediente en el supermercado. ‚ÄúYes‚Äù si est√° disponible, ‚ÄúNo‚Äù si no lo est√°.

Ac√° tenemos un ejemplo:

```json
{
  "Spaguetthi With Meat":
  [
    {
      "ingredient": "Spaguetti",
      "optional": "No",
      "quantity": "200g",
      "estimated_price": "5.00",
      "available": "No"
    },
    {
      "ingredient": "Meat",
      "optional": "No",
      "quantity": "1kg",
      "estimated_price": "10.00",
      "available": "Yes"
    },
    {
      "ingredient": "Pepper",
      "optional": "Yes",
      "quantity": "at ease",
      "estimated_price": "1.00",
      "available": "No"
    }

  ]
}
```

Una pregunta importante es, ¬øC√≥mo lograremos que el LLM nos estructure el output como lo requerimos?

**SPOILER: Prompt Engineering**

# üî®¬†Tools

En este post no busco explicar a fondo las herramientas que utilizar√©, si no que mostrar su uso. Para entender m√°s sobre ellas te dejar√© enlaces a sus propias documentaciones (probablemente mucho mejor explicado como lo har√≠a yo).

A continuaci√≥n mencionar√© un poco sobre las herramientas que utilizaremos en las siguientes secciones:

- **Langchain**: LangChain es un framework dise√±ado para simplificar la creaci√≥n de aplicaciones utilizando modelos de lenguaje grandes (LLM). Como framework de integraci√≥n del modelo de lenguaje, los casos de uso de LangChain se superponen en gran medida con los de los modelos de lenguaje en general, incluidos el an√°lisis y resumen de documentos, los chatbots y el an√°lisis de c√≥digo.
- **ChatGPT API**: ChatGPT API es una interfaz de programaci√≥n de aplicaciones (API) que permite a los desarrolladores interactuar con el modelo de lenguaje ChatGPT de OpenAI. Esta API permite enviar solicitudes a trav√©s de una conexi√≥n de red para obtener respuestas generadas por el modelo en tiempo real. Al utilizar la API, los desarrolladores pueden integrar f√°cilmente la funcionalidad de ChatGPT en sus propias aplicaciones, productos o servicios. Proporciona una manera conveniente de aprovechar la potencia de ChatGPT para tareas como conversaci√≥n, generaci√≥n de texto y respuestas a preguntas en una amplia gama de aplicaciones y escenarios.

# üß†¬†Prototyping

La forma final en la que plantearemos la soluci√≥n ser√°:

1. Usar un LLM que obtenga la receta seg√∫n la comida
2. Usar un LLM que obtenga los ingredientes de la receta obtenida del paso 1
3. Crear la cadena final de LLM. Esto es, unir los resultados de los pasos 1 y 2.
4. Parsear los resultados. Esto es, estructurarlos.

::: {.callout-note}
üëÄ Inicialmente habia pensado en que un LLM directamente me entregue los ingredientes desde una comida especificada, pero luego de unas cuantas iteraciones llegu√© a que de la forma en base a chains consegu√≠a mejores resultados

:::

## 1. Obtener receta mediante LLM

### Prompt Templates

En este punto vale la pena preguntarnos, c√≥mo esperamos que el usuario interact√∫e con nuestra aplicaci√≥n? Queremos que el usuario haga la pregunta completa? Ahora sabemos que esto no es una buena idea por varias razones:

1. El usuario podria ingresar incluso preguntas que no est√©n relacionadas con la aplicaci√≥n (comida)
2. El usuario puede preguntar de forma ineficiente
3. Obtener la estructura json que deseamos ser√≠a imposible
4. El usuario no sabe de `Prompt Engineering`

La idea es que el usuario s√≥lo ingrese el nombre de la comida, y por detr√°s nuestra aplicaci√≥n haga el resto. Para esto, Langchain cuenta con una herramienta llamada Prompt Templates, que como su nombre lo indica es una plantilla del prompt. 

Esto es, imaginemos nuestra plantilla es: ‚Äú¬øCuales son los ingredientes que necesito para cocinar {COMIDA}?‚Äù Entonces si la entrada del usuario es ‚ÄúLasagna‚Äù, el prompt quedar√°  ‚Äú¬øCuales son los ingredientes que necesito para cocinar *Lasagna*?‚Äù. 

### üë®üèæ‚Äçüíª¬†Code time!

Para crear un template, primero debemos definir la estructura:

```python
template_string = """
Give me a list of ingredients to cook {food}.
"""
```

Notemos que el input en este caso es ‚Äúfood‚Äù. Luego usamos Langchain

```python
from langchain.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(template_string)
```

Luego simplemente obtenemos el prompt final para entregarle al modelo de la siguiente forma:

```python
user_input = 'Lasagna'
final_prompt = prompt.format(food=user_input)
```

As√≠ de simple! Ahora quiero que nos compliquemos un poco m√°s la vida. ChatGPT hizo unos cambios en su API, por lo que en Langchain ahora aparece un nuevo elemento llamado `ChatPromptTemplate`

La idea de este template, que si bien tambi√©n acepta entradas como las vistas anteriormente, ahora permite ingresar mensajes con `roles`. Existen tres tipos de roles: System, Human, AI. Bien brevemente te explico que deberian ser:

- System Message: Las instrucciones que se le quiere entregar al modelo
- Human Message: Las entradas del usuario
- AI Message: Alguna respuesta por parte de el modelo

Ac√° te dejo un post del por qu√© de esta implementaci√≥n, que viene de la mano con `ChatModels`: [https://blog.langchain.dev/chat-models/](https://blog.langchain.dev/chat-models/)

Debido a que s√≥lo el paso 1 utiliza input de usuario, s√≥lo lo haremos as√≠ en este paso:

```python
from langchain.prompts import (
    ChatPromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

#¬†System template
first_system_template_str = """
    You are a good chef, users need you to bring them recipes from given food.
"""
first_system_template = SystemMessagePromptTemplate.from_template(first_system_template_str)

# Human Template
first_human_template_str = "{food}"
first_human_template = HumanMessagePromptTemplate.from_template(first_human_template_str)

# First Prompt Template
first_prompt = ChatPromptTemplate.from_messages([first_system_template, first_human_template])
```

### LLM Model


Ahora que ya tenemos una entrada para nuestro modelo, necesitamos llamarlo!

Para poder hacer uso de un modelo de LLM (En este caso ChatOpenAI) primero debemos crearnos una cuenta en [openai](https://openai.com/blog/openai-api) y crear una nueva API Key. Luego, debemos crear un archivo `.env` en la ra√≠z del proyecto con el siguiente contenido:

```bash
OPENAI_API_KEY = "<tu api key>"
```

::: {.callout-warning}
No debes dejar que nadie vea tu API KEY, asi que no subas tu .env a ning√∫n lugar p√∫blico!
:::

Si no, puedes agregarlo directamente utilizando la libreria openai o Langchain.Luego de obtener y entregar tu API KEY, en Langchain basta con hacer algo como:

```python
from langchain.chat_models import ChatOpenAI

chat = ChatOpenAI(temperature=0.0)
prompt = "Porfavor hazme la tesis!"

result = chat(prompt)
```

En este caso, estaremos utilizando dos LLM, en donde la salida de una es la entrada de otra. Langchain ya tiene algo preparado para esto! se les llama Chains. En este caso, tenemos una cadena simple (Dos LLM secuenciales), pero existen otros tipos de arquitecturas mucho m√°s complejas ‚ò†Ô∏è

Para poder crear nuestra primera Chain no es tanto m√°s complejo que el ejemplo anterior, s√≥lo debemos agregar unos pasos extras:

```python
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI

# LLM Definition
llm = ChatOpenAI(temperature=0)

# Chain Step 1
recipe_chain = LLMChain(llm=llm, prompt=first_prompt, output_key="recipe")

```

En el c√≥digo anterior estamos utilizando la LLM ChatOpenAI, y creando la primera parte de la cadena. El par√°metro `output_key` nos servir√° m√°s adelante para comunicarle a la segunda parte cual es el nombre del primer output. 

## 2. Obtener ingredientes de una receta mediante LLM

Ya sabemos que la salida del paso anterior ser√° una receta, a la cual deberemos extraer los ingredientes. Adem√°s, √©ste es el √∫ltimo paso, por lo que deberemos preocuparnos de que el output que salga del LLM sea adecuado y simple de  ‚Äúparsear‚Äù.

Para el output, lo que haremos ser√° aplicar un poco del ya conocido `prompt engineering`. 

Adem√°s, en este caso, ya que no existen tantos riesgos de `prompt injection` , es que usaremos templates sencillos (no haremos uso de los roles en los mensajes)

 

```python
second_step_template = """I need you to bring me the ingredients contained in the following recipe: \
recipe: {recipe}
{format_instructions}
{example_instructions}"""
```

Vemos que tenemos tres entradas:

- recipe: Este input ser√° el resultado del paso anterior.
- format_instructions: Ac√° le comunicaremos al LLM como queremos el formato del output.
- example_instructions: Ac√° aplicamos un poco de lo que se llama `few-shot` , que es b√°sicamente darle un par de ejemplos al LLM para que entienda como esperamos el resultado.

```python
# Format Instructions
custom_format_instructions = """
The output should be in a json format, formatted in the following schema:
{
  "food": List // List of ingredients
  [
    {
      "ingredient": string // Name of one ingredient
      "quantity": string  // Quantity of the ingredient 
      "optional": string  // Whether or not that ingredient is optional to cook the food. "Yes" if the ingredient is not indispensable to cook, "No" if is the ingredient is indispensable.
      "estimated_price": string  // The ingredient's estimated price in dolars
      "available": string // Random "Yes" or "No"
    }
  ]
}
"""
```

```python
	example_instructions = """
Follow the schema of this example:
{
  "food":
  [
    {
      "ingredient": "Spaguetti",
      "optional": "No",
      "quantity": "200g",
      "estimated_price": "5.00",
      "available": "No"
    },
    {
      "ingredient": "Meat",
      "optional": "No",
      "quantity": "1kg",
      "estimated_price": "10.00",
      "available": "Yes"
    },
    {
      "ingredient": "Pepper",
      "optional": "Yes",
      "quantity": "at ease",
      "estimated_price": "1.00",
      "available": "No"
    }

  ]
}
"""
```

::: {.callout-warning}
‚úãüèΩ Es importante que sepas que langchain cuenta con un m√≥dulo de Output parser que crea por detr√°s el format_instructions e incluso cuenta con funciones que transforman la salida del LLM(string) en el formato que deseabamos (json, lista, Pydantic, etc).
La raz√≥n de porqu√© yo no ocup√© esto fue que no funciona para json nesteados. Pero me bas√© en sus instrucciones para crear `custom_format_instructions`

:::

Luego seguimos como lo vimos anteriormente:

```python
second_prompt = ChatPromptTemplate.from_template(second_step_template)
ingredient_chain = LLMChain(llm=llm, prompt=second_prompt, output_key="ingredients")
```

## 3. Cadena Final

Luego necesitamos orquestar las cadenas que creamos anteriormente para obtener la cadena final. En Langchain se hace de la siguiente forma:

```python
overall_simple_chain = SequentialChain(chains=[recipe_chain, ingredient_chain], verbose=True,
                                       input_variables=["food", "format_instructions", "example_instructions"],
                                       output_variables=["recipe", "ingredients"])

```

## 4. Output Parser

Finalmente, necesitamos parsear el resultado obtenido desde la LLM. Esto es ***string ‚Üí json.***

Debido a la forma en que le pedimos el resultado al LLM, es que se nos hace muy sencillo:

```python
import json
import pandas as pd

result = overall_simple_chain(
        {
            "food": food, # Esto lo entrega el usuario
            "format_instructions": custom_format_instructions, # Esto lo entregamos nosotros
            "example_instructions": example_instructions, # Esto lo entregamos nosotros
        }
    )

# String to json(dict en python)
dict_response = json.loads(result['ingredients']) 

# Dict to df
output_df = pd.DataFrame(data = dict_response['food'])

```

Finalmente, para la entrada  ‚ÄúLasagna‚Äù, podemos obtener algo asi:

![Output 1 as DataFrame](LLM%20Recipe%20ee36a92d74c24a5d986142d48f3b51c8/Untitled.png)

# üßê¬†Front-End

Nuestra aplicaci√≥n no puede quedarse s√≥lo en c√≥digo, es por esto que creamos un peque√±o front-end para los usuarios. A continuaci√≥n te dejo una imagen, pero lo mejor es que corras el c√≥digo por ti mismo y los pruebes! Te invito a mejorar el algoritmo! 

![Input and Output 0](LLM%20Recipe%20ee36a92d74c24a5d986142d48f3b51c8/Untitled%201.png)

![Output 1](LLM%20Recipe%20ee36a92d74c24a5d986142d48f3b51c8/Untitled%202.png)

Notar que en el front-end se le entrega al usuario tanto la receta como el resumen de los ingredientes. Algo interesante de gradio es el bot√≥n `Avisar`. Este bot√≥n sirve para recibir feedback de los usuarios en caso de que el resultado no haya sido satisfactorio, y as√≠ puedes mejorar tu producto.

# üöÄ¬†Pr√≥ximos Pasos

Esto es s√≥lo una soluci√≥n inicial, podemos mejorarla de muchas formas, algunas que se me ocurren por ahora son:

- Prompt Optimization: Mejorar m√°s los prompt. Quiz√° agregarle el hecho de que una persona puede ingresar algo que no es una comida. Podemos simplemente decirle ‚ÄúSi piensas que no es una comida, entrega este resultado..‚Äù
- Fine-Tuning: Podriamos entrenar un LLM con datos de recetarios
- Document: Podriamos hacer que el LLM considere un libro de recetas en particular a la hora de crear su respuesta. Quiz√° tambi√©n agregarle productos propios del supermercado para incentivar su compra. Esto es desafiante debido a que un documento tiene muchos caracteres y los LLM tienen un limite llamado `context_length`. Afortunadamente ya existen metodolog√≠as para sobrellevar esto.

# ü•≥¬†Conclusi√≥n

En este blog exploramos el mundo de los Large Language Models (LLM) y su aplicaci√≥n en la vida cotidiana. El LLM m√°s conocido, ChatGPT, es un modelo matem√°tico capaz de comprender y generar lenguaje humano de manera avanzada. Descubrimos c√≥mo utilizar los LLM para obtener recetas y extraer los ingredientes de forma estructurada, utilizando t√©cnicas de Prompt Engineering.

Mediante el uso de herramientas como LangChain, Python y Gradio, pudimos construir una soluci√≥n que permite al usuario ingresar el nombre de una comida y obtener autom√°ticamente los ingredientes necesarios para cocinarla. Utilizando Prompt Templates y Chains, logramos interactuar con el modelo de manera eficiente y obtener respuestas precisas.

Aunque esta soluci√≥n es solo un primer paso en el mundo de los LLM, demuestra el potencial de estos modelos para simplificar tareas y mejorar nuestra vida diaria. A medida que la tecnolog√≠a avance, seguiremos explorando nuevas aplicaciones y t√©cnicas para aprovechar al m√°ximo los Large Language Models en beneficio de la humanidad.

Recuerda que en github podr√°s encontrar el c√≥digo utilizado en este proyecto.