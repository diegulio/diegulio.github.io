{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44752558-7ddd-496e-965a-7ead4eeea12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PROJECT_ID = \"tc-sc-bi-bigdata-corp-tsod-dev\" \n",
    "BUCKET = \"gs://enrollment_verification_pipeline\"\n",
    "REGION = \"us-central1\"\n",
    "PIPELINE_ROOT = f\"{BUCKET}/pipeline_root/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae3f7a7-29e4-4aee-9c93-1481a757921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kfp\n",
    "#!pip install google-cloud-aiplatform\n",
    "#!pip install google-cloud\n",
    "#!pip install google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e036173d-1767-4b46-be26-d981cca1670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "from typing import NamedTuple, List, Dict, Text\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from datetime import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dffc4d1-5183-463c-8bd4-825450a389b1",
   "metadata": {},
   "source": [
    "# Get Data to pred\n",
    "Descargamos el mantenedor, obtenemos la sheet del formulario y de revisión. Obtenemos un df con las filas del formulario que no esten revisadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fb80ce-cea6-4e55-8b22-f170bbff909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero un componente solo para obtener las id, esto porque vertex ai tiraba\n",
    "# error cuando queria retornar string y tambien datasets\n",
    "\n",
    "# Se puede mejorar haciendo que el componente get_data reciba estos ids y asi \n",
    "# no se replica tarea\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"google-cloud\",\n",
    "        \"google-cloud-secret-manager\",\n",
    "        \"google-api-python-client\",\n",
    "        \"gcsfs\",\n",
    "        \"gspread\",\n",
    "        \"oauth2client\",\n",
    "    ],\n",
    "    output_component_file=\"get_ids.yaml\"\n",
    ")\n",
    "def get_ids(\n",
    "    project_id: str,\n",
    "    id_mantenedor: str,\n",
    "    drive_secret_id: str,\n",
    ")-> NamedTuple(\"outputs\", [(\"id_revision\", str), (\"id_formulario\", str)]):\n",
    "    import io\n",
    "    from googleapiclient.errors import HttpError\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "    import pandas as pd\n",
    "    import gspread\n",
    "    from google.oauth2.credentials import Credentials\n",
    "    from googleapiclient.discovery import build\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "    \n",
    "    \n",
    "    def save_secret_token(project_id, secret_id, version_id, token_file):\n",
    "        \"\"\"\n",
    "        Access the payload for the given secret version if one exists. The version\n",
    "        can be a version number as a string (e.g. \"5\") or an alias (e.g. \"latest\").\n",
    "        \"\"\"\n",
    "\n",
    "        # Import the Secret Manager client library.\n",
    "        import google_crc32c\n",
    "        from google.cloud import secretmanager\n",
    "\n",
    "        # Create the Secret Manager client.\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version.\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version.\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Verify payload checksum.\n",
    "        crc32c = google_crc32c.Checksum()\n",
    "        crc32c.update(response.payload.data)\n",
    "        if response.payload.data_crc32c != int(crc32c.hexdigest(), 16):\n",
    "            print(\"Data corruption detected.\")\n",
    "            return response\n",
    "\n",
    "        # Print the secret payload.\n",
    "        #\n",
    "        # WARNING: Do not print the secret in a production environment - this\n",
    "        # snippet is showing how to access the secret material.\n",
    "        payload = response.payload.data.decode(\"UTF-8\")\n",
    "        \n",
    "        \n",
    "        import json\n",
    "        dictt = json.loads(payload)\n",
    "        # save token\n",
    "       \n",
    "        with open(token_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dictt, f)\n",
    "        return True\n",
    "    \n",
    "   # Get the outdated worksheet as a pandas DataFrame\n",
    "    def get_sheet(spreadsheet_id, creds):\n",
    "        client = gspread.authorize(creds)\n",
    "        spreadsheet = client.open_by_key(spreadsheet_id)\n",
    "        worksheet = spreadsheet.get_worksheet(0)\n",
    "        df = pd.DataFrame(worksheet.get_all_records())\n",
    "        return df\n",
    "    \n",
    "    def get_id(id_mantenedor, creds):\n",
    "        #importar mantenedor\n",
    "        id_mantenedor = id_mantenedor\n",
    "        mantenedor_df = get_sheet(id_mantenedor, creds)\n",
    "\n",
    "        #obtener los id de las sheet\n",
    "        id_formulario = mantenedor_df.iloc[0,1].split('/')[5]\n",
    "        id_revision = mantenedor_df.iloc[1,1].split('/')[5]\n",
    "\n",
    "     \n",
    "        return id_revision, id_formulario\n",
    "    \n",
    "    \n",
    "\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    TOKEN_FILE = 'token.json'\n",
    "    save_secret_token(project_id, drive_secret_id, \"latest\", TOKEN_FILE)\n",
    "    creds = creds = ServiceAccountCredentials.from_json_keyfile_name(TOKEN_FILE, SCOPES)\n",
    "\n",
    "    id_revision, id_formulario = get_id(id_mantenedor, creds)\n",
    "    \n",
    "\n",
    "    return (id_revision, id_formulario)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00962174-dfa2-4d88-953f-ba591da67d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"google-cloud\",\n",
    "        \"google-cloud-secret-manager\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"google-api-python-client\",\n",
    "        \"tqdm\",\n",
    "        \"gcsfs\",\n",
    "        \"gspread\",\n",
    "        \"oauth2client\",\n",
    "    ],\n",
    "    output_component_file=\"pred_get_data.yaml\"\n",
    ")\n",
    "def get_data(\n",
    "    project_id: str,\n",
    "    id_mantenedor: str,\n",
    "    drive_secret_id: str,\n",
    "    docs_df: Output[Dataset],\n",
    ")-> List:\n",
    "    \n",
    "    import io\n",
    "    from googleapiclient.errors import HttpError\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "    import pandas as pd\n",
    "    import gspread\n",
    "    from google.oauth2.credentials import Credentials\n",
    "    from googleapiclient.discovery import build\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "    \n",
    "    \n",
    "    def save_secret_token(project_id, secret_id, version_id, token_file):\n",
    "        \"\"\"\n",
    "        Access the payload for the given secret version if one exists. The version\n",
    "        can be a version number as a string (e.g. \"5\") or an alias (e.g. \"latest\").\n",
    "        \"\"\"\n",
    "\n",
    "        # Import the Secret Manager client library.\n",
    "        import google_crc32c\n",
    "        from google.cloud import secretmanager\n",
    "\n",
    "        # Create the Secret Manager client.\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version.\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version.\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Verify payload checksum.\n",
    "        crc32c = google_crc32c.Checksum()\n",
    "        crc32c.update(response.payload.data)\n",
    "        if response.payload.data_crc32c != int(crc32c.hexdigest(), 16):\n",
    "            print(\"Data corruption detected.\")\n",
    "            return response\n",
    "\n",
    "        # Print the secret payload.\n",
    "        #\n",
    "        # WARNING: Do not print the secret in a production environment - this\n",
    "        # snippet is showing how to access the secret material.\n",
    "        payload = response.payload.data.decode(\"UTF-8\")\n",
    "        \n",
    "        \n",
    "        import json\n",
    "        dictt = json.loads(payload)\n",
    "        # save token\n",
    "       \n",
    "        with open(token_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dictt, f)\n",
    "        return True\n",
    "    \n",
    "   # Get the outdated worksheet as a pandas DataFrame\n",
    "    def get_sheet(spreadsheet_id, creds):\n",
    "        client = gspread.authorize(creds)\n",
    "        spreadsheet = client.open_by_key(spreadsheet_id)\n",
    "        worksheet = spreadsheet.get_worksheet(0)\n",
    "        df = pd.DataFrame(worksheet.get_all_records())\n",
    "        return df\n",
    "    \n",
    "    def get_docs_to_pred(id_mantenedor, creds):\n",
    "        #importar mantenedor\n",
    "        id_mantenedor = id_mantenedor\n",
    "        mantenedor_df = get_sheet(id_mantenedor, creds)\n",
    "\n",
    "        #obtener los id de las sheet\n",
    "        id_formulario = mantenedor_df.iloc[0,1].split('/')[5]\n",
    "        id_revision = mantenedor_df.iloc[1,1].split('/')[5]\n",
    "\n",
    "        #descargar\n",
    "        revision_df = get_sheet(id_revision, creds)\n",
    "        formulario_df = get_sheet(id_formulario, creds)\n",
    "\n",
    "        #obtenemos las filas \n",
    "        if revision_df.empty:\n",
    "            docs_df = formulario_df.copy()\n",
    "        if not revision_df.empty:\n",
    "            docs_df = formulario_df[~formulario_df['Cédula de Identidad'].isin(revision_df['Cédula de Identidad'])]\n",
    "            docs_df.reset_index(inplace = True, drop = True)\n",
    "        return id_revision, docs_df\n",
    "    \n",
    "    \n",
    "\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    TOKEN_FILE = 'token.json'\n",
    "    save_secret_token(project_id, drive_secret_id, \"latest\", TOKEN_FILE)\n",
    "    creds = creds = ServiceAccountCredentials.from_json_keyfile_name(TOKEN_FILE, SCOPES)\n",
    "\n",
    "    id_revision, doc_df = get_docs_to_pred(id_mantenedor, creds)\n",
    "    \n",
    "\n",
    "    # Upload Artifacts\n",
    "    doc_df.to_csv(docs_df.path + \".csv\", index = False)\n",
    "    #print(id_revision)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc62d8-4a33-4f9f-81e6-d806a1505796",
   "metadata": {},
   "source": [
    "# Get Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc178dde-6aa5-4f1c-95b7-7373d9466853",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "    ],\n",
    "    output_component_file=\"get_ground_truth.yaml\"\n",
    ")\n",
    "def get_ground_truth(\n",
    "    docs_df: Input[Dataset],\n",
    "    #docs_gt: OutputPath()\n",
    "    ground_truth: OutputPath()\n",
    "    )-> None:\n",
    "    import pandas as pd\n",
    "    # get data artifacts\n",
    "    df = pd.read_csv(docs_df.path+\".csv\")\n",
    "    docs_gt = []\n",
    "    for row in df.itertuples():\n",
    "        docs_gt.append({'index': row[0],\n",
    "                           'cedula': {\n",
    "                               'id' : row[8].split('=')[1], #the drive id of the cedula document\n",
    "                               'rut': row[6]  #the rut number of the driver\n",
    "                                     }, \n",
    "                           'licencia': {\n",
    "                               'id' : row[9].split('=')[1], #the drive id of the licencia document\n",
    "                               'rut': row[6]  #the rut number of the driver\n",
    "                                       }\n",
    "                           }\n",
    "                               )\n",
    "    import pickle\n",
    "    with open(f'{ground_truth}.pkl', 'wb') as f:\n",
    "        pickle.dump(docs_gt, f)\n",
    "    #return docs_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb774e0-0f1a-4d8f-a62f-bebffc5f58d5",
   "metadata": {},
   "source": [
    "# Get images\n",
    "PDFs files from drive to cropped images in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a47c0e1-cecd-4a6c-929a-404f11e8fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu\",\n",
    "    packages_to_install=[\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"pdf2image\",\n",
    "        \"opencv-python\",\n",
    "        \"google-api-python-client\",\n",
    "        \"google-cloud-secret-manager\",\n",
    "        \"google-cloud\",\n",
    "        \"oauth2client\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"tqdm\",\n",
    "        \"Pillow\"\n",
    "    ],\n",
    "    output_component_file=\"get_images.yaml\"\n",
    ")\n",
    "def get_images(\n",
    "    bucket:str,\n",
    "    project_id:str,\n",
    "    drive_secret_id: str,\n",
    "    document:str, \n",
    "    docs_gt: InputPath(),\n",
    "    docs_ds: Input[Dataset],\n",
    "    docs_gt_artifact: OutputPath()\n",
    "    ):\n",
    "    \n",
    "    ############################\n",
    "    # Librerias\n",
    "    ############################\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    from pdf2image import convert_from_bytes \n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    from googleapiclient.errors import HttpError\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "    from googleapiclient.discovery import build\n",
    "    import io\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from tempfile import TemporaryFile\n",
    "    from PIL import Image\n",
    "    from tqdm import tqdm\n",
    "    import pickle\n",
    "    \n",
    "    # install with subprocess\n",
    "    os.system('apt-get update')\n",
    "    os.system('sudo apt-get install poppler-utils -y')\n",
    "    #import subprocess\n",
    "    #subprocess.run([\"sudo apt-get install python-poppler\"], shell = True)\n",
    "    \n",
    "    ############################\n",
    "    # Helper Functions\n",
    "    ############################\n",
    "    \n",
    "    def save_secret_token(project_id, secret_id, version_id, token_file):\n",
    "        \"\"\"\n",
    "        Access the payload for the given secret version if one exists. The version\n",
    "        can be a version number as a string (e.g. \"5\") or an alias (e.g. \"latest\").\n",
    "        \"\"\"\n",
    "\n",
    "        # Import the Secret Manager client library.\n",
    "        import google_crc32c\n",
    "        from google.cloud import secretmanager\n",
    "\n",
    "        # Create the Secret Manager client.\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version.\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version.\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Verify payload checksum.\n",
    "        crc32c = google_crc32c.Checksum()\n",
    "        crc32c.update(response.payload.data)\n",
    "        if response.payload.data_crc32c != int(crc32c.hexdigest(), 16):\n",
    "            print(\"Data corruption detected.\")\n",
    "            return response\n",
    "\n",
    "        # Print the secret payload.\n",
    "        #\n",
    "        # WARNING: Do not print the secret in a production environment - this\n",
    "        # snippet is showing how to access the secret material.\n",
    "        payload = response.payload.data.decode(\"UTF-8\")\n",
    "        \n",
    "        \n",
    "        import json\n",
    "        dictt = json.loads(payload)\n",
    "        # save token\n",
    "       \n",
    "        with open(token_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dictt, f)\n",
    "        return True\n",
    "    \n",
    "    def download_file(real_file_id, creds):\n",
    "        \"\"\"Downloads a file\n",
    "        Args:\n",
    "            real_file_id: ID of the file to download\n",
    "        Returns : IO object with location.\n",
    "\n",
    "        Load pre-authorized user credentials from the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # create drive api clien\n",
    "            file_id = real_file_id\n",
    "\n",
    "            # pylint: disable=maybe-no-member\n",
    "            service = build('drive', 'v3', credentials=creds)\n",
    "            request = service.files().get_media(fileId=file_id)\n",
    "            file = io.BytesIO()\n",
    "            downloader = MediaIoBaseDownload(file, request)\n",
    "            done = False\n",
    "            while done is False:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print(F'Download {int(status.progress() * 100)}.')\n",
    "\n",
    "        except HttpError as error:\n",
    "            print(F'An error occurred: {error}')\n",
    "            file = None\n",
    "        return file.getvalue()\n",
    "    \n",
    "    \n",
    "    ############################\n",
    "    # download necessary scripts\n",
    "    ############################\n",
    "    \n",
    "    def download_blob(bucket_name, source_blob_name, destination_file_name, project_id):\n",
    "        \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "        # The ID of your GCS bucket\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "        # The ID of your GCS object\n",
    "        # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "        # The path to which the file should be downloaded\n",
    "        # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "        storage_client = storage.Client(project= project_id)\n",
    "\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Construct a client side representation of a blob.\n",
    "        # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "        # any content from Google Cloud Storage. As we don't need additional data,\n",
    "        # using `Bucket.blob` is preferred here.\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "\n",
    "        print(\n",
    "            \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "                source_blob_name, bucket_name, destination_file_name\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    os.mkdir(\"utils\")\n",
    "    download_blob(bucket, \"utils/autoanchor.py\", \"utils/autoanchor.py\", project_id)\n",
    "    download_blob(bucket, \"utils/crop.py\", \"utils/crop.py\", project_id)\n",
    "    download_blob(bucket, \"utils/datasets.py\", \"utils/datasets.py\", project_id)\n",
    "    download_blob(bucket, \"utils/general.py\", \"utils/general.py\", project_id)\n",
    "    download_blob(bucket, \"utils/google_utils.py\", \"utils/google_utils.py\", project_id)\n",
    "    download_blob(bucket, \"utils/loss.py\", \"utils/loss.py\", project_id)\n",
    "    download_blob(bucket, \"utils/metrics.py\", \"utils/metrics.py\", project_id)\n",
    "    download_blob(bucket, \"utils/plots.py\", \"utils/plots.py\", project_id)\n",
    "    download_blob(bucket, \"utils/torch_utils.py\", \"utils/torch_utils.py\", project_id)\n",
    "    \n",
    "    os.mkdir(\"models\")\n",
    "    download_blob(bucket, \"models/common.py\", \"models/common.py\", project_id)\n",
    "    download_blob(bucket, \"models/experimental.py\", \"models/experimental.py\", project_id)\n",
    "    download_blob(bucket, \"models/yolo.py\", \"models/yolo.py\", project_id)\n",
    "    download_blob(bucket, \"models/best.pt\", \"models/best.pt\", project_id)\n",
    "    \n",
    "    from utils.crop import crop\n",
    "    ############################\n",
    "    # Logic\n",
    "    ############################\n",
    "    \n",
    "    # Read Artifact\n",
    "    docs_df = pd.read_csv(docs_ds.path + \".csv\")\n",
    "    \n",
    "    with open(docs_gt + \".pkl\", 'rb') as f:\n",
    "        docs_gt = pickle.load(f)\n",
    "    \n",
    "    #En primer lugar nos aseguramos que la carpeta de destino exista y esté vacía\n",
    "    dir = 'crop_pipeline'\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    \n",
    "    #Nos creamos una lista de los nombres de los archivos de interés (files)\n",
    "    #y un dataframe en el que iremos guardando la ID de los documentos encontrados\n",
    "    #y la confianza con la que fue hallado cada uno.\n",
    "    if document == 'licencia':\n",
    "        col_name = 'Licencia de Conducir'\n",
    "        classes = 1\n",
    "    elif document == 'cedula':\n",
    "        col_name = 'Cédula de Identidad'\n",
    "        classes = 0\n",
    "        \n",
    "    docs = docs_df[col_name][:]\n",
    "    \n",
    "    # Credentials\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    TOKEN_FILE = 'token.json'\n",
    "    save_secret_token(project_id, drive_secret_id, \"latest\", TOKEN_FILE)\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(TOKEN_FILE, SCOPES)\n",
    "    \n",
    "    #Luego recorremos el total de los archivos de interés (los .pdf obtenidos de excel)\n",
    "    for file in docs:\n",
    "        doc_id = file.split('=')[1]\n",
    "        try:\n",
    "            pdf = download_file(doc_id, creds) #descargamos el pdf\n",
    "            pdf_images = convert_from_bytes(pdf, fmt=\"jpeg\", size=(960,1280)) #lo pasamos a jpeg\n",
    "        except:\n",
    "            # Si falla, no se pudo leer la imagen, la reemplazamos por una imagen en blanco\n",
    "            white_img = np.full((960,1280), 255, dtype = np.float32)\n",
    "            pdf_images = [white_img]\n",
    "        #Guardamos todas las páginas de todos los pdf con distintos nombres\n",
    "        for i in range(len(pdf_images)):\n",
    "            cv2.imwrite(('crop_pipeline/'+doc_id + str(i) + '.jpeg'),cv2.cvtColor(np.array(pdf_images[i]), cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    #Una vez ya tenemos todas las páginas, intentamos encontrar el documento que nos interesa en cada una de ellas.\n",
    "\n",
    "    df_docs = pd.DataFrame(index=['ID_IMG' , 'ID', 'IMG', 'found', 'confidence']).T\n",
    "    \n",
    "    try:\n",
    "        c = crop(weights=\"models/best.pt\", source = 'crop_pipeline/', save_img = False, save_path= 'crop', classes = classes, df = df_docs)\n",
    "    except:\n",
    "        c = [[]]\n",
    "    \n",
    "    df_docs =  df_docs.sort_values('confidence', ascending=False).drop_duplicates(['ID'], keep ='first').sort_index().reset_index(drop=True)\n",
    "    #Cambiamos los found de 1 o 0 a True y False\n",
    "    for i in df_docs.index:\n",
    "        if df_docs.loc[i,'found'] == 1:\n",
    "            df_docs.loc[i,'found'] = True\n",
    "        elif df_docs.loc[i,'found'] == 0:\n",
    "            df_docs.loc[i,'found'] = False\n",
    "            \n",
    "    #Si los dataframe cuentan con información acerca de un recorte de un documento\n",
    "    #en particular, entonces se marca en 'found'        \n",
    "    for i in range(len(docs_gt)): \n",
    "        doc_id = docs_gt[i][document]['id']\n",
    "        found_doc = df_docs.loc[df_docs['ID']==doc_id]['found'].item()\n",
    "        docs_gt[i][document]['found'] = found_doc       \n",
    "\n",
    "    # limpiamos los archivos de la carpeta\n",
    "    for f in os.listdir(dir):\n",
    "        if f != '.ipynb_checkpoints':\n",
    "            os.remove('crop_pipeline/' +f)\n",
    "            \n",
    "    # Save images to GCS \n",
    "    project_bucket = \"enrollment_verification\"\n",
    "    client = storage.Client(project= project_id)\n",
    "    buck = client.get_bucket(project_bucket)\n",
    "    \n",
    "    \n",
    "    for i in tqdm(df_docs.index):\n",
    "        nombre_img = df_docs.loc[i,'ID']+'.jpeg'\n",
    "        path = f'inference_{document}/'+ nombre_img\n",
    "        df_docs.loc[i,'ID_IMG'] = nombre_img\n",
    "        blob = buck.blob(path)\n",
    "        if df_docs.loc[i,'found']:\n",
    "            img = np.array(df_docs.loc[i,'IMG'][0])\n",
    "            pil_img = Image.fromarray(img)\n",
    "            b = io.BytesIO() \n",
    "            pil_img.save(b, 'jpeg') \n",
    "            pil_img.close()\n",
    "            blob.upload_from_string(b.getvalue(), content_type='image/jpeg')\n",
    "    \n",
    "    # Save artifacts\n",
    "    import pickle\n",
    "    with open(docs_gt_artifact + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(docs_gt, f)\n",
    "    \n",
    "    #return docs_gt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19633975-a351-408d-9b95-cc6e75426e3f",
   "metadata": {},
   "source": [
    "# Donut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d36871-4756-48ad-82f9-ff17e4d920b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/pytorch-gpu\",\n",
    "    packages_to_install=[\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"transformers\",\n",
    "        \"opencv-python\",\n",
    "        \"google-api-python-client\",\n",
    "        \"google-cloud-secret-manager\",\n",
    "        \"google-cloud\",\n",
    "        \"oauth2client\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"tqdm\",\n",
    "        \"Pillow\",\n",
    "        \"datasets\"\n",
    "    ],\n",
    "    output_component_file=\"inference.yaml\"\n",
    ")\n",
    "def inference(\n",
    "    project_id: str,\n",
    "    bucket:str,\n",
    "    document:str,\n",
    "    model:str,\n",
    "    docs_gt_artifact: InputPath(),\n",
    "    docs_pred_artifact: OutputPath()\n",
    "             ):\n",
    "    \n",
    "    ###################\n",
    "    # Librerias\n",
    "    ###################\n",
    "    \n",
    "    import copy\n",
    "    import os\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    \n",
    "    ###################\n",
    "    # Helper Functions\n",
    "    ###################\n",
    "    \n",
    "    def download_blob(bucket_name, source_blob_name, destination_file_name, project_id):\n",
    "        \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "        # The ID of your GCS bucket\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "        # The ID of your GCS object\n",
    "        # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "        # The path to which the file should be downloaded\n",
    "        # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "        storage_client = storage.Client(project= project_id)\n",
    "\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Construct a client side representation of a blob.\n",
    "        # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "        # any content from Google Cloud Storage. As we don't need additional data,\n",
    "        # using `Bucket.blob` is preferred here.\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "\n",
    "        print(\n",
    "            \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "                source_blob_name, bucket_name, destination_file_name\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    os.mkdir(\"models\")\n",
    "    download_blob(bucket, \"models/donut.py\", \"models/donut.py\", project_id)\n",
    "    \n",
    "    from models.donut import donut_inference\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_secret_token(project_id, secret_id, version_id):\n",
    "        \"\"\"\n",
    "        Access the payload for the given secret version if one exists. The version\n",
    "        can be a version number as a string (e.g. \"5\") or an alias (e.g. \"latest\").\n",
    "        \"\"\"\n",
    "\n",
    "        # Import the Secret Manager client library.\n",
    "        import google_crc32c\n",
    "        from google.cloud import secretmanager\n",
    "\n",
    "        # Create the Secret Manager client.\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version.\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version.\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Verify payload checksum.\n",
    "        crc32c = google_crc32c.Checksum()\n",
    "        crc32c.update(response.payload.data)\n",
    "        if response.payload.data_crc32c != int(crc32c.hexdigest(), 16):\n",
    "            print(\"Data corruption detected.\")\n",
    "            return response\n",
    "\n",
    "        # Print the secret payload.\n",
    "        #\n",
    "        # WARNING: Do not print the secret in a production environment - this\n",
    "        # snippet is showing how to access the secret material.\n",
    "        payload = response.payload.data.decode(\"UTF-8\")\n",
    "        \n",
    "        \n",
    "        return payload\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    # Logic\n",
    "    ####################\n",
    "    \n",
    "     # Load Artifact\n",
    "    with open(docs_gt_artifact + \".pkl\", 'rb') as f:\n",
    "        docs_gt = pickle.load(f)\n",
    "    \n",
    "    hf_token = get_secret_token(project_id, \"HF_TOKEN\", \"latest\")\n",
    "    \n",
    "    docs_pred = copy.deepcopy(docs_gt)\n",
    "    licencia_output = []\n",
    "    cedula_output = []\n",
    "    \n",
    "    project_bucket = \"enrollment_verification\"\n",
    "    \n",
    "    if document == 'licencia':\n",
    "        #model = \"AA-supply/donut-finetuned-lic-crop\"\n",
    "        path = f'inference_{document}'\n",
    "    elif document == 'cedula':\n",
    "        #model = \"AA-supply/donut-finetuned-cedula-crop\"\n",
    "        path = f'inference_{document}'\n",
    "        \n",
    "\n",
    "    docs_pred = donut_inference(docs_pred, project_id, hf_token, document, model, project_bucket,path, licencia_output)\n",
    "    #docs_pred = donut_inference(docs_pred, 'cedula', \"AA-supply/donut-finetuned-cedula-crop\", 'crop_pipeline_cedula', cedula_output)\n",
    "    \n",
    "    # Save artifacts\n",
    "    import pickle\n",
    "    with open(docs_pred_artifact + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(docs_pred, f)\n",
    "    \n",
    "    #return docs_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7a273-76f2-43f2-82fa-79485644b2be",
   "metadata": {},
   "source": [
    "# Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c81fd9d9-fb3b-47c5-b676-e90ba1f334c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"tqdm\"\n",
    "    ],\n",
    "    output_component_file=\"postprocess.yaml\"\n",
    ")\n",
    "def postprocess(\n",
    "               document: str,\n",
    "               docs_gt: InputPath(),\n",
    "               docs_pred: InputPath(),\n",
    "               docs_gt_artifact: OutputPath(),\n",
    "               docs_pred_artifact: OutputPath(),\n",
    "               ):\n",
    "    \n",
    "    from datetime import date\n",
    "    from datetime import datetime   \n",
    "    import copy\n",
    "    import re \n",
    "    import pickle\n",
    "    \n",
    "    # Load Artifact\n",
    "    with open(docs_pred + \".pkl\", 'rb') as f:\n",
    "        docs_pred = pickle.load(f)\n",
    "        \n",
    "    with open(docs_gt + \".pkl\", 'rb') as f:\n",
    "        docs_gt = pickle.load(f)\n",
    "    \n",
    "    #Esta función se asegura de que el rut  están en el formato correcto\n",
    "    def preproc(gt):\n",
    "        ground_truth = copy.deepcopy(gt)\n",
    "        r= ground_truth['rut']\n",
    "        r = str(r).upper()\n",
    "        r = r.strip()    \n",
    "        if r == '':\n",
    "            return ground_truth\n",
    "        else:\n",
    "            r = re.sub(r'[^0-9K]+', '', r)\n",
    "            ground_truth['rut_proc'] = r \n",
    "            return ground_truth\n",
    "\n",
    "    #Esta función se asegura de que el rut y la fecha de vencimiento\n",
    "    #de una predicción estén en el formato correcto\n",
    "    def postproc(pred):\n",
    "        dic = copy.deepcopy(pred)\n",
    "\n",
    "        r = dic['rut']\n",
    "        r = str(r).upper()\n",
    "        v = dic['venc'].upper()\n",
    "\n",
    "        r = r.strip() #elimina los espacios al princio y al final del string\n",
    "        if r == '':\n",
    "            r = 'Not Found'\n",
    "        elif r == 'NOT FOUND':\n",
    "            r = 'Not Found'\n",
    "        else:\n",
    "            r = re.sub(r'[^0-9K]+', '', r)  #reemplaza todos los caracteres que no sean 0-9 o K por ''\n",
    "\n",
    "        #Por otro lado la fecha de vencimiento será estandarizada, chequearemos que los primeros\n",
    "        #dos dígitos sean números (el día) y los últimos 4 sean dígitos (el año).\n",
    "        #Entre estos dos está el mes, el cual puede estar en formato numérico o en letras, si\n",
    "        #está en letras, lo pasaremos a número.\n",
    "        #Finalmente, nos aseguremos que estos 4 dígitos sean interpretables en formato fecha,\n",
    "        #si alguno de ellos no está, se entregará una fecha antigua, la cual evitará que sea aprobada.\n",
    "        v = v.strip()\n",
    "        if v == '':\n",
    "            fecha = 'Not Found'\n",
    "        else:\n",
    "            v = re.sub(\"\\s\\s+\" , \" \", v) #reemplazamos los espacios repetidos por un solo espacio \" \"\n",
    "            v = v.replace('/', ' ')\n",
    "            v = re.sub(r'[^A-Za-z0-9 ]+', '', v)  #reemplaza todos los caracteres que no sean alfanumericos o espacio por ''\n",
    "\n",
    "            fecha = re.findall(r'\\b(\\d{2})\\s(\\d{2})\\s(\\d{4})\\b', v)\n",
    "            if not fecha:\n",
    "                fecha = re.findall(r'\\b(\\d{2})\\s(ENE|FEB|MAR|ABR|MAYO|JUN|JUL|AGO|SEPT|OCT|NOV|DIC)\\s(\\d{4})\\b', v)\n",
    "\n",
    "            if fecha:\n",
    "                dia = fecha[0][0]\n",
    "                mes = fecha[0][1]\n",
    "                año = fecha[0][2]\n",
    "                if not mes.isnumeric():\n",
    "                    months = {'ENE' : '01', 'FEB' : '02', 'MAR' : '03', 'ABR':'04' , 'MAYO':'05' , 'JUN':'06', 'JUL':'07' , 'AGO':'08' , 'SEPT':'09' , 'OCT':'10', 'NOV':'11', 'DIC':'12'}\n",
    "                    mes = months[mes]\n",
    "                fecha = f'{dia}/{mes}/{año}'\n",
    "            try:\n",
    "                datetime.date(datetime.strptime(fecha,'%d/%m/%Y'))\n",
    "            except:\n",
    "                fecha = 'Error de Lectura'\n",
    "            if not fecha:\n",
    "                fecha = 'Not Found'\n",
    "\n",
    "        dic['rut_proc'] = r\n",
    "        dic['venc_proc'] = fecha\n",
    "\n",
    "        return dic\n",
    "\n",
    "    #Hacemos el preprocesamiento y el postprocesamiento\n",
    "    # Aprovechamos que len(docs_gt)==len(docs_pred)\n",
    "    for i in range(len(docs_gt)):\n",
    "        docs_gt[i][document] = preproc(docs_gt[i][document])\n",
    "        docs_pred[i][document] = postproc(docs_pred[i][document])\n",
    "    #    docs_gt[i]['licencia'] = preproc(docs_gt[i]['licencia'])\n",
    "\n",
    "    #for i in range(len(docs_pred)):\n",
    "    #    docs_pred[i]['cedula'] = postproc(docs_pred[i]['cedula'])\n",
    "    #    docs_pred[i]['licencia'] = postproc(docs_pred[i]['licencia'])\n",
    "    \n",
    "    # Save artifacts\n",
    "    import pickle\n",
    "    with open(docs_gt_artifact + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(docs_gt, f)\n",
    "        \n",
    "    with open(docs_pred_artifact + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(docs_pred, f)\n",
    "    \n",
    "    #return docs_gt, docs_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94106d61-e031-46a0-b708-31776373eec0",
   "metadata": {},
   "source": [
    "# Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c4e9c41-75bd-4a51-9a5b-7dc5d760fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"tqdm\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"nltk\"\n",
    "    ],\n",
    "    output_component_file=\"verification.yaml\"\n",
    ")\n",
    "def verification(\n",
    "    project_id:str,\n",
    "    bucket:str,\n",
    "    docs_df_artifact: Input[Dataset],\n",
    "    docs_gt_cedula_artifact: InputPath(),\n",
    "    docs_pred_cedula_artifact: InputPath(),\n",
    "    docs_gt_licencia_artifact: InputPath(),\n",
    "    docs_pred_licencia_artifact: InputPath(),\n",
    "    df_val_artifact: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    \n",
    "    ###################\n",
    "    # Helper Functions\n",
    "    ###################\n",
    "    \n",
    "    def download_blob(bucket_name, source_blob_name, destination_file_name, project_id):\n",
    "        \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "        # The ID of your GCS bucket\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "        # The ID of your GCS object\n",
    "        # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "        # The path to which the file should be downloaded\n",
    "        # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "        storage_client = storage.Client(project= project_id)\n",
    "\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Construct a client side representation of a blob.\n",
    "        # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "        # any content from Google Cloud Storage. As we don't need additional data,\n",
    "        # using `Bucket.blob` is preferred here.\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "\n",
    "        print(\n",
    "            \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "                source_blob_name, bucket_name, destination_file_name\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    os.mkdir(\"utils\")\n",
    "    download_blob(bucket, \"utils/verification.py\", \"utils/verification.py\", project_id)\n",
    "    \n",
    "    from utils.verification import observaciones,venc_verifier, rut_verifier, proximity_check, gen_checker, length_checker, verificador, verificador_venc, correcciones\n",
    "    \n",
    "    \n",
    "    # get data artifacts\n",
    "    import pickle\n",
    "    docs_df = pd.read_csv(docs_df_artifact.path +\".csv\")\n",
    "    \n",
    "    with open(docs_gt_cedula_artifact + \".pkl\", 'rb') as f:\n",
    "        docs_gt_cedula = pickle.load(f)\n",
    "    \n",
    "    with open(docs_pred_cedula_artifact + \".pkl\", 'rb') as f:\n",
    "        docs_pred_cedula = pickle.load(f)\n",
    "    \n",
    "    with open(docs_gt_licencia_artifact + \".pkl\", 'rb') as f:\n",
    "        docs_gt_licencia = pickle.load(f)\n",
    "    \n",
    "    with open(docs_pred_licencia_artifact + \".pkl\", 'rb') as f:\n",
    "        docs_pred_licencia = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_val = docs_df.copy(deep = True)\n",
    "    new_columns = ['VERIFICADO',  'OBSERVACIONES GENERALES','OBSERVACIONES CEDULA','OBSERVACIONES LICENCIA', 'LEC RUT CED.','LEC RUT LIC.', 'LEC VENC CED.','LEC VENC LIC.', 'CORRECCIONES RUT']\n",
    "    for i in range(len(new_columns)):\n",
    "        df_val[new_columns[i]] = np.nan\n",
    "    \n",
    "    \n",
    "    obs_gen_licencia, obs_licencia = observaciones(docs_gt_licencia, docs_pred_licencia, \"licencia\")\n",
    "    obs_gen_cedula, obs_cedula = observaciones(docs_gt_cedula, docs_pred_cedula, \"cedula\")\n",
    "    correcciones_ = correcciones(docs_gt_cedula, docs_pred_cedula, docs_pred_licencia)\n",
    "    \n",
    "    # medio innecesario esto\n",
    "    obs_gen = []\n",
    "    for i in range(len(obs_gen_cedula)):\n",
    "        if obs_gen_cedula[i] != '' and obs_gen_licencia[i] != '':\n",
    "            ob = f'{obs_gen_cedula[i]}, {obs_gen_licencia[i]}'\n",
    "        elif obs_gen_cedula != '':\n",
    "            ob = obs_gen_cedula[i]\n",
    "        elif obs_gen_licencia != '':\n",
    "            ob = obs_gen_licencia[i]\n",
    "        else:\n",
    "            ob = ''\n",
    "        obs_gen.append(ob)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    #obs = observaciones(docs_gt, docs_pred)\n",
    "    df_val['OBSERVACIONES GENERALES']  = obs_gen\n",
    "    df_val['OBSERVACIONES CEDULA']  = obs_cedula\n",
    "    df_val['OBSERVACIONES LICENCIA']  = obs_licencia\n",
    "    df_val['CORRECCIONES RUT'] = correcciones_\n",
    "    \n",
    "    \n",
    "    # Puede pasar que docs_pred licencia se de disinto largo que cedula? \n",
    "    for i in range(len(docs_pred_cedula)):\n",
    "        pred_id_cedula = docs_pred_cedula[i]['index']\n",
    "        pred_id_licencia = docs_pred_licencia[i]['index']\n",
    "        ruts_general = rut_verifier(docs_gt_cedula[pred_id_cedula]['cedula']['rut_proc'],docs_pred_cedula[i]['cedula']['rut_proc'],docs_pred_licencia[pred_id_licencia]['licencia']['rut_proc'])\n",
    "        venc_general = venc_verifier(docs_pred_cedula[pred_id_cedula]['cedula']['venc_proc'],docs_pred_licencia[pred_id_licencia]['licencia']['venc_proc'])\n",
    "        df_val['VERIFICADO'].iloc[pred_id_cedula] = ruts_general and venc_general\n",
    "        #Predicciones:\n",
    "        df_val['LEC RUT CED.'].iloc[pred_id_cedula] = docs_pred_cedula[pred_id_cedula]['cedula']['rut']\n",
    "        df_val['LEC RUT LIC.'].iloc[pred_id_licencia] = docs_pred_licencia[pred_id_licencia]['licencia']['rut']\n",
    "        df_val['LEC VENC CED.'].iloc[pred_id_cedula] = docs_pred_cedula[pred_id_cedula]['cedula']['venc']\n",
    "        df_val['LEC VENC LIC.'].iloc[pred_id_licencia] = docs_pred_licencia[pred_id_licencia]['licencia']['venc']\n",
    "\n",
    "    # Upload Artifacts\n",
    "    df_val.to_csv(df_val_artifact.path + \".csv\", index = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf67d1-2e3d-4515-b54d-ad0b23af4eae",
   "metadata": {},
   "source": [
    "# Load to Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f311a89d-9820-48f4-807e-4443847d382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"google-api-python-client\",\n",
    "        \"google-cloud-secret-manager\",\n",
    "        \"google-cloud\",\n",
    "        \"oauth2client\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"gspread\",\n",
    "        \"gspread-formatting\",\n",
    "    ],\n",
    "    output_component_file=\"upload_results.yaml\"\n",
    ")\n",
    "def upload_results(\n",
    "    project_id:str,\n",
    "    drive_secret_id:str,\n",
    "    id_revision: str,\n",
    "    df_val_artifact: Input[Dataset]\n",
    "    ):\n",
    "    \n",
    "    import gspread\n",
    "    import pandas as pd\n",
    "    from gspread_formatting import get_conditional_format_rules,ConditionalFormatRule,GridRange,BooleanRule, BooleanCondition,CellFormat, Color  \n",
    "    from googleapiclient.errors import HttpError\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "    import pandas as pd\n",
    "    import gspread\n",
    "    from google.oauth2.credentials import Credentials\n",
    "    from googleapiclient.discovery import build\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "    #####################\n",
    "    ## Helper Functions\n",
    "    #####################\n",
    "    \n",
    "    def save_secret_token(project_id, secret_id, version_id, token_file):\n",
    "        \"\"\"\n",
    "        Access the payload for the given secret version if one exists. The version\n",
    "        can be a version number as a string (e.g. \"5\") or an alias (e.g. \"latest\").\n",
    "        \"\"\"\n",
    "\n",
    "        # Import the Secret Manager client library.\n",
    "        import google_crc32c\n",
    "        from google.cloud import secretmanager\n",
    "\n",
    "        # Create the Secret Manager client.\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        # Build the resource name of the secret version.\n",
    "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "        # Access the secret version.\n",
    "        response = client.access_secret_version(request={\"name\": name})\n",
    "\n",
    "        # Verify payload checksum.\n",
    "        crc32c = google_crc32c.Checksum()\n",
    "        crc32c.update(response.payload.data)\n",
    "        if response.payload.data_crc32c != int(crc32c.hexdigest(), 16):\n",
    "            print(\"Data corruption detected.\")\n",
    "            return response\n",
    "\n",
    "        # Print the secret payload.\n",
    "        #\n",
    "        # WARNING: Do not print the secret in a production environment - this\n",
    "        # snippet is showing how to access the secret material.\n",
    "        payload = response.payload.data.decode(\"UTF-8\")\n",
    "        \n",
    "        \n",
    "        import json\n",
    "        dictt = json.loads(payload)\n",
    "        # save token\n",
    "       \n",
    "        with open(token_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dictt, f)\n",
    "        return True\n",
    "\n",
    "    def append_to_sheet(spreadsheet_id, df, client):\n",
    "        # Get the outdated worksheet as a pandas DataFrame\n",
    "        spreadsheet = client.open_by_key(spreadsheet_id)\n",
    "        worksheet = spreadsheet.get_worksheet(0)\n",
    "\n",
    "        # Append the columns names\n",
    "        #worksheet.add_rows(1)\n",
    "        worksheet.update([list(df.columns.values)] , raw = True)\n",
    "\n",
    "        # Append the rows from the DataFrame to the worksheet\n",
    "        df.fillna('',inplace=True)\n",
    "        df_rows = df.values.tolist()\n",
    "        worksheet.add_rows(len(df_rows))\n",
    "        worksheet.append_rows(df_rows)\n",
    "        print('Google Spreadsheet updated successfully.')\n",
    "\n",
    "    def add_color(spreadsheet_id, client):\n",
    "        # Get the outdated worksheet as a pandas DataFrame\n",
    "        spreadsheet = client.open_by_key(spreadsheet_id)\n",
    "        worksheet = spreadsheet.get_worksheet(0)\n",
    "        rules = get_conditional_format_rules(worksheet)\n",
    "        rules.clear()\n",
    "\n",
    "        rule_g = ConditionalFormatRule( ranges=[GridRange.from_a1_range('M:M', worksheet)],\n",
    "                                     booleanRule=BooleanRule(condition=BooleanCondition('TEXT_EQ', ['TRUE']),\n",
    "                                                             format=CellFormat(backgroundColor=Color(0.15, 0.5, 0.15))\n",
    "                                                            )\n",
    "                                    )\n",
    "        rule_r = ConditionalFormatRule( ranges=[GridRange.from_a1_range('M:M', worksheet)],\n",
    "                                     booleanRule=BooleanRule(condition=BooleanCondition('TEXT_EQ', ['FAlSE']),\n",
    "                                                             format=CellFormat(backgroundColor=Color(0.5,0.15,0.15))\n",
    "                                                            )\n",
    "                                    )\n",
    "\n",
    "        # or, to replace any existing rules with just your single rule:\n",
    "\n",
    "        rules.append(rule_g)\n",
    "        rules.append(rule_r)\n",
    "        rules.save()\n",
    "    \n",
    "    #####################\n",
    "    # Logic\n",
    "    #####################\n",
    "    \n",
    "    # load artifact\n",
    "    df_val = pd.read_csv(df_val_artifact.path + \".csv\")\n",
    "        \n",
    "    # Authenticate and authorize the application to access the Google Drive API\n",
    "    # Credentials\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    TOKEN_FILE = 'token.json'\n",
    "    save_secret_token(project_id, drive_secret_id, \"latest\", TOKEN_FILE)\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(TOKEN_FILE, SCOPES)\n",
    "    client = gspread.authorize(creds)\n",
    "    \n",
    "    append_to_sheet(id_revision, df_val, client)\n",
    "    add_color(id_revision, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901640b-9c3d-4027-bbf4-8677cd7ab96b",
   "metadata": {},
   "source": [
    "# BUILD PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3258b796-4e70-4b66-b48e-4379d34e0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ID = \"tc-sc-bi-bigdata-corp-tsod-dev\" # se puede sacar desde gcp automaticamente\n",
    "BUCKET = \"enrollment_verification_pipeline\"\n",
    "#MODEL_DIRECTORY = os.environ['AIP_MODEL_DIR']\n",
    "DRIVE_SECRET_ID = \"drive_token\"\n",
    "ID_MANTENEDOR = \"1Wj-wKyzGyfLoujoUE68z1uPtVhTqLTMWgZ3KhChPvdY\"\n",
    "MODEL_LICENCIA = \"AA-supply/donut-finetuned-crop-licencia-DA\"\n",
    "MODEL_CEDULA = \"AA-supply/donut-finetuned-crop-cedula-DA\"\n",
    "    \n",
    "@kfp.dsl.pipeline(\n",
    "    # i.e. in my case: PIPELINE_ROOT = 'gs://my-bucket/test_vertex/pipeline_root/'\n",
    "    # Can be overriden when submitting the pipeline\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=\"enrollment-verification-pipeline\",  # Your own naming for the pipeline.\n",
    ")\n",
    "def pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    bucket:str = BUCKET,\n",
    "    id_mantenedor:str = ID_MANTENEDOR,\n",
    "    drive_secret_id:str = DRIVE_SECRET_ID,\n",
    "    model_licencia:str = MODEL_LICENCIA,\n",
    "    model_cedula:str=MODEL_CEDULA\n",
    "):\n",
    "    # Get Data and Ground Truth\n",
    "    \n",
    "    get_ids_task = get_ids(\n",
    "    project_id=project_id,\n",
    "    id_mantenedor=id_mantenedor,\n",
    "    drive_secret_id=drive_secret_id)\n",
    "    \n",
    "    get_data_task = get_data( \n",
    "    project_id = project_id,\n",
    "    id_mantenedor=id_mantenedor,\n",
    "    drive_secret_id = drive_secret_id\n",
    "    )\n",
    "    \n",
    "    grund_truth_task = get_ground_truth(\n",
    "    docs_df = get_data_task.outputs[\"docs_df\"]\n",
    "    )\n",
    "    \n",
    "    # Get Images\n",
    "    \n",
    "    license_get_images_task = get_images(bucket=BUCKET,\n",
    "    project_id=project_id,\n",
    "    drive_secret_id = drive_secret_id,\n",
    "    document = 'licencia',                                    \n",
    "    docs_gt = grund_truth_task.output,\n",
    "    docs_ds=get_data_task.outputs[\"docs_df\"]\n",
    "    ).add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
    "    \n",
    "    cedula_get_images_task = get_images(bucket=BUCKET,\n",
    "    project_id=project_id,\n",
    "    drive_secret_id = drive_secret_id,\n",
    "    document = 'cedula',\n",
    "    docs_gt = grund_truth_task.output,\n",
    "    docs_ds=get_data_task.outputs[\"docs_df\"]\n",
    "    ).add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
    "    \n",
    "    # Inference\n",
    "    \n",
    "    licence_inference_task = inference(\n",
    "    project_id=project_id ,\n",
    "    bucket=bucket,\n",
    "    document='licencia',\n",
    "    model = model_licencia,\n",
    "    docs_gt_artifact=license_get_images_task.outputs[\"docs_gt_artifact\"]\n",
    "    ).add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
    "    \n",
    "    cedula_inference_task = inference(\n",
    "    project_id=project_id ,\n",
    "    bucket=bucket,\n",
    "    document='cedula',\n",
    "    model = model_cedula,\n",
    "    docs_gt_artifact=cedula_get_images_task.outputs[\"docs_gt_artifact\"]\n",
    "    ).add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
    "    \n",
    "    # Postprocess\n",
    "     \n",
    "    licence_postprocess_task = postprocess(\n",
    "    document = 'licencia',\n",
    "    docs_gt=license_get_images_task.outputs[\"docs_gt_artifact\"],\n",
    "    docs_pred=licence_inference_task.outputs[\"docs_pred_artifact\"]\n",
    "    )\n",
    "    \n",
    "    cedula_postprocess_task = postprocess(\n",
    "    document= 'cedula',\n",
    "    docs_gt=cedula_get_images_task.outputs[\"docs_gt_artifact\"],\n",
    "    docs_pred=cedula_inference_task.outputs[\"docs_pred_artifact\"]\n",
    "    )\n",
    "    \n",
    "    # Verification\n",
    "    verification_task = verification(\n",
    "    project_id = project_id,\n",
    "    bucket = bucket,\n",
    "    docs_df_artifact = get_data_task.outputs[\"docs_df\"],\n",
    "    docs_gt_cedula_artifact = cedula_postprocess_task.outputs[\"docs_gt_artifact\"],\n",
    "    docs_pred_cedula_artifact = cedula_postprocess_task.outputs[\"docs_pred_artifact\"],\n",
    "    docs_gt_licencia_artifact = licence_postprocess_task.outputs[\"docs_gt_artifact\"],\n",
    "    docs_pred_licencia_artifact = licence_postprocess_task.outputs[\"docs_pred_artifact\"],\n",
    "    )\n",
    "    \n",
    "    # Upload results\n",
    "    upload_results_task = upload_results(\n",
    "    project_id=project_id,\n",
    "    drive_secret_id=drive_secret_id,\n",
    "    id_revision=get_ids_task.outputs[\"id_revision\"],\n",
    "    df_val_artifact=verification_task.outputs[\"df_val_artifact\"]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d933dc8-9c84-4e44-9701-691376a273a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compiling the 'PIPELINE'    \n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"enrollment_verification.json\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d192274-57aa-494d-b69a-a253fc392229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1003479373544/locations/us-central1/pipelineJobs/enrollment-verification-pipeline-20230324184311\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1003479373544/locations/us-central1/pipelineJobs/enrollment-verification-pipeline-20230324184311')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/enrollment-verification-pipeline-20230324184311?project=1003479373544\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = \"enrollment_verification\",\n",
    "                             template_path = \"enrollment_verification.json\",\n",
    "                             #job_id = JOB_ID,\n",
    "                             pipeline_root = PIPELINE_ROOT,\n",
    "                            # parameter_values = {\"query\":query,\n",
    "                            #                    \"project_id\": PROJECT_ID},\n",
    "                             enable_caching = False,\n",
    "                             #encryption_spec_key_name = CMEK,\n",
    "                             #labels = LABELS,\n",
    "                             #credentials = CREDENTIALS,\n",
    "                             project = PROJECT_ID,\n",
    "                             location = REGION)\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc43a97-0b8f-4518-9fd6-58afa21e0953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244eda87-ebaf-4597-b394-245daf523b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6237722-e46b-414e-9ba4-663e07bf2070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bc67f-67ae-4ea7-bd18-c593c12c55f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
