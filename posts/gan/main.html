<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.21">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Diegulio">
<meta name="dcterms.date" content="2024-02-28">

<title>Paper Implementation Series: Generative Adversarial Networks – 🎯</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo3.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-f9d2e688c83043042fa051765c26f29b.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-67839544912bc898267f15026188952a.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-f9d2e688c83043042fa051765c26f29b.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-1e494f7d463c62844c0a5ba0ca2952b9.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-2bc1395d7f116208967e2be16dafdae1.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-1e494f7d463c62844c0a5ba0ca2952b9.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Paper Implementation Series: Generative Adversarial Networks – 🎯">
<meta property="og:description" content="Implementamos el paper Generative Adversarial Networks">
<meta property="og:image" content="https://diegulio.github.io/posts/gan/paper.jpeg">
<meta property="og:site_name" content="🎯">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo3.png" alt="" class="navbar-logo light-content">
    <img src="../../logo3.png" alt="" class="navbar-logo dark-content">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">🎯</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/diegulio"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/dieguliomachado/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Paper Implementation Series: Generative Adversarial Networks</h1>
            <p class="subtitle lead">Implementamos el paper Generative Adversarial Networks</p>
                                <div class="quarto-categories">
                <div class="quarto-category">python</div>
                <div class="quarto-category">pytorch</div>
                <div class="quarto-category">paper</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Diegulio </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 28, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#tópico-gan-paper-implementation" id="toc-tópico-gan-paper-implementation" class="nav-link active" data-scroll-target="#tópico-gan-paper-implementation">📝 Tópico: GAN Paper Implementation</a></li>
  <li><a href="#motivación-implementar-un-paper" id="toc-motivación-implementar-un-paper" class="nav-link" data-scroll-target="#motivación-implementar-un-paper">Motivación: Implementar un Paper</a></li>
  <li><a href="#tool-path-que-utilizaremos" id="toc-tool-path-que-utilizaremos" class="nav-link" data-scroll-target="#tool-path-que-utilizaremos">🔨&nbsp;Tool Path: Que utilizaremos</a></li>
  <li><a href="#concept-path-que-aprenderemos" id="toc-concept-path-que-aprenderemos" class="nav-link" data-scroll-target="#concept-path-que-aprenderemos">💭&nbsp;Concept Path: Que aprenderemos</a></li>
  <li><a href="#estrategia-como-abordamos" id="toc-estrategia-como-abordamos" class="nav-link" data-scroll-target="#estrategia-como-abordamos">♟️&nbsp;Estrategia: Como abordamos</a></li>
  <li><a href="#prototyping-gan" id="toc-prototyping-gan" class="nav-link" data-scroll-target="#prototyping-gan">🧠&nbsp;Prototyping: GAN</a>
  <ul class="collapse">
  <li><a href="#generative-adversarial-networks" id="toc-generative-adversarial-networks" class="nav-link" data-scroll-target="#generative-adversarial-networks">Generative Adversarial Networks</a></li>
  <li><a href="#implementando-gan" id="toc-implementando-gan" class="nav-link" data-scroll-target="#implementando-gan">👨🏾‍💻&nbsp;Implementando GAN</a>
  <ul class="collapse">
  <li><a href="#generador" id="toc-generador" class="nav-link" data-scroll-target="#generador">1. Generador</a></li>
  <li><a href="#discriminador" id="toc-discriminador" class="nav-link" data-scroll-target="#discriminador">2. Discriminador</a></li>
  <li><a href="#rutina-de-entrenamiento" id="toc-rutina-de-entrenamiento" class="nav-link" data-scroll-target="#rutina-de-entrenamiento">3. Rutina de Entrenamiento</a></li>
  <li><a href="#resultados" id="toc-resultados" class="nav-link" data-scroll-target="#resultados">4. Resultados</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#dcgan" id="toc-dcgan" class="nav-link" data-scroll-target="#dcgan">DCGAN</a></li>
  <li><a href="#conclusiones" id="toc-conclusiones" class="nav-link" data-scroll-target="#conclusiones">Conclusiones</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<p><a href="https://github.com/diegulio/paper_implementations"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" class="img-fluid"></a></p>
<section id="tópico-gan-paper-implementation" class="level1">
<h1>📝 Tópico: GAN Paper Implementation</h1>
<p>En este post, implementaremos un paper (SPOILER: en realidad serán 2) desde 0, esto con el fin de mostrar todo lo que se puede aprender de este proyecto y motivar al lector a intentarlo. Intentaré narrar este post como un “diario de vida”, mostrando los desafíos que superé, y los que no. Los papers a implementar en esta ocasión serán:</p>
<ul>
<li><a href="https://arxiv.org/abs/1406.2661"><strong>Generative Adversarial Networks</strong></a></li>
<li><strong><a href="https://arxiv.org/abs/1511.06434">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></strong></li>
</ul>
<p>Elegí el primero ya que evalué que era un paper totalmente implementable desde el punto de vista técnico, era un modelo y algoritmo “simple” de implementar y no se necesitaría mucho cómputo. El segundo lo implementé sólo con la intención de mejorar los resultados del primero, y con el objetivo de seguir aprendiendo!</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>⚠️
</div>
</div>
<div class="callout-body-container callout-body">
<p>No me centraré demasiado en aspectos técnicos de alto nivel, como el uso básico de Pytorch. El lector puede consultar internet o mi post <strong><a href="https://diegulio.github.io/posts/pytorch_breed_classification/main.html">Image Classification with Pytorch Lightning</a></strong> si está interesado</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>🛑
</div>
</div>
<div class="callout-body-container callout-body">
<p>DISCLAIMER: En este paper no muestro <strong>cómo implementar</strong> un paper, si no más bien como lo implementé yo, desde mi conocimiento, por lo que me disculpo de antemano si algo de lo que habré hecho fue ineficiente o incorrecto y agradeceré un montón si me informan cualquier sugerencia! ❤️</p>
</div>
</div>
</section>
<section id="motivación-implementar-un-paper" class="level1">
<h1>Motivación: Implementar un Paper</h1>
<p>Una muy buena práctica que grandes mentes en tecnología normalmente recomiendan es implementar un paper. Esto hace mucho sentido, ya que con esto realmente nos ensuciamos las manos con los algoritmos, ponemos atención a detalles técnicos, aprendemos nuevas metodologías, mejora sustancialmente nuestros skills técnicos, y finalmente nos ayuda a leer mejor los papers. Incluso en muchas publicaciones de trabajo en las big tech es un requisito</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/interview.png" class="img-fluid figure-img"></p>
<figcaption>AI Post</figcaption>
</figure>
</div>
<p>Personalmente, a veces navego por Kaggle viendo soluciones, donde muchas veces los ganadores utilizan soluciones hechas “a mano”. Con esto me refiero a no simplemente llamar un modelo y ejecutar el <code>.fit()</code> , si no que crear la estructura de tu modelo desde 0, e incluso innovar en la rutina de entrenamiento. En ocasiones, los puntitos de performance que se gana con esto hace la diferencia en el Leaderboard.</p>
<p>La ofuscación me recorre cuando me doy cuenta que yo no sería capaz de implementar algo así (siempre lo pienso sin siquiera intentarlo). Es por esto que me decidí a pasar por este proceso, y me propuse implementar un paper que me llame la atención, y apuntar a obtener los mismos resultados (caso ideal).</p>
<p>En este post, no quiero sólo mostrar la solución final, porque siento que eso desalienta al lector, haciéndolo creer que llegué a la solución al primer intento. En esta ocasión, mostraré la mayoría de los desafíos por los que tuve que pasar, los éxitos y los fracasos; el paso a paso de como llegué a lo que sería mi solución final.</p>
</section>
<section id="tool-path-que-utilizaremos" class="level1">
<h1>🔨&nbsp;Tool Path: Que utilizaremos</h1>
<p>A continuación les dejo las herramientas que utilizaremos en este post:</p>
<ol type="1">
<li><strong>Pytorch</strong>: es una biblioteca de código abierto para el desarrollo de aplicaciones de aprendizaje profundo y la investigación en inteligencia artificial.</li>
</ol>
</section>
<section id="concept-path-que-aprenderemos" class="level1">
<h1>💭&nbsp;Concept Path: Que aprenderemos</h1>
<p>A continuación algunos de los conceptos que veremos:</p>
<ol type="1">
<li>Generative Adversarial Networks</li>
<li>Convolutional Neural Networks (Próximamente 😢)
<ol type="1">
<li>Downsampling</li>
<li>Upsampling</li>
</ol></li>
</ol>
</section>
<section id="estrategia-como-abordamos" class="level1">
<h1>♟️&nbsp;Estrategia: Como abordamos</h1>
<p>La estrategia fue la siguiente.</p>
<ol type="1">
<li><strong>Vencer el Síndrome del Impostor</strong>: Tarea difícil, por ahora metámonos en la cabeza que nosotros también somos capaces de crear cosas, y que no es tan difícil como lo creemos. Ahora es el momento en que lo comprobamos.</li>
<li><strong>Buscar un Paper para leer</strong>: Acá debemos considerar algunas limitaciones.</li>
<li><strong>Leer el Paper:</strong> Existen algunos tips en la forma de leer un paper.</li>
<li><strong>Implementar</strong>: Acá se encuentra la complejidad técnica.</li>
<li><strong>Ver resultados</strong>: A cruzar los dedos y esperar tener resultados similares 🤞🏽</li>
</ol>
</section>
<section id="prototyping-gan" class="level1">
<h1>🧠&nbsp;Prototyping: GAN</h1>
<p>Comenzaremos con el paper Generative Adversarial Networks. Yo no soy experto en leer papers, existen algunos tips para esto, como el saltarse algunas secciones que pueden no ser muy interesantes o el orden en el cual deberiamos leer, pero todo esto normalmente es para ocasiones en donde estas leyendo para conocer el estado del arte. En este momento, ya que queremos implementar la solución, yo simplemente opté por leer todo el paper (no es tan largo).</p>
<p>A medida lo leía, intentaba entender todo lo necesario para implementarlo, cada vez que no entendía algún término o técnica utilizada, lo googleaba, esto me llevó a aprender muchas cosas nuevas.</p>
<section id="generative-adversarial-networks" class="level2">
<h2 class="anchored" data-anchor-id="generative-adversarial-networks">Generative Adversarial Networks</h2>
<p>A grandes rasgos, en este paper muestran un nuevo camino para la generación de datos. Es importante que sólo le digamos camino, porque no buscan traer la mejor solución que promete superar a todo, si no que remarcan que el objetivo es dar a conocer una forma prometedora en la que se podría generar datos, y que con el tiempo podria traer grandes resultados con ayuda de la comunidad investigadora. No sólo generar imágenes, si no también otros tipos de datos como audio o texto.</p>
<p>Efectivamente, esta metodología marcó un antes y un después para la generación de imágenes, que luego fue superada por los modelos de difusión debido a la inestabilidad y dificultad de entrenamiento que las GAN traen. Aún así, la comunidad mejoró este primer paper trayendo una gran cantidad de GANs al mundo, logrando grandiosos resultados (DCGAN, cGAN, styleGAN, CycleGAN, etc)</p>
<p>En resumen, esta metodología consta de 2 modelos, un <strong>Generador</strong> y un <strong>Discriminador</strong>. La misión del <strong>Generador</strong> es, como lo indica su nombre, generar imágenes (a partir de aquí solo hablaremos de imágenes) que sigan la misma distribución que el conjunto de datos, esto de forma indirecta, ya que en realidad lo entrenamos para que logre engañar al <strong>Discriminador.</strong> El <strong>Discriminador</strong>, por otro lado, es entrenado para clasificar entre imágenes provenientes de la data real e imágenes generadas por el <strong>Generador</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled.png" class="img-fluid figure-img"></p>
<figcaption>Generative Adversarial Networks</figcaption>
</figure>
</div>
<p>En la próxima sección comenzaremos a construir esta metodología.</p>
</section>
<section id="implementando-gan" class="level2">
<h2 class="anchored" data-anchor-id="implementando-gan">👨🏾‍💻&nbsp;Implementando GAN</h2>
<p>Según el paper, para implementar la solución, necesitamos 3 componentes:</p>
<ol type="1">
<li>Generador</li>
<li>Discriminador</li>
<li>Rutina de Entrenamiento</li>
</ol>
<p>El objetivo es construir un modelo que genere imágenes que provengan de la misma distribución que nuestro conjunto de datos (que se parezcan). En esta ocasión utilizaremos el conocido conjunto de datos MNIST, ya que es uno de los utilizados en el paper y es bastante simple de encontrar y utilizar.</p>
<blockquote class="blockquote">
<p>We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database(TFD) [28], and CIFAR-10 [21].</p>
</blockquote>
<p>Acá un vistazo de como luce el conjunto de datos MNIST (son imágenes de números escritos a mano)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 1.png" class="img-fluid figure-img"></p>
<figcaption>MNIST Dataset</figcaption>
</figure>
</div>
<section id="generador" class="level3">
<h3 class="anchored" data-anchor-id="generador">1. Generador</h3>
<p>Leamos que dice el paper sobre esto:</p>
<blockquote class="blockquote">
<p>To learn the generator’s distribution <span class="math inline">\(p_g\)</span> over data <span class="math inline">\(x\)</span>, we define a prior on input noise variables <span class="math inline">\(p_z(z)\)</span>, then represent a mapping to data space as <span class="math inline">\(G(z;\theta_g)\)</span>, where <span class="math inline">\(G\)</span> is a differentiable function represented by a multilayer perceptron with parameters <span class="math inline">\(\theta_g\)</span>.</p>
</blockquote>
<p>Lo que se entiende de acá es que nosotros definiremos una distribución a priori para el input <span class="math inline">\(z\)</span> del generador <span class="math inline">\(G\)</span>. Este ruido <span class="math inline">\(z\)</span> es luego transformado por el Generador (una red neuronal) para obtener una imagen generada que sigue la distribución <span class="math inline">\(p_g\)</span>. Nuestro objetivo entonces es que la distribución <span class="math inline">\(p_g = p_{data}\)</span> , esto también lo podemos concluir de el extracto del paper:</p>
<blockquote class="blockquote">
<p>The generator <span class="math inline">\(G\)</span> implicitly defines a probability distribution <span class="math inline">\(p_g\)</span> as the distribution of the samples <span class="math inline">\(G(z)\)</span> obtained when <span class="math inline">\(z∼p_z\)</span>. Therefore, we would like Algorithm 1 to converge to a good estimator of <span class="math inline">\(p_{data}\)</span>, if given enough capacity and training time.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 2.png" class="img-fluid figure-img"></p>
<figcaption>Generator</figcaption>
</figure>
</div>
<p>No se ven tan complicado, ahora veamos más detalles sobre el generador en si:</p>
<blockquote class="blockquote">
<p>The generator nets used a mixture of rectifier linear activations [19,9] and sigmoid activations …. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network.</p>
</blockquote>
<p>Acá nos topamos con la primera complejidad: No tenemos muchos detalles. Bien podemos ver que no nos dan información sobre cuantos layers tiene el MLP, cuantas neuronas cada layer, ni como transformamos el resultado a una imagen! Esto último era bastante confuso para mi, en primera instancia creia que se utilizaban Convolutional Neural Networks, pero luego me di cuenta que no era necesariamente así.</p>
<p>Creo que el motivo del por qué no hay tanto detalle en el modelo, es que buscan dejarlo abierto a que la comunidad comience a explorar y experimentar con esta arquitectura de solución.</p>
<p>Además del paper, me apoyé de algunos otros blogs donde explicaban este modelo, la mayoría hablaba directamente de Convolutional Neural Networks. Aún así, me quise apegar lo más posible al paper y descarté esta solución por el momento. Incluso el paper sugiere el uso de CNN bien sutilmente cuando escriben lo siguiente en el pie de foto de los resultados</p>
<blockquote class="blockquote">
<ol start="4" type="a">
<li>CIFAR-10 (convolutional discriminator and “deconvolutional” generator)</li>
</ol>
</blockquote>
<p>Ésta era la única pista de que al menos para CIFAR-10 utilizaron redes convolucionales.</p>
<p>Luego de algunas iteraciones, llegué a la conclusión que sería algo como lo siguiente:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 3.png" class="img-fluid figure-img"></p>
<figcaption>Generation of an Image from Noise</figcaption>
</figure>
</div>
<p>La operación sería:</p>
<ol type="1">
<li>Obtenemos <span class="math inline">\(noise\)</span> vector de la distribución a priori <span class="math inline">\(p_z\)</span> de dimensión <span class="math inline">\(latent\_size\)</span> (la distribución a priori suele ser gaussiana o uniforme)</li>
<li>Pasamos el vector por una red neuronal (NN)</li>
<li>Transformamos el vector final proveniente del último layer de la NN a las dimensiones de la imagen ( un simple <code>.reshape(img_size)</code>)</li>
</ol>
<p>Vamos a ver el código ! 🤖🤖🤖</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Noise Vector</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> torch.randn(LATENT_SIZE)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Generator NN</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Generator(nn.Module):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generator model: In charge of generate </span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    real-like images from a noise distribution</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, latent_size, img_size):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Generator, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># layers to use</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(latent_size, <span class="dv">128</span>),  <span class="co"># Nx100</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>,<span class="dv">256</span>), <span class="co"># Nx256</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>),</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>), <span class="co"># Nx512</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">512</span>),</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">1024</span>), <span class="co"># Nx1024</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">1024</span>),</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1024</span>, img_size<span class="op">*</span>img_size), <span class="co"># Nx28*28</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>            nn.Tanh()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.model(x)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Generate Img</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> G(noise.unsqueeze(<span class="dv">0</span>))</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> out.view(IMG_SIZE, IMG_SIZE)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.imshow(img.detach().numpy(), cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Con el modelo sin entrenar, el resultado es algo del estilo:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 4.png" class="img-fluid figure-img"></p>
<figcaption>Generated Image from nontrained Generator</figcaption>
</figure>
</div>
<p>Será muy interesante ver como el modelo evoluciona de tal manera que pueda convertir este ruido en imágenes parecidas al conjunto de datos!</p>
<p><strong>Observaciones 👀</strong></p>
<ol type="1">
<li>Las dimensiones del conjunto de datos MNIST es (28x28x1) → son imágenes en blanco y negro</li>
<li>Notar que la última capa debe tener una dimensión de salida igual a la dimensión de la imagen, esto con el objetivo de poder moldear(reshape) el vector resultante a la imagen. Esto es, el vector de salida debe ser de <span class="math inline">\(28*28*1=784\)</span></li>
<li>Al comienzo utilicé ReLU como función de activación, en el paper dicen: <em>The generator nets used a mixture of rectifier linear activations</em>. Pero luego de algunas iteraciones vi que era mejor LeakyReLU</li>
<li>Al comienzo no utilicé BatchNorm, pero también me di cuenta (navegando por internet) que era importante para este tipo de modelo para estabilidad y convergencia.</li>
<li>La salida de el modelo es pasado por una función <span class="math inline">\(Tanh()\)</span> para que se encuentre en un rango [-1,1]. Las imágenes originales también son normalizadas previamente para que pertenezcan a este rango.</li>
<li>LATENT_SIZE suele ser 100</li>
<li>Importante mencionar que al principio sólo comencé con unas cuantas capas, y fui iterando hasta llegar a la arquitectura mostrada arriba según los resultados y experimentos de otras personas.</li>
<li>En el paper DCGAN (veremos más adelante), mencionan que el modelo se ve beneficiado al utilizar BatchNorm en todas las capas menos en la primera del Generador y la última del Discriminador.</li>
</ol>
</section>
<section id="discriminador" class="level3">
<h3 class="anchored" data-anchor-id="discriminador">2. Discriminador</h3>
<p>Ya tenemos el modelo que generará imagenes y que intentará engañar al Discriminador. El Discriminador es bastante más sencillo, sólo debemos pensar que es un clasificador de imágenes tal y como lo conocemos. Esto es, recibimos una imagen (pixeles), y devolvemos una clase (0 si es fake, 1 si es real)</p>
<p>El paper dice lo siguiente del Discriminador D:</p>
<blockquote class="blockquote">
<p>We also define a second multilayer perceptron <span class="math inline">\(D(x;\theta_d)\)</span> that outputs a single scalar. <span class="math inline">\(D(x)\)</span>represents the probability that <span class="math inline">\(x\)</span> came from the data rather than <span class="math inline">\(p_g\)</span>. We train D to maximize the probability of assigning the correct label to both training examples and samples from G.</p>
</blockquote>
<p>En cúanto a los detalles, sólo tenemos :</p>
<blockquote class="blockquote">
<p>while the discriminator net used maxout [10] activations. Dropout [17] was applied in training the discriminator net.</p>
</blockquote>
<p>Si lo visualizamos, sería algo como:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 5.png" class="img-fluid figure-img"></p>
<figcaption>Discriminator</figcaption>
</figure>
</div>
<p>El proceso es el siguiente:</p>
<ol type="1">
<li>La entrada puede ser una imagen real proveniente del dataset (<span class="math inline">\(p_{data}\)</span>) o proveniente del Generador (<span class="math inline">\(p_{g}\)</span>)</li>
<li>La Imagen entra al Discriminador (NN)</li>
<li>El resultado es una probabilidad [0,1], en donde 0 es imagen fake y 1 real.</li>
</ol>
<p>Es algo así como el proceso contrario de el Generador. Vamos al código!</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#0. Define MaxOut Activation</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MaxOut(nn.Module):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_units, num_pieces):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MaxOut, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_units <span class="op">=</span> num_units</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_pieces <span class="op">=</span> num_pieces</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(num_units, num_units <span class="op">*</span> num_pieces)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape the output to separate pieces</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        maxout_output <span class="op">=</span> <span class="va">self</span>.fc(x).view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_pieces, <span class="va">self</span>.num_units)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Take the maximum value across pieces</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        output, _ <span class="op">=</span> torch.<span class="bu">max</span>(maxout_output, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Imagen o Imagen Generada</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>out <span class="co"># Esto viene del Generador que hicimos arriba (o de el dataset original)</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Discriminador</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Discriminator(nn.Module):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generator model: In charge of classify</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">    images between real and syntetic generated</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">    by the generator </span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_size):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Discriminator, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            nn.Linear(img_size<span class="op">*</span>img_size, <span class="dv">512</span>), <span class="co">#N x 512</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            MaxOut(<span class="dv">512</span>, <span class="dv">4</span>),</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>), <span class="co"># N x 256</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            MaxOut(<span class="dv">256</span>, <span class="dv">4</span>),</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">1</span>), <span class="co"># N x 1</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid()</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.model(x)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Se predice</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> Discriminator()</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> D(out)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>El resultado es simple, será algo como 0.495. Algo así como que es tan probable que sea una imagen real como fake según el discriminador. Al comienzo puede ser así ya que no está entrenado.</p>
<p><strong>Observaciones 👀</strong></p>
<ol type="1">
<li>La entrada es la misma dimensión que la salida del Generador (784)</li>
<li>Acá no utilizamos BatchNorm (aunque si debí hacerlo según el paper DCGAN)</li>
<li>Utilizamos MaxOut activation como lo indica el paper, éste no se encuentra por defecto en Pytorch, por lo que hay que definirlo “a mano”</li>
<li>Olvidé utilizar Dropout como lo recomienda el paper, esto podría mejorar los resultados, al igual que agregar BatchNorm</li>
<li>Al igual que en el Generador, la arquitectura se fue iterando según resultados.</li>
</ol>
</section>
<section id="rutina-de-entrenamiento" class="level3">
<h3 class="anchored" data-anchor-id="rutina-de-entrenamiento">3. Rutina de Entrenamiento</h3>
<p>Una vez tenemos el Generador y el Discriminador, sólo nos queda entrenar. Según el paper (lo que entendí), no lo hacen de la forma tradicional (iterando sobre el dataset original). Ellos proponen lo siguiente:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 6.png" class="img-fluid figure-img"></p>
<figcaption>GAN Training Algorithm</figcaption>
</figure>
</div>
<p>Proponen, en cada iteración, primero optimizar el Discriminador por K pasos:</p>
<ol type="1">
<li>Extraen un minibatch de imagenes reales</li>
<li>Extraen un minibatch de imagenes generadas por G</li>
<li>Hacen un update en base a Gradient Ascent</li>
</ol>
<p>Luego, optimizan el Generador en un sólo paso:</p>
<ol type="1">
<li>Extrae un minibatch de imagenes generadas por G</li>
<li>Hacen un update de los parámetros en base a Gradient Descend</li>
</ol>
<p>Según indican en el paper, updatean más veces el Discriminador, ya que buscan que el Discriminador se mantenga cercano al óptimo mientras el Generador se va actualizando lentamente hasta converger.</p>
<blockquote class="blockquote">
<p>This results in D being maintained near its optimal solution, so long as G changes slowly enough.</p>
</blockquote>
<p>Lo que vemos acá, es que el generador actualiza sus parámetros utilizando información implicita entregada por el Discriminador. Optimizamos el Generador para lograr engañar al Discriminador.</p>
<p>Algo importante a poner atención, es la función objetivo, que tenemos 2, la del discriminador y la del generador, vamos a ver esto con más detalle.</p>
<hr>
<p><strong>Loss Functions 📎</strong></p>
<p><strong>Discriminador</strong></p>
<p>Recordemos que el Discriminador <span class="math inline">\(D\)</span> es un clasificador común, por lo que podemos utilizar la función común para estos casos (Cross Entropy Loss). Desentrañemos esto hasta llegar a lo que tienen en el paper:</p>
<p>Queremos clasificar correctamente los positivos (imagenes reales) y negativos (imagenes generadas), la función objetivo Binary Cross Entropy está dada por:</p>
<p><span class="math display">\[
-\dfrac{1}{m} \sum{y_ilog(\hat{y_i}) + (1-y_i)log(1-\hat{y_i})}
\]</span></p>
<p>donde <span class="math inline">\(y_i\)</span> es el label (0 o 1) de la imagen i ; <span class="math inline">\(\hat{y_i}\)</span> es la predicción de la imagen i, fake o real ; <span class="math inline">\(m\)</span> es el tamaño del minibatch</p>
<p>Notemos que</p>
<ul>
<li>cuando <span class="math inline">\(y_i = 1\)</span>, la imagen <span class="math inline">\(x_i\)</span> es real , osea que <span class="math inline">\(\hat{y_i} = D(x_i)\)</span> y lo de la derecha es 0 en la ecuación</li>
<li>cuando <span class="math inline">\(y_i=0\)</span>, la imagen <span class="math inline">\(G(z_i)\)</span> es fake ,osea que <span class="math inline">\(\hat{y_i} = D(G(z_i))\)</span> y lo de la izquierda es 0 en la ecuación</li>
</ul>
<p>Es por esto que podemos reducir la ecuación a:</p>
<p><span class="math display">\[
Min -\dfrac{1}{m} \sum{ log(D(x_i)) + log(1-D(G(z_i)))}
\]</span></p>
<p>Si bien esto se puede traducir a que queremos maximizar</p>
<p><span class="math display">\[
Max \dfrac{1}{m} \sum{ log(D(x_i)) + log(1-D(G(z_i)))}
\]</span></p>
<p>tal como sale en el paper (sin el signo negativo), por conveniencia no lo haremos y optaremos por minimizar el Binary Cross Entropy y asi el código se reduce a utilizar la función ya hecha en Pytorch <code>BCELoss()</code> quedando algo como</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>BCELoss([D(x), D(G(z))], [<span class="dv">1</span>, <span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Otras soluciones optan por optimizarlas por separado, o por promediar ambos. En mi caso las concatené junto con sus targets.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>💡
</div>
</div>
<div class="callout-body-container callout-body">
<p>Maximizar una función Z es equivalente a Minimizar -Z</p>
</div>
</div>
<p><strong>Generador</strong></p>
<p>Para el generador, es similar, sólo que ahora sólo le entregamos imágenes falsas y además queremos que <span class="math inline">\(D\)</span> se equivoque. Por lo tanto, maximizamos la función de pérdida Binary Cross Entropy (que se equivoque), y sólo observamos cuando <span class="math inline">\(y_i = 0\)</span> (sólo imágenes falsas), teniendo</p>
<p><span class="math display">\[
Max -\dfrac{1}{m} \sum{  log(1-D(G(z_i)))}
\]</span></p>
<p>Ahora lo llevamos a minimizar por conveniencia en código (en Pytorch por defecto se busca minimizar la función objetivo)</p>
<p><span class="math display">\[
Min \dfrac{1}{m} \sum{  log(1-D(G(z_i)))}
\]</span></p>
<p>obteniendo lo que aparece en el paper. En código sería algo como</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">-</span>BCELoss(D(G(z)), <span class="dv">0</span>) <span class="co"># El signo - es porque implicitamente maximizamos BCE</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
<p>Fuaaa, cuanta matemática 🤯🤯🤯, comprobemos esto de forma intuitiva:</p>
<p>Para el <strong>Discriminador</strong> tenemos entonces:</p>
<p><span class="math display">\[
Max \dfrac{1}{m} \sum{ log(D(x_i)) + log(1-D(G(z_i)))}
\]</span></p>
<p>si lo separamos, queremos entonces:</p>
<p><span class="math display">\[
Max \dfrac{1}{m} \sum{ log(D(x_i)) }
\]</span></p>
<p><span class="math display">\[
Max \dfrac{1}{m} \sum{ log(1-D(G(z_i)) }
\]</span></p>
<p>La primera función (olvidando la suma y eso) es <span class="math inline">\(ln(x)\)</span> → si, la base utilizada es la exponencial, por lo que es el logaritmo natural. Su gráfica es:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Notes_240225_163331.jpg" class="img-fluid figure-img"></p>
<figcaption>ln(x)</figcaption>
</figure>
</div>
<p>El eje horizontal es <span class="math inline">\(D(x_i)\)</span>: <em>La probabilidad de que la imagen real <span class="math inline">\(x_i\)</span> sea clasificada como real</em>, con un rango [0,1]. Ya que la imagen es real, queremos que D prediga un valor alto (ojalá 1), lo que si vemos la gráfica es igual a alcanzar el máximo de la función (<code>máx optimo</code>). Es por esto que maximizamos <span class="math inline">\(\dfrac{1}{m} \sum{ log(D(x_i)) }\)</span></p>
<p>Ahora vamos con <span class="math inline">\(log(1-D(G(z_i))\)</span> , cuya gráfica es:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 7.png" class="img-fluid figure-img"></p>
<figcaption>ln(1-x) Discriminador</figcaption>
</figure>
</div>
<p>Ahora la gráfica cambia, el eje horizontal está dado por <span class="math inline">\(D(G(z_i))\)</span>: <em>La probabilidad de que la imagen falsa <span class="math inline">\(G(z_i)\)</span> sea clasificada como <strong>real. V</strong></em>emos que la función se maximiza cuando <span class="math inline">\(D(G(z_i))\)</span> se acerca a 0, y esto es lo que queremos porque <span class="math inline">\(G(z_i)\)</span> es una imagen falsa y debe ser clasificada como tal en el Discriminador (ojalá 0). Es por esto que buscamos maximizar <span class="math inline">\(\dfrac{1}{m} \sum{ log(1-D(G(z_i)) }\)</span></p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>🔥
</div>
</div>
<div class="callout-body-container callout-body">
<p>IMPORTANTE! Acá vemos la función objetivo tal y como está en el paper (maximizando), pero en el código, por conveniencia, lo llevamos a minimizar la BCELoss, lo cual es equivalente, tal como lo mostramos en la parte matemática. Ésto en los gráficos es parecido sólo que ahora buscamos minimizar ambas partes, dejaré este ejercicio como tarea para el lector!</p>
</div>
</div>
<p>Para el <strong>Generador</strong>, es muy similar a la segunda ecuación del Discriminador, sólo que ahora buscamos minimizar (ir hacia a la derecha).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 8.png" class="img-fluid figure-img"></p>
<figcaption>ln(1-x) Generador</figcaption>
</figure>
</div>
<p>Vemos que la idea es ir hacia la derecha porque queremos que el Discriminador se equivoque, o sea que aunque le entreguemos una imagen falsa <span class="math inline">\(D(G(z_i))\)</span>, el discriminador prediga que es real (se acerque a 1). Si vemos bien, a medida <span class="math inline">\(D(G(z_i))\)</span> se acerca a 1, la función <span class="math inline">\(\dfrac{1}{m} \sum{  log(1-D(G(z_i)))}\)</span> se acerca a Esto puede provocar inestabilidad en la optimización, y es por esto que mucha gente utiliza la equivalencia:</p>
<p><span class="math display">\[
Min \dfrac{1}{m} \sum{ log(D(G(z_i))) }
\]</span></p>
<p>como función objetivo de el Generador. Acá vemos que utilizamos la parte donde <span class="math inline">\(y_i = 1\)</span> , osea que la imagen es real, pero minimizamos haciendo que el Discriminador diga que es falsa. Intuitivamente se pierde un poco el sentido ya que en realidad la imagen entregada no es real, pero matemáticamente es equivalente y logra mayor estabilidad y al minimizar el óptimo es 0 y no <span class="math inline">\(-\infin\)</span> (aún así yo utilicé la primera forma)</p>
<p>Espero esta sección se haya entendido, es un tanto complicado de escribir con palabras. El proceso de entendimiento de la función objetivo, fue un proceso bastante entretenido e interesante, que sin duda me sirvió para mejorar algunas aptitudes a la hora de leer papers. Aún así mencionar que es importante apoyarse en la literatura (libros, blogs, papers), el objetivo no es comprobar que tu podrías haber llegado a las mismas conclusiones que los autores por tu lado, si no que eres capaz de entender (matemática- o intuitiva- mente) lo que se propone. Invito al lector a iniciar una conversación conmigo si le quedó alguna duda, o si notó que pude haberme equivocado en algo.</p>
<hr>
<p><strong>Train</strong> 🏋🏾</p>
<p>Ahora ya podemos comenzar a entrenar el modelo, primero vamos a inicializar algunos componentes importantes:</p>
<ol type="1">
<li>Configuración</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CFG</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>latent_size <span class="op">=</span> <span class="dv">100</span> <span class="co"># noise dimension</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>img_size <span class="op">=</span> <span class="dv">28</span> <span class="co"># image shape</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="co"># GPU</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20000</span> <span class="co"># TRAINING ITERATIONS</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">1</span> <span class="co"># Discriminator steps</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol type="1">
<li><strong>Modelos:</strong> Utilizamos los modelos construidos anteriormente</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Models </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> Generator(cfg.latent_dim, cfg.img_size)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> Discriminator(cfg.img_size)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>G.to(cfg.device)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>D.to(cfg.device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol type="1">
<li><strong>Loss Functions</strong>: Recordemos que gracias a que re-definimos la matemática de las funciones objetivos, podemos ocupar la función pre-construida en Pytorch <code>BCELoss()</code></li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Losses</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>D_LOSS <span class="op">=</span> nn.BCELoss()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>G_LOSS <span class="op">=</span> nn.BCELoss()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol type="1">
<li><strong>Optimizadores</strong>: Utilizamos Adam, con un learning rate bastante más pequeño que el usual y unos betas específicos. Esto es uno de los problemas de GAN, es muy sensible a los hyperparámetros, cuando normalmente los modelos (Deeplearning) son robustos a estos.</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizers</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>d_optimizer <span class="op">=</span> torch.optim.Adam(D.parameters(), lr <span class="op">=</span> <span class="fl">0.0002</span>, betas <span class="op">=</span> (<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>g_optimizer <span class="op">=</span> torch.optim.Adam(G.parameters(), lr <span class="op">=</span> <span class="fl">0.0002</span>, betas <span class="op">=</span> (<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol type="1">
<li><strong>Dataset</strong>: Ya que MNIST es bastante conocido, ya se encuentra disponible en Pytorch para descargar</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Datasets (Images and Noise)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((cfg.img_size, cfg.img_size)),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),  <span class="co"># Convert PIL image or numpy array to tensor</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.5</span>,), (<span class="fl">0.5</span>,))  <span class="co"># Normalize the tensor with mean and standard deviation</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> MNIST(root <span class="op">=</span> <span class="st">''</span>, download <span class="op">=</span> <span class="va">True</span>, transform <span class="op">=</span>transform )</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>sampler <span class="op">=</span> RandomSampler(dataset) <span class="co"># To get random images each iteration</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># DataLoader</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>original_dl <span class="op">=</span> DataLoader(dataset, batch_size <span class="op">=</span> cfg.batch_size, sampler <span class="op">=</span> sampler, pin_memory<span class="op">=</span>torch.cuda.is_available())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Notar que acá normalizamos las imágenes para que estén en el rango [-1,1] y nos aseguramos que tengan el mismo tamaño 28x28. También, ya que no iremos avanzando batch por batch, iremos sampleando aleatoriamente el dataset tal como lo dice la rutina de entrenamiento pertenciente al paper.</p>
<ol type="1">
<li>Rutina de entrenamiento: Iremos sentencia por sentencia navegando la rutina de entrenamiento y codeando!</li>
</ol>
<blockquote class="blockquote">
<p><em>for number of training iterations do</em></p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(cfg.epochs):</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p>for k steps do</p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(cfg.epochs):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(cfg.k):</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p>Sample minibatch of m noise samples {z(1), . . . , z(m)} from noise prior <span class="math inline">\(p_g(z)\)</span>.</p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(cfg.epochs):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(cfg.k):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># noise minibatch </span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> torch.randn(batch_size, cfg.latent_dim)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p>Sample minibatch of m examples {x(1), . . . , x(m)} from data generating distribution <span class="math inline">\(p_{data}(x)\)</span></p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(cfg.epochs):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(cfg.k):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># noise minibatch </span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> torch.randn(batch_size, cfg.latent_dim)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># original minibatch</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        x, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(original_dl))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p>Update the discriminator by ascending its stochastic gradient</p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(cfg.epochs):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(cfg.k):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># noise minibatch </span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> torch.randn(batch_size, cfg.latent_dim)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># original minibatch</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        x, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(original_dl))</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">##############################</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Discriminator Optimization </span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">##############################</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        d_optimizer.zero_grad()</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        G_z <span class="op">=</span> G(z) <span class="co"># Generated Image</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        D_x  <span class="op">=</span> D(x) <span class="co"># real image's probability of being real</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        D_G_z <span class="op">=</span> D(G_z.detach()) <span class="co"># fake image's probability of being real</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concat real and fakes</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        samples <span class="op">=</span> torch.cat([D_G_z, D_x]).to(cfg.device)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make targets (1 for real, 0 for fakes)</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> torch.cat([torch.zeros(D_G_z.size()[<span class="dv">0</span>]), torch.ones(D_x.size()[<span class="dv">0</span>])]).to(cfg.device)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Discriminator Loss</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loss</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        d_loss <span class="op">=</span> D_LOSS(samples, targets.unsqueeze(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        d_loss.backward() <span class="co"># backward</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adjust learning weights</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        d_optimizer.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p>End for. Sample minibatch of m noise samples {z(1), . . . , z(m)} from noise prior <span class="math inline">\(p_g(z)\)</span>.</p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(cfg.epochs)):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(cfg.k):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># noise minibatch </span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> torch.randn(batch_size, cfg.latent_dim)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># original minibatch</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        x, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(original_dl))</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">##############################</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Discriminator Optimization </span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">##############################</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        d_optimizer.zero_grad()</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        G_z <span class="op">=</span> G(z) <span class="co"># Generated Image</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        D_x  <span class="op">=</span> D(x) <span class="co"># real image's probability of being real</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        D_G_z <span class="op">=</span> D(G_z.detach()) <span class="co"># fake image's probability of being real</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concat real and fakes</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        samples <span class="op">=</span> torch.cat([D_G_z, D_x]).to(cfg.device)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># make targets (1 for real, 0 for fakes)</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> torch.cat([torch.zeros(D_G_z.size()[<span class="dv">0</span>]), torch.ones(D_x.size()[<span class="dv">0</span>])]).to(cfg.device)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Discriminator Loss</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loss</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        d_loss <span class="op">=</span> D_LOSS(samples, targets.unsqueeze(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        d_loss.backward() <span class="co"># backward</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adjust learning weights</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        d_optimizer.step()</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Minibatch from noise prior</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> torch.randn(batch_size, cfg.latent_dim).to(cfg.device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p>Update the generator by descending its stochastic gradient:</p>
</blockquote>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(cfg.epochs)):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(cfg.k):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># noise minibatch </span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> torch.randn(batch_size, cfg.latent_dim)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># original minibatch</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        x, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(original_dl))</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">##############################</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Discriminator Optimization </span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">##############################</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        d_optimizer.zero_grad()</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        G_z <span class="op">=</span> G(z) <span class="co"># Generated Image</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        D_x  <span class="op">=</span> D(x) <span class="co"># real image's probability of being real</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        D_G_z <span class="op">=</span> D(G_z.detach()) <span class="co"># fake image's probability of being real</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concat real and fakes</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        samples <span class="op">=</span> torch.cat([D_G_z, D_x]).to(cfg.device)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># make targets (1 for real, 0 for fakes)</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> torch.cat([torch.zeros(D_G_z.size()[<span class="dv">0</span>]), torch.ones(D_x.size()[<span class="dv">0</span>])]).to(cfg.device)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Discriminator Loss</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loss</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        d_loss <span class="op">=</span> D_LOSS(samples, targets.unsqueeze(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        d_loss.backward() <span class="co"># backward</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adjust learning weights</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        d_optimizer.step()</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Minibatch from noise prior</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> torch.randn(batch_size, cfg.latent_dim).to(cfg.device)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">##############################</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Generator Optimization </span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">##############################</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    g_optimizer.zero_grad()</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    G_z <span class="op">=</span> G(z) <span class="co"># Generated Images</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>    D_G_z <span class="op">=</span> D(G_z) <span class="co"># fake/generated image's probability of being real</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># targets</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> torch.zeros(D_G_z.size()[<span class="dv">0</span>]).to(cfg.device)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loss</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    g_loss <span class="op">=</span> <span class="op">-</span>G_LOSS(D_G_z, targets.unsqueeze(<span class="op">-</span><span class="dv">1</span>)) <span class="co"># Max BCE</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    g_loss.backward()</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adjust learning weights</span></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>    g_optimizer.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Y con eso ya estamos! a correr y a observar los resultados, a continuación algunas observaciones:</p>
<ol type="1">
<li>También se puede hacer el entrenamiento común (minibatch stochastic gradient descend )</li>
<li>Vi que en algunos lados utilizan el mismo noise <span class="math inline">\(z\)</span> para ambas partes de la optimización</li>
<li>En la linea <code>D_G_z = D(G_z.detach())</code> , el .detach() es para no actualizar parametros del Generador en la parte del Discriminador, por lo que quitamos G_z de del grafo computacional.</li>
<li>Entrené por 20.000 iteraciones! se demoró aprox 15 minutos. Notemos que no son epochs (pasadas por el training set), si no que sólo iteraciones del algoritmo.</li>
</ol>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>⚠️
</div>
</div>
<div class="callout-body-container callout-body">
<p>WARNING: Si quieres utilizar el código, te recomiendo verlo directamente desde Github ya que acá quité algunas cosas para que se pueda entender mejor</p>
</div>
</div>
</section>
<section id="resultados" class="level3">
<h3 class="anchored" data-anchor-id="resultados">4. Resultados</h3>
<p>Lo primero a observar es la curva de aprendizaje, que no es tan bonita</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 9.png" class="img-fluid figure-img"></p>
<figcaption>Learning Curve GAN</figcaption>
</figure>
</div>
<p>Podemos ver lo inestable que es el entrenamiento, aunque si tiende a minimizar ambas funciones. Ahora vemos que imágenes genera:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 10.png" class="img-fluid figure-img"></p>
<figcaption>Sample Results</figcaption>
</figure>
</div>
<p>Fuaaa 🤩&nbsp;Parecer ser que el modelo intenta converger y genera algunas imágenes que si podrian engañar al ojo humano.</p>
<p>Si te lo estás preguntando, la evaluación real propuesta en el paper no es simple, en el paper utilizan unas técnicas específicas, pero para dejarlo simple, acá simplemente evaluaremos con la vista!</p>
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 11.png" class="img-fluid"></p>
<p>Algo importante es que idealmente en generación de imágenes no se aprenda de memoria las características de el conjunto de entrenamiento, si no que le agregue un poco de su sazón! e.g al generar caras, en donde el set de entrenamiento no tienen ninguna cara con barba, nos gustaría que pudiese generar caras con barba (esto es una limitante)</p>
<p>Veamos como va evolucionando el modelo cada 1000 iteraciones!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 12.png" class="img-fluid figure-img"></p>
<figcaption>Iteración 0</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 13.png" class="img-fluid figure-img"></p>
<figcaption>Iteración 2000</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 14.png" class="img-fluid figure-img"></p>
<figcaption>Iteración 1000</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 15.png" class="img-fluid figure-img"></p>
<figcaption>Iteración 3000</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 16.png" class="img-fluid figure-img"></p>
<figcaption>Iteración 4000</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 17.png" class="img-fluid figure-img"></p>
<figcaption>Iteración 15000</figcaption>
</figure>
</div>
<p>Observamos que ya a partir de la iteración 4000 logra hacer un “6” bastante decente.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>📝
</div>
</div>
<div class="callout-body-container callout-body">
<p>PD: Mis disculpas por no haber hecho un plot más ordenado acá!</p>
</div>
</div>
</section>
</section>
</section>
<section id="dcgan" class="level1">
<h1>DCGAN</h1>
<p>Mientras construiamos esto, nos preguntábamos si podiamos utilizar CNNs en vez de MLPs. En el Paper <strong><a href="https://arxiv.org/abs/1511.06434">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a> lo hacen!</strong></p>
<p>Me encantaría incluir la explicación de como implementé este paper también (me gustó más porque incluyen todos los detalles de arquitectura), pero se está alargando mucho el post 😢. Lo dejaré para un futuro, aún así puedes ver el código y notebook en el Github. Para no dejarte con las ganas, te dejaré con los resultados obtenidos, los cuales no son tan mejores en cuánto al dataset MNIST, pareciera que son predicciones más suaves, pero debo mencionar que acá utilicé un <code>img_size</code> de 64x64</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 18.png" class="img-fluid figure-img"></p>
<figcaption>MNIST Generations By DCGAN</figcaption>
</figure>
</div>
<p>Me gustan más las generaciones hechas por GAN tradicional (Vanilla). Quizá pude haber mejorado más el modelo, o haber iterado por más tiempo. Aún así, creo que la ventaja de DCGAN es que permite generar tipos de imágenes más complejas. Dicho esto, probé con el dataset CELEB, que son imágenes de celebridades y estos fueron los resultados:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GAN Paper Implementation 52ba99d208bf45dfb1a22f2077ec3f77/Untitled 19.png" class="img-fluid figure-img"></p>
<figcaption>CELEB Generations by DCGAN</figcaption>
</figure>
</div>
<p>Yo veo que es un buen intento! Pude haber seguido iterando para encontrar mejores resultados, pero creo que por ahora es bastante decente! (Y me quedé sin unidades de cómputo 💸💸💸)</p>
<p>Los GANs posteriores crean imágenes bastantes más realistas! Y para que hablar de los modelos de difusión y los avances que se han hecho hasta la fecha. 🤯🤯🤯🤯</p>
</section>
<section id="conclusiones" class="level1">
<h1>Conclusiones</h1>
<p>La implementación del paper sobre Adversarial Neural Networks (GAN) ha demostrado ser una herramienta poderosa para la generación de datos realistas y la mejora de la calidad en diversas aplicaciones. La capacidad de generar contenido nuevo y convincente a través de la competencia entre el generador y el discriminador abre un mundo de posibilidades creativas. A medida que continuamos explorando y refinando estas técnicas, podemos anticipar avances significativos en campos como la visión por computadora, el diseño de imágenes y la generación de contenido multimedia. ¡El futuro de las GAN promete emocionantes desarrollos en la generación de contenido artificial!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/diegulio\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>diegulio🧡2025</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>