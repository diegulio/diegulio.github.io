---
title: "Paper Implementation Series: Generative Adversarial Networks"
image: "paper.jpeg"
author: "Diegulio"
categories: [python, pytorch, paper]
subtitle: "Implementamos el paper Generative Adversarial Networks"
date: "2024-02-28"
---

[![](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/diegulio/paper_implementations)

# üìù T√≥pico: GAN Paper Implementation

En este post, implementaremos un paper (SPOILER: en realidad ser√°n 2) desde 0, esto con el fin de mostrar todo lo que se puede aprender de este proyecto y motivar al lector a intentarlo.  Intentar√© narrar este post como un ‚Äúdiario de vida‚Äù, mostrando los desaf√≠os que super√©, y los que no. Los papers a implementar en esta ocasi√≥n ser√°n: 

- [**Generative Adversarial Networks**](https://arxiv.org/abs/1406.2661)
- **[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)**

Eleg√≠ el primero ya que evalu√© que era un paper totalmente implementable desde el punto de vista t√©cnico, era un modelo y algoritmo ‚Äúsimple‚Äù de implementar y no se necesitar√≠a mucho c√≥mputo. El segundo lo implement√© s√≥lo con la intenci√≥n de mejorar los resultados del primero, y con el objetivo de seguir aprendiendo! 

::: {.callout-note icon=false}
### ‚ö†Ô∏è
No me centrar√© demasiado en aspectos t√©cnicos de alto nivel, como el uso b√°sico de Pytorch. El lector puede consultar internet o mi post **[Image Classification with Pytorch Lightning](https://diegulio.github.io/posts/pytorch_breed_classification/main.html)** si est√° interesado

:::

::: {.callout-note icon=false}
### üõë
DISCLAIMER: En este paper no muestro **c√≥mo implementar** un paper, si no m√°s bien como lo implement√© yo, desde mi conocimiento, por lo que me disculpo de antemano si algo de lo que habr√© hecho fue ineficiente o incorrecto y agradecer√© un mont√≥n si me informan cualquier sugerencia! ‚ù§Ô∏è
:::

# Motivaci√≥n: Implementar un Paper

Una muy buena pr√°ctica que grandes mentes en tecnolog√≠a normalmente recomiendan es implementar un paper. Esto hace mucho sentido, ya que con esto realmente nos ensuciamos las manos con los algoritmos, ponemos atenci√≥n a detalles t√©cnicos, aprendemos nuevas metodolog√≠as, mejora sustancialmente nuestros skills t√©cnicos, y finalmente nos ayuda a leer mejor los papers. Incluso en muchas publicaciones de trabajo en las big tech es un requisito

![AI Post](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/interview.png)

Personalmente, a veces navego por Kaggle viendo soluciones, donde muchas veces los ganadores utilizan soluciones hechas ‚Äúa mano‚Äù. Con esto me refiero a no simplemente llamar un modelo y ejecutar el `.fit()` , si no que crear la estructura de tu modelo desde 0, e incluso innovar en la rutina de entrenamiento. En ocasiones, los puntitos de performance que se gana con esto hace la diferencia en el Leaderboard. 

La ofuscaci√≥n me recorre cuando me doy cuenta que yo no ser√≠a capaz de implementar algo as√≠ (siempre lo pienso sin siquiera intentarlo). Es por esto que me decid√≠ a pasar por este proceso, y me propuse implementar un paper que me llame la atenci√≥n, y apuntar a obtener los mismos resultados (caso ideal). 

En este post, no quiero s√≥lo mostrar la soluci√≥n final, porque siento que eso desalienta al lector, haci√©ndolo creer que llegu√© a la soluci√≥n al primer intento. En esta ocasi√≥n, mostrar√© la mayor√≠a de los desaf√≠os por los que tuve que pasar, los √©xitos y los fracasos; el paso a paso de como llegu√© a lo que ser√≠a mi soluci√≥n final.

# üî®¬†Tool Path: Que utilizaremos

A continuaci√≥n les dejo las herramientas que utilizaremos en este post:

1. **Pytorch**:  es una biblioteca de c√≥digo abierto para el desarrollo de aplicaciones de aprendizaje profundo y la investigaci√≥n en inteligencia artificial.

# üí≠¬†Concept Path: Que aprenderemos

A continuaci√≥n algunos de los conceptos que veremos: 

1. Generative Adversarial Networks
2. Convolutional Neural Networks (Pr√≥ximamente üò¢)
    1. Downsampling
    2. Upsampling

# ‚ôüÔ∏è¬†Estrategia: Como abordamos

La estrategia fue la siguiente.

1. **Vencer el S√≠ndrome del Impostor**: Tarea dif√≠cil, por ahora met√°monos en la cabeza que nosotros tambi√©n somos capaces de crear cosas, y que no es tan dif√≠cil como lo creemos. Ahora es el momento en que lo comprobamos.
2. **Buscar un Paper para leer**: Ac√° debemos considerar algunas limitaciones.
3. **Leer el Paper:** Existen algunos tips en la forma de leer un paper.
4. **Implementar**: Ac√° se encuentra la complejidad t√©cnica.
5. **Ver resultados**: A cruzar los dedos y esperar tener resultados similares ü§ûüèΩ

# üß†¬†Prototyping: GAN

Comenzaremos con el paper Generative Adversarial Networks.  Yo no soy experto en leer papers, existen algunos tips para esto, como el saltarse algunas secciones que pueden no ser muy interesantes o el orden en el cual deberiamos leer, pero todo esto normalmente es para ocasiones en donde estas leyendo para conocer el estado del arte. En este momento, ya que queremos implementar la soluci√≥n, yo simplemente opt√© por leer todo el paper (no es tan largo). 

A medida lo le√≠a, intentaba entender todo lo necesario para implementarlo, cada vez que no entend√≠a alg√∫n t√©rmino o t√©cnica utilizada, lo googleaba, esto me llev√≥ a aprender muchas cosas nuevas. 

## Generative Adversarial Networks

A grandes rasgos, en este paper muestran un nuevo camino para la generaci√≥n de datos. Es importante que s√≥lo le digamos camino, porque no buscan traer la mejor soluci√≥n que promete superar a todo, si no que remarcan que el objetivo es dar a conocer una forma prometedora en la que se podr√≠a generar datos, y que con el tiempo podria traer grandes resultados con ayuda de la comunidad investigadora.  No s√≥lo generar im√°genes, si no tambi√©n otros tipos de datos como audio o texto.  

Efectivamente, esta metodolog√≠a marc√≥ un antes y un despu√©s para la generaci√≥n de im√°genes, que luego fue superada por los modelos de difusi√≥n debido a la inestabilidad y dificultad de entrenamiento que las GAN traen. A√∫n as√≠, la comunidad mejor√≥ este primer paper trayendo una gran cantidad de GANs al mundo, logrando grandiosos resultados (DCGAN, cGAN, styleGAN, CycleGAN, etc)

En resumen, esta metodolog√≠a consta de 2 modelos, un **Generador** y un **Discriminador**. La misi√≥n del **Generador** es, como lo indica su nombre, generar im√°genes (a partir de aqu√≠ solo hablaremos de im√°genes) que sigan la misma distribuci√≥n que el conjunto de datos, esto de forma indirecta, ya que en realidad lo entrenamos para que logre enga√±ar al **Discriminador.** El **Discriminador**, por otro lado, es entrenado para clasificar entre im√°genes provenientes de la data real e im√°genes generadas por el **Generador**. 

![Generative Adversarial Networks](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled.png)


En la pr√≥xima secci√≥n comenzaremos a construir esta metodolog√≠a.

## üë®üèæ‚Äçüíª¬†Implementando GAN

Seg√∫n el paper, para implementar la soluci√≥n, necesitamos 3 componentes:

1. Generador
2. Discriminador
3. Rutina de Entrenamiento

El objetivo es construir un modelo que genere im√°genes que provengan de la misma distribuci√≥n que nuestro conjunto de datos (que se parezcan). En esta ocasi√≥n utilizaremos el conocido conjunto de datos MNIST, ya que es uno de los utilizados en el paper y es bastante simple de encontrar y utilizar.

> We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database(TFD) [28], and CIFAR-10 [21].
> 

Ac√° un vistazo de como luce el conjunto de datos MNIST (son im√°genes de n√∫meros escritos a mano)

![MNIST Dataset](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%201.png)



### 1. Generador

Leamos que dice el paper sobre esto: 

> To learn the generator‚Äôs distribution $p_g$ over data $x$, we define a prior on input noise variables $p_z(z)$, then represent a mapping to data space as $G(z;\theta_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $\theta_g$.
> 

Lo que se entiende de ac√° es que nosotros definiremos una distribuci√≥n a priori para el input  $z$ del generador $G$. Este ruido $z$  es luego transformado por el Generador (una red neuronal) para obtener una imagen generada que sigue la distribuci√≥n $p_g$. Nuestro objetivo entonces es que la distribuci√≥n $p_g = p_{data}$ , esto tambi√©n lo podemos concluir de el extracto del paper:

> The generator $G$ implicitly defines a probability distribution $p_g$ as the distribution of the samples $G(z)$ obtained when $z‚àºp_z$. Therefore, we would like Algorithm 1 to converge to a good estimator of $p_{data}$, if given enough capacity and training time.
> 

![Generator](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%202.png)



No se ven tan complicado, ahora veamos m√°s detalles sobre el generador en si:

> The generator nets used a mixture of rectifier linear activations [19,9] and sigmoid activations ‚Ä¶. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network.
> 

Ac√° nos topamos con la primera complejidad: No tenemos muchos detalles. Bien podemos ver que no nos dan informaci√≥n sobre cuantos layers tiene el MLP, cuantas neuronas cada layer, ni como transformamos el resultado a una imagen! Esto √∫ltimo era bastante confuso para mi, en primera instancia creia que se utilizaban Convolutional Neural Networks, pero luego me di cuenta que no era necesariamente as√≠. 

Creo que el motivo del por qu√© no hay tanto detalle en el modelo, es que buscan dejarlo abierto a que la comunidad comience a explorar y experimentar con esta arquitectura de soluci√≥n. 

Adem√°s del paper, me apoy√© de algunos otros blogs donde explicaban este modelo, la mayor√≠a hablaba directamente de Convolutional Neural Networks. A√∫n as√≠, me quise apegar lo m√°s posible al paper y descart√© esta soluci√≥n por el momento. Incluso el paper sugiere el uso de CNN bien sutilmente cuando escriben lo siguiente en el pie de foto de los resultados

> d) CIFAR-10 (convolutional discriminator
and ‚Äúdeconvolutional‚Äù generator)
> 

√âsta era la √∫nica pista de que al menos para CIFAR-10 utilizaron redes convolucionales. 

Luego de algunas iteraciones, llegu√© a la conclusi√≥n que ser√≠a algo como lo siguiente:

![Generation of an Image from Noise](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%203.png)

La operaci√≥n ser√≠a:

1. Obtenemos $noise$  vector de la distribuci√≥n a priori $p_z$ de dimensi√≥n $latent\_size$ (la distribuci√≥n a priori suele ser gaussiana o uniforme)
2. Pasamos el vector por una red neuronal (NN)
3. Transformamos el vector final proveniente del √∫ltimo layer de la NN a las dimensiones de la imagen ( un simple `.reshape(img_size)`)

Vamos a ver el c√≥digo ! ü§ñü§ñü§ñ

```python
# 1. Noise Vector
noise = torch.randn(LATENT_SIZE)

# 2. Generator NN
class Generator(nn.Module):

    """Generator model: In charge of generate 
    real-like images from a noise distribution

    """
    def __init__(self, latent_size, img_size):
        super(Generator, self).__init__()

        # layers to use
        self.model = nn.Sequential(
            nn.Linear(latent_size, 128),  # Nx100
            nn.LeakyReLU(),
            nn.Linear(128,256), # Nx256
            nn.BatchNorm1d(256),
            nn.LeakyReLU(),
            nn.Linear(256, 512), # Nx512
            nn.BatchNorm1d(512),
            nn.LeakyReLU(),
            nn.Linear(512, 1024), # Nx1024
            nn.BatchNorm1d(1024),
            nn.LeakyReLU(),
            nn.Linear(1024, img_size*img_size), # Nx28*28
            nn.Tanh()
        )

    def forward(self, x):
        x = self.model(x)
        return x

# 3. Generate Img
out = G(noise.unsqueeze(0))
img = out.view(IMG_SIZE, IMG_SIZE)
plt.imshow(img.detach().numpy(), cmap='gray')
```

Con el modelo sin entrenar, el resultado es algo del estilo: 

![Generated Image from nontrained Generator](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%204.png)

Ser√° muy interesante ver como el modelo evoluciona de tal manera que pueda convertir este ruido en im√°genes parecidas al conjunto de datos! 

**Observaciones üëÄ**

1. Las dimensiones del conjunto de datos MNIST es (28x28x1) ‚Üí son im√°genes en blanco y negro
2. Notar que la √∫ltima capa debe tener una dimensi√≥n de salida igual a la dimensi√≥n de la imagen, esto con el objetivo de poder moldear(reshape) el vector resultante a la imagen. Esto es, el vector de salida debe ser de $28*28*1=784$
3. Al comienzo utilic√© ReLU como funci√≥n de activaci√≥n, en el paper dicen: *The generator nets used a mixture of rectifier linear activations*. Pero luego de algunas iteraciones vi que era mejor LeakyReLU
4. Al comienzo no utilic√© BatchNorm, pero tambi√©n me di cuenta (navegando por internet) que era importante para este tipo de modelo para estabilidad y convergencia. 
5. La salida de el modelo es pasado por una funci√≥n $Tanh()$ para que se encuentre en un rango [-1,1]. Las im√°genes originales tambi√©n son normalizadas previamente para que pertenezcan a este rango.
6. LATENT_SIZE suele ser 100
7. Importante mencionar que al principio s√≥lo comenc√© con unas cuantas capas, y fui iterando hasta llegar a la arquitectura mostrada arriba seg√∫n los resultados y experimentos de otras personas.
8. En el paper DCGAN (veremos m√°s adelante), mencionan que el modelo se ve beneficiado al utilizar BatchNorm en todas las capas menos en la primera del Generador y la √∫ltima del Discriminador. 

### 2. Discriminador

Ya tenemos el modelo que generar√° imagenes y que intentar√° enga√±ar al Discriminador.  El Discriminador es bastante m√°s sencillo, s√≥lo debemos pensar que es un clasificador de im√°genes tal y como lo conocemos. Esto es, recibimos una imagen (pixeles), y devolvemos una clase (0 si es fake, 1 si es real) 

El paper dice lo siguiente del Discriminador D:

> We also define a second multilayer perceptron $D(x;\theta_d)$ that outputs a single scalar. $D(x)$represents the probability that  $x$ came from the data rather than $p_g$. We train D to maximize the probability of assigning the correct label to both training examples and samples from G.
> 

En c√∫anto a los detalles, s√≥lo tenemos :

> while the discriminator net used maxout [10] activations. Dropout [17]
was applied in training the discriminator net.
> 

Si lo visualizamos, ser√≠a algo como:

![Discriminator](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%205.png)


El proceso es el siguiente:

1. La entrada puede ser una imagen real proveniente del dataset ($p_{data}$) o proveniente del Generador ($p_{g}$)
2. La Imagen entra al Discriminador (NN)
3. El resultado es una probabilidad [0,1], en donde 0 es imagen fake y 1 real.

Es algo as√≠ como el proceso contrario de el Generador. Vamos al c√≥digo!

```python
#0. Define MaxOut Activation
class MaxOut(nn.Module):
    def __init__(self, num_units, num_pieces):
        super(MaxOut, self).__init__()
        self.num_units = num_units
        self.num_pieces = num_pieces
        self.fc = nn.Linear(num_units, num_units * num_pieces)

    def forward(self, x):
        # Reshape the output to separate pieces
        maxout_output = self.fc(x).view(-1, self.num_pieces, self.num_units)
        # Take the maximum value across pieces
        output, _ = torch.max(maxout_output, dim=1)
        return output

# 1. Imagen o Imagen Generada
out # Esto viene del Generador que hicimos arriba (o de el dataset original)

# 2. Discriminador
class Discriminator(nn.Module):
    """Generator model: In charge of classify
    images between real and syntetic generated
    by the generator 

    """
    def __init__(self, img_size):
        super(Discriminator, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(img_size*img_size, 512), #N x 512
            MaxOut(512, 4),
            nn.Linear(512, 256), # N x 256
            MaxOut(256, 4),
            nn.Linear(256, 1), # N x 1
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.model(x)
        return x

# 3. Se predice
D = Discriminator()
p = D(out)
```

El resultado es simple, ser√° algo como 0.495. Algo as√≠ como que es tan probable que sea una imagen real como fake seg√∫n el discriminador. Al comienzo puede ser as√≠ ya que no est√° entrenado. 

**Observaciones üëÄ**

1. La entrada es la misma dimensi√≥n que la salida del Generador (784)
2. Ac√° no utilizamos BatchNorm (aunque si deb√≠ hacerlo seg√∫n el paper DCGAN)
3. Utilizamos MaxOut activation como lo indica el paper, √©ste no se encuentra por defecto en Pytorch, por lo que hay que definirlo ‚Äúa mano‚Äù
4. Olvid√© utilizar Dropout como lo recomienda el paper, esto podr√≠a mejorar los resultados, al igual que agregar BatchNorm
5. Al igual que en el Generador, la arquitectura se fue iterando seg√∫n resultados. 

### 3. Rutina de Entrenamiento

Una vez tenemos el Generador y el Discriminador, s√≥lo nos queda entrenar. Seg√∫n el paper (lo que entend√≠), no lo hacen de la forma tradicional (iterando sobre el dataset original). Ellos proponen lo siguiente:

![GAN Training Algorithm](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%206.png)

Proponen, en cada iteraci√≥n, primero optimizar el Discriminador por K pasos:

1. Extraen un minibatch de imagenes reales
2. Extraen un minibatch de imagenes generadas por G
3. Hacen un update en base a Gradient Ascent

Luego, optimizan el Generador en un  s√≥lo paso:

1. Extrae un minibatch de imagenes generadas por G
2. Hacen un update de los par√°metros en base a Gradient Descend

Seg√∫n indican en el paper, updatean m√°s veces el Discriminador, ya que buscan que el Discriminador se mantenga cercano al √≥ptimo mientras el Generador se va actualizando lentamente hasta converger. 

> This results in D being maintained near its optimal solution, so long as G changes slowly enough.
> 

Lo que vemos ac√°, es que el generador actualiza sus par√°metros utilizando informaci√≥n implicita entregada por el Discriminador. Optimizamos el Generador para lograr enga√±ar al Discriminador.

Algo importante a poner atenci√≥n, es la funci√≥n objetivo, que tenemos 2, la del discriminador y la del generador, vamos a ver esto con m√°s detalle.

---

**Loss Functions üìé**

**Discriminador**

Recordemos que el Discriminador $D$ es un clasificador com√∫n, por lo que podemos utilizar la funci√≥n com√∫n para estos casos (Cross Entropy Loss). Desentra√±emos esto hasta llegar a lo que tienen en el paper:

Queremos clasificar correctamente los positivos (imagenes reales) y negativos (imagenes generadas), la funci√≥n objetivo Binary Cross Entropy est√° dada por:

$$
-\dfrac{1}{m} \sum{y_ilog(\hat{y_i}) + (1-y_i)log(1-\hat{y_i})} 
$$

donde $y_i$  es el label (0 o 1) de la imagen i ; $\hat{y_i}$ es la predicci√≥n de la imagen i, fake o real ; $m$ es el tama√±o del minibatch

Notemos que

- cuando $y_i = 1$, la imagen $x_i$ es real , osea que $\hat{y_i} = D(x_i)$  y lo de la derecha es 0 en la ecuaci√≥n
- cuando $y_i=0$, la imagen $G(z_i)$  es fake ,osea que $\hat{y_i} = D(G(z_i))$ y lo de la izquierda es 0 en la ecuaci√≥n

Es por esto que podemos reducir la ecuaci√≥n a:

$$
Min -\dfrac{1}{m} \sum{ log(D(x_i)) + log(1-D(G(z_i)))} 
$$

Si bien esto se puede traducir a que queremos maximizar 

$$
Max \dfrac{1}{m} \sum{ log(D(x_i)) + log(1-D(G(z_i)))} 
$$

tal como sale en el paper (sin el signo negativo), por conveniencia no lo haremos y optaremos por minimizar el Binary Cross Entropy y asi el c√≥digo se reduce a utilizar la funci√≥n ya hecha en Pytorch `BCELoss()` quedando algo como

```python
BCELoss([D(x), D(G(z))], [1, 0])
```

Otras soluciones optan por optimizarlas por separado, o por promediar ambos. En mi caso las concaten√© junto con sus targets.

::: {.callout-note icon=false}
### üí° 
Maximizar una funci√≥n Z es equivalente a Minimizar -Z

:::

**Generador**

Para el generador, es similar, s√≥lo que ahora s√≥lo le entregamos im√°genes falsas y adem√°s queremos que $D$  se equivoque. Por lo tanto, maximizamos la funci√≥n de p√©rdida Binary Cross Entropy (que se equivoque), y s√≥lo observamos cuando $y_i = 0$ (s√≥lo im√°genes falsas), teniendo 

$$
Max -\dfrac{1}{m} \sum{  log(1-D(G(z_i)))} 
$$

Ahora lo llevamos a minimizar por conveniencia en c√≥digo (en Pytorch por defecto se busca minimizar la funci√≥n objetivo)

$$
Min \dfrac{1}{m} \sum{  log(1-D(G(z_i)))} 
$$

obteniendo lo que aparece en el paper.  En c√≥digo ser√≠a algo como

```python
-BCELoss(D(G(z)), 0) # El signo - es porque implicitamente maximizamos BCE
```

---

Fuaaa, cuanta matem√°tica ü§Øü§Øü§Ø, comprobemos esto de forma intuitiva:

Para el **Discriminador** tenemos entonces:

$$
Max \dfrac{1}{m} \sum{ log(D(x_i)) + log(1-D(G(z_i)))} 
$$

si lo separamos, queremos entonces:

$$
Max \dfrac{1}{m} \sum{ log(D(x_i)) } 
$$

$$
Max \dfrac{1}{m} \sum{ log(1-D(G(z_i)) } 
$$

La primera funci√≥n (olvidando la suma y eso) es $ln(x)$ ‚Üí si, la base utilizada es la exponencial, por lo que es el logaritmo natural. Su gr√°fica es:

![ln(x)](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Notes_240225_163331.jpg)

El eje horizontal es $D(x_i)$: *La probabilidad de que la imagen real $x_i$ sea clasificada como real*, con un rango [0,1]. Ya que la imagen es real, queremos que D prediga un valor alto (ojal√° 1), lo que si vemos la gr√°fica es igual a alcanzar el m√°ximo de la funci√≥n (`m√°x optimo`). Es por esto que maximizamos $\dfrac{1}{m} \sum{ log(D(x_i)) }$ 

Ahora vamos con $log(1-D(G(z_i))$ , cuya gr√°fica es:

![ln(1-x) Discriminador](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%207.png)

Ahora la gr√°fica cambia, el eje horizontal est√° dado por $D(G(z_i))$: *La probabilidad de que la imagen falsa $G(z_i)$ sea clasificada como **real. V***emos que la funci√≥n se maximiza cuando $D(G(z_i))$ se acerca a 0, y esto es lo que queremos porque $G(z_i)$ es una imagen falsa y debe ser clasificada como tal en el Discriminador (ojal√° 0). Es por esto que buscamos maximizar $\dfrac{1}{m} \sum{ log(1-D(G(z_i)) }$  

::: {.callout-note icon=false}
### üî• 
IMPORTANTE! Ac√° vemos la funci√≥n objetivo tal y como est√° en el paper (maximizando), pero en el c√≥digo, por conveniencia, lo llevamos a minimizar la BCELoss, lo cual es equivalente, tal como lo mostramos en la parte matem√°tica. √âsto en los gr√°ficos es parecido s√≥lo que ahora buscamos minimizar ambas partes, dejar√© este ejercicio como tarea para el lector!

:::

Para el **Generador**, es muy similar a la segunda ecuaci√≥n del Discriminador, s√≥lo que ahora buscamos minimizar (ir hacia a la derecha). 

![ln(1-x) Generador](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%208.png)

Vemos que la idea es ir hacia la derecha porque queremos que el Discriminador se equivoque, o sea que aunque le entreguemos una imagen falsa $D(G(z_i))$, el discriminador prediga que es real (se acerque a 1).  Si vemos bien, a medida $D(G(z_i))$ se acerca a 1, la funci√≥n $\dfrac{1}{m} \sum{  log(1-D(G(z_i)))}$   se acerca a Esto puede provocar inestabilidad en la optimizaci√≥n, y es por esto que mucha gente utiliza la equivalencia:

$$
Min \dfrac{1}{m} \sum{ log(D(G(z_i))) } 
$$

como funci√≥n objetivo de el Generador. Ac√° vemos que utilizamos la parte donde $y_i = 1$ , osea que la imagen es real, pero minimizamos haciendo que el Discriminador diga que es falsa. Intuitivamente se pierde un poco el sentido ya que en realidad la imagen entregada no es real, pero matem√°ticamente es equivalente y logra mayor estabilidad y al minimizar el √≥ptimo es 0 y no $-\infin$ (a√∫n as√≠ yo utilic√© la primera forma)

Espero esta secci√≥n se haya entendido, es un tanto complicado de escribir con palabras. El proceso de entendimiento de la funci√≥n objetivo, fue un proceso bastante entretenido e interesante, que sin duda me sirvi√≥ para mejorar algunas aptitudes a la hora de leer papers. A√∫n as√≠ mencionar que es importante apoyarse en la literatura (libros, blogs, papers), el objetivo no es comprobar que tu podr√≠as haber llegado a las mismas conclusiones que los autores por tu lado, si no que eres capaz de entender (matem√°tica- o intuitiva- mente) lo que se propone. Invito al lector a iniciar una conversaci√≥n conmigo si le qued√≥ alguna duda, o si not√≥ que pude haberme equivocado en algo. 

---

**Train** üèãüèæ

Ahora ya podemos comenzar a entrenar el modelo, primero vamos a inicializar algunos componentes importantes:

1. Configuraci√≥n

```python
# CFG
latent_size = 100 # noise dimension
img_size = 28 # image shape
device = "cuda" # GPU
epochs = 20000 # TRAINING ITERATIONS
k = 1 # Discriminator steps
```

1. **Modelos:** Utilizamos los modelos construidos anteriormente 

 

```python
# Models 
G = Generator(cfg.latent_dim, cfg.img_size)
D = Discriminator(cfg.img_size)

G.to(cfg.device)
D.to(cfg.device)
```

1. **Loss Functions**: Recordemos que gracias a que re-definimos la matem√°tica de las funciones objetivos, podemos ocupar la funci√≥n pre-construida en Pytorch `BCELoss()`

```python
# Losses
D_LOSS = nn.BCELoss()
G_LOSS = nn.BCELoss()
```

1. **Optimizadores**: Utilizamos Adam, con un learning rate bastante m√°s peque√±o que el usual y unos betas espec√≠ficos. Esto es uno de los problemas de GAN, es muy sensible a los hyperpar√°metros, cuando normalmente los modelos (Deeplearning) son robustos a estos.

```python
# Optimizers
d_optimizer = torch.optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))
g_optimizer = torch.optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))

```

1. **Dataset**: Ya que MNIST es bastante conocido, ya se encuentra disponible en Pytorch para descargar

```python
# Datasets (Images and Noise)
transform = transforms.Compose([
    transforms.Resize((cfg.img_size, cfg.img_size)),
    transforms.ToTensor(),  # Convert PIL image or numpy array to tensor
    transforms.Normalize((0.5,), (0.5,))  # Normalize the tensor with mean and standard deviation
])

dataset = MNIST(root = '', download = True, transform =transform )
sampler = RandomSampler(dataset) # To get random images each iteration

# DataLoader
original_dl = DataLoader(dataset, batch_size = cfg.batch_size, sampler = sampler, pin_memory=torch.cuda.is_available())
```

Notar que ac√° normalizamos las im√°genes para que est√©n en el rango [-1,1] y nos aseguramos que tengan el mismo tama√±o 28x28. Tambi√©n, ya que no iremos avanzando batch por batch, iremos sampleando aleatoriamente el dataset tal como lo dice la rutina de entrenamiento pertenciente al paper.

1. Rutina de entrenamiento: Iremos sentencia por sentencia navegando la rutina de entrenamiento y codeando!

> *for number of training iterations do*
> 

```python
for epoch in range(cfg.epochs):
```

> for k steps do
> 

```python
for epoch in range(cfg.epochs):
	for k in range(cfg.k):
```

> Sample minibatch of m noise samples {z(1), . . . , z(m)} from noise prior $p_g(z)$.
> 

```python
for epoch in range(cfg.epochs):
	for k in range(cfg.k):
		# noise minibatch 
		z = torch.randn(batch_size, cfg.latent_dim)
```

> Sample minibatch of m examples {x(1), . . . , x(m)} from data generating distribution $p_{data}(x)$
> 

```python
for epoch in range(cfg.epochs):
	for k in range(cfg.k):
		# noise minibatch 
		z = torch.randn(batch_size, cfg.latent_dim)
		# original minibatch
		x, _ = next(iter(original_dl))
```

> Update the discriminator by ascending its stochastic gradient
> 

```python
for epoch in range(cfg.epochs):
	for k in range(cfg.k):
		# noise minibatch 
		z = torch.randn(batch_size, cfg.latent_dim)
		# original minibatch
		x, _ = next(iter(original_dl))

		##############################
        ## Discriminator Optimization 
        ##############################
		d_optimizer.zero_grad()
		G_z = G(z) # Generated Image
		D_x  = D(x) # real image's probability of being real
		D_G_z = D(G_z.detach()) # fake image's probability of being real
		
		# Concat real and fakes
		samples = torch.cat([D_G_z, D_x]).to(cfg.device)
		# Make targets (1 for real, 0 for fakes)
		targets = torch.cat([torch.zeros(D_G_z.size()[0]), torch.ones(D_x.size()[0])]).to(cfg.device)

		# Discriminator Loss
		# Loss
        d_loss = D_LOSS(samples, targets.unsqueeze(-1))
		d_loss.backward() # backward
		
		# Adjust learning weights
        d_optimizer.step()

```

> End for. Sample minibatch of m noise samples {z(1), . . . , z(m)} from noise prior  $p_g(z)$.
> 

```python
for epoch in tqdm(range(cfg.epochs)):
	for k in range(cfg.k):
		# noise minibatch 
		z = torch.randn(batch_size, cfg.latent_dim)
		# original minibatch
		x, _ = next(iter(original_dl))

		##############################
        ## Discriminator Optimization 
        ##############################
		d_optimizer.zero_grad()
		G_z = G(z) # Generated Image
		D_x  = D(x) # real image's probability of being real
		D_G_z = D(G_z.detach()) # fake image's probability of being real
		
		# Concat real and fakes
		samples = torch.cat([D_G_z, D_x]).to(cfg.device)
		# make targets (1 for real, 0 for fakes)
		targets = torch.cat([torch.zeros(D_G_z.size()[0]), torch.ones(D_x.size()[0])]).to(cfg.device)

		# Discriminator Loss
		# Loss
        d_loss = D_LOSS(samples, targets.unsqueeze(-1))
		d_loss.backward() # backward
		
		# Adjust learning weights
        d_optimizer.step()
	
	# Minibatch from noise prior
	z = torch.randn(batch_size, cfg.latent_dim).to(cfg.device)

```

> Update the generator by descending its stochastic gradient:
> 

```python
for epoch in tqdm(range(cfg.epochs)):
	for k in range(cfg.k):
		# noise minibatch 
		z = torch.randn(batch_size, cfg.latent_dim)
		# original minibatch
		x, _ = next(iter(original_dl))

		##############################
        ## Discriminator Optimization 
        ##############################
		d_optimizer.zero_grad()
		G_z = G(z) # Generated Image
		D_x  = D(x) # real image's probability of being real
		D_G_z = D(G_z.detach()) # fake image's probability of being real
		
		# Concat real and fakes
		samples = torch.cat([D_G_z, D_x]).to(cfg.device)
		# make targets (1 for real, 0 for fakes)
		targets = torch.cat([torch.zeros(D_G_z.size()[0]), torch.ones(D_x.size()[0])]).to(cfg.device)

		# Discriminator Loss
		# Loss
        d_loss = D_LOSS(samples, targets.unsqueeze(-1))
		d_loss.backward() # backward
		
		# Adjust learning weights
        d_optimizer.step()
	
    # Minibatch from noise prior
    z = torch.randn(batch_size, cfg.latent_dim).to(cfg.device)

    ##############################
    ## Generator Optimization 
    ##############################
    g_optimizer.zero_grad()
    G_z = G(z) # Generated Images
    D_G_z = D(G_z) # fake/generated image's probability of being real
    # targets
    targets = torch.zeros(D_G_z.size()[0]).to(cfg.device)
    # Loss
    g_loss = -G_LOSS(D_G_z, targets.unsqueeze(-1)) # Max BCE
    g_loss.backward()

    # Adjust learning weights
    g_optimizer.step()

```

Y con eso ya estamos! a correr y a observar los resultados, a continuaci√≥n algunas observaciones:

1. Tambi√©n se puede hacer el entrenamiento com√∫n (minibatch stochastic gradient descend )
2. Vi que en algunos lados utilizan el mismo noise $z$ para ambas partes de la optimizaci√≥n
3. En la linea `D_G_z = D(G_z.detach())` , el .detach() es para no actualizar parametros del Generador en la parte del Discriminador, por lo que quitamos G_z de del grafo computacional. 
4. Entren√© por 20.000 iteraciones! se demor√≥ aprox 15 minutos. Notemos que no son epochs (pasadas por el training set), si no que s√≥lo iteraciones del algoritmo.

::: {.callout-note icon=false}
### ‚ö†Ô∏è 
WARNING: Si quieres utilizar el c√≥digo, te recomiendo verlo directamente desde Github ya que ac√° quit√© algunas cosas para que se pueda entender mejor

:::

### 4. Resultados

Lo primero a observar es la curva de aprendizaje, que no es tan bonita

![Learning Curve GAN](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%209.png)

Podemos ver lo inestable que es el entrenamiento, aunque si tiende a minimizar ambas funciones. Ahora vemos que im√°genes genera:

![Sample Results](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2010.png)

Fuaaa ü§©¬†Parecer ser que el modelo intenta converger y genera algunas im√°genes que si podrian enga√±ar al ojo humano. 

Si te lo est√°s preguntando, la evaluaci√≥n real propuesta en el paper no es simple, en el paper utilizan unas t√©cnicas espec√≠ficas, pero para dejarlo simple, ac√° simplemente evaluaremos con la vista! 

![](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2011.png)

Algo importante es que  idealmente en generaci√≥n de im√°genes no se aprenda de memoria las caracter√≠sticas de el conjunto de entrenamiento, si no que le agregue un poco de su saz√≥n! e.g al generar caras, en donde el set de entrenamiento no tienen ninguna cara con barba, nos gustar√≠a que pudiese generar caras con barba (esto es una limitante)

Veamos como va evolucionando el modelo cada 1000 iteraciones! 

![Iteraci√≥n 0](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2012.png)



![Iteraci√≥n 2000](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2013.png)



![Iteraci√≥n 1000](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2014.png)



![Iteraci√≥n 3000](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2015.png)



![Iteraci√≥n 4000](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2016.png)


![Iteraci√≥n 15000](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2017.png)


Observamos que ya a partir de la iteraci√≥n 4000 logra hacer un ‚Äú6‚Äù bastante decente. 

::: {.callout-note icon=false}
### üìù
PD: Mis disculpas por no haber hecho un plot m√°s ordenado ac√°! 
:::
# DCGAN

Mientras construiamos esto, nos pregunt√°bamos si podiamos utilizar CNNs en vez de MLPs. En el Paper **[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) lo hacen!** 

Me encantar√≠a incluir la explicaci√≥n de como implement√© este paper tambi√©n (me gust√≥ m√°s porque incluyen todos los detalles de arquitectura), pero se est√° alargando mucho el post üò¢. Lo dejar√© para un futuro, a√∫n as√≠ puedes ver el c√≥digo y notebook en el Github. Para no dejarte con las ganas, te dejar√© con los resultados obtenidos, los cuales no son tan mejores en cu√°nto al dataset MNIST, pareciera que son predicciones m√°s suaves, pero debo mencionar que ac√° utilic√© un `img_size` de 64x64

![MNIST Generations By DCGAN](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2018.png)


Me gustan m√°s las generaciones hechas por GAN tradicional (Vanilla). Quiz√° pude haber mejorado m√°s el modelo, o haber iterado por m√°s tiempo. A√∫n as√≠, creo que la ventaja de DCGAN es que permite generar tipos de im√°genes m√°s complejas. Dicho esto, prob√© con el dataset CELEB, que son im√°genes de celebridades y estos fueron los resultados:

![CELEB Generations by DCGAN](GAN%20Paper%20Implementation%2052ba99d208bf45dfb1a22f2077ec3f77/Untitled%2019.png)



Yo veo que es un buen intento! Pude haber seguido iterando para encontrar mejores resultados, pero creo que por ahora es bastante decente! (Y me qued√© sin unidades de c√≥mputo üí∏üí∏üí∏)

Los GANs posteriores crean im√°genes bastantes m√°s realistas! Y para que hablar de los modelos de difusi√≥n y los avances que se han hecho hasta la fecha. ü§Øü§Øü§Øü§Ø

# Conclusiones

La implementaci√≥n del paper sobre Adversarial Neural Networks (GAN) ha demostrado ser una herramienta poderosa para la generaci√≥n de datos realistas y la mejora de la calidad en diversas aplicaciones. La capacidad de generar contenido nuevo y convincente a trav√©s de la competencia entre el generador y el discriminador abre un mundo de posibilidades creativas. A medida que continuamos explorando y refinando estas t√©cnicas, podemos anticipar avances significativos en campos como la visi√≥n por computadora, el dise√±o de im√°genes y la generaci√≥n de contenido multimedia. ¬°El futuro de las GAN promete emocionantes desarrollos en la generaci√≥n de contenido artificial!