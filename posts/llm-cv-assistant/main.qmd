---
title: "Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum"
image: "cv-assistant.png"
author: "Diegulio"
categories: [python, llm, palm, streamlit, langchain]
subtitle: "Creamos una aplicaci√≥n web que te permite chatear con tu CV!"
date: "2023-07-25"
---

# LLM CV Reviewer

[![](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/diegulio/llm-cv-helper)

# T√≥pico: LLM - Chat ‚ù§Ô∏è Data

En mi √∫ltimo post: [*Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT*](https://diegulio.github.io/posts/llm-recipe/main.html) pudimos observar las capacidades que podemos aprovechar de los Large Language Models (LLM).üò≤¬†D√©mos un vistazo a que aprendimos en el blog pasado:

- Introducci√≥n LLM
- OpenAI ChatGPT Model
    - Chat Model
    - Text Model
- Lanchain
    - Prompts
    - Chains
- Gradio

TLDR (too long to read): En ese blog creamos una aplicaci√≥n con una interfaz gr√°fica utilizando Gradio en donde se le permit√≠a al usuario ingresar una comida y obtener tanto la receta como informaci√≥n de los ingredientes. Para esto utilizamos los poderes de los modelos brindados por OpenAI ü§ñ¬†y orquestando todo con las facilidades que nos entrega Langchain ü¶ú‚õìÔ∏è.  La arquitectura fue ‚Äúsimple‚Äù üôÑ, utilizamos una cadena de LLMs en donde cada uno era acompa√±ado por un prompt template. 

Un punto importante a mencionar es que la informaci√≥n utilizada para obtener los ingredientes proven√≠a de un mont√≥n de informaci√≥n esparcida por internet con la cual fue entrenado el modelo de OpenAI. Dicho esto, existir√°n muchas ocasiones en donde nos gustar√≠a que nuestros modelos puedan consultar nuestra propia informaci√≥n! Imagina que te hubiese gustado que el modelo solo responda con recetas de tu pa√≠s  ü•ò

**Adivina que! En este post veremos como resolver esto pero sin comida, no queremos quedar satisfechos de aprender! üìö**

::: {.callout-note icon=false}
## üìù¬†Nota
Algo importante a mencionar es que el hecho de integrar tu informaci√≥n a un LLM puede hacerse de diversas formas, tales como utilizando Fine-tuning o ingresando tu informaci√≥n como contexto. En este caso nos enfocaremos en la √∫ltima. D√©jame saber si te interesar√≠a que escribiera alg√∫n otro post sobre Fine-tuning!

:::

# üìÅ¬†Motivaci√≥n: Revisi√≥n de Curr√≠culum

En una de las empresas que trabaj√© sol√≠a entrevistar a los futuros practicantes y memoristas que se unir√≠an al equipo (tarea que disfrutaba mucho, por cierto). Lamentablemente mi memoria me suele fallar y a veces recordaba que ten√≠a la entrevista unos minutos antes por lo que no alcanzaba a leer el curriculum del candidato (cosa importante). Este problema lo solucionaba leyendo el curriculum mientras entrevistaba al candidato o candidata pero esto generaba poca fluidez en mi conversaci√≥n y a veces p√©rdida de informaci√≥n de lo que la candidata me dec√≠a. Esto era sumamente riesgoso ya que encuentro importante escuchar con atenci√≥n las experiencias de los candidatos, pero tristemente mi multitasking me fallaba.

Entonces ahora pens√©, que entretenido seria tener un asistente que me ayudara a leer los curriculums de los candidatos! Y que mejor si este asistente es un robot (as√≠ puedo explotarlo muajaj üòà)

**üß†¬†Soluci√≥n: Utilizar  LLM para ‚Äúchatear‚Äù con los CV entregados por los candidatos.**

Como siempre, √©sto s√≥lo fue una excusa para aprender sobre como conectar mis documentos con un LLM.  

Cabe mencionar que esta soluci√≥n es muy escalable, imaginemos utilizarla para ayudar a una empresa de recursos humanos a entrevistar multiples candidatos a la vez, obteniendo comparaciones, estimaciones de *fit* con la empresa, etc. 

# üë®üèæ‚Äçüíª Demo

A modo de motivaci√≥n, te dejo un simple demo de lo que construiremos! 
![cv-assitant-demo](LLM%20CV%20Reviewer%208556526ffb61499f8011fb71d42d7d69/cv-assitant-demo.gif)

# üî®¬†Tool Path: Que utilizaremos

A continuaci√≥n les dejo las herramientas que utilizaremos en este post:

1. **üå¥¬†PaLM API**: LLM entrenado por Google AI
2.  ü¶ú‚õìÔ∏è¬†**LangChain**: Para poder comunicarme de manera f√°cil con la API de PaLM, adem√°s de aprovechar un mont√≥n de los facilitadores que tiene para construir herramientas basadas en LLM
3. üêç¬†**Python**: Lenguaje de Programaci√≥n
4. üëë¬†**Streamlit**: Framework para crear interfaz de usuario

::: {.callout-note icon=false}
## üëÄ 
Notar un par de diferencias con el post anterior, ahora utilizaremos los poderes de los LLM brindados por Google AI, y aprovecharemos los nuevos componentes que nos entrega Streamlit para poder crear interfaces en base a chats (Antes utilizabamos OpenAI para el LLM y Gradio para la interfaz)
:::

# üí≠¬†Concept Path: Que aprenderemos

A continuaci√≥n algunos de los conceptos que aprenderemos: 

1. **LLM**: Modelo de lenguaje
2. **Document Management**: Como procesamos documentos para un LLM
3. **Embeddings**: Como traducimos texto (de documentos) a algo entendible para un LLM
4. **Vector Stores**: Donde almacenamos los embeddings de los textos
5. **Retrievers**: De que forma le entregamos el documento a un LLM

# ‚ôüÔ∏è¬†Estrategia: Como abordamos

Creo que en esta secci√≥n tendremos muuuuucho que abordar ü´†. Primero entendamos como los LLM suelen usar la informaci√≥n de los documentos, y luego desentra√±aremos las oscuras t√©cnicas que existen para llevar a cabo esto.

## Document As Context

Desde una vista general, los LLM utilizan la informaci√≥n de documentos como **contexto**.  Imaginemos tenemos un documento de texto (un PDF) bastante simple que contiene algo como:

> Diego tiene 25 a√±os, es Ingeniero Civil Industrial, le gusta ver anim√©, jugar videojuegos, ir al gimnasio y jugar p√°del.
> 

Luego, si un usuario pregunta:

::: {.callout-note icon=false}
## üëµüèΩ 
Hola, a diego le gusta hacer deporte?

:::

Si usamos el LLM como tal, sin entregarle el documento, el modelo responder√≠a algo como:

::: {.callout-note icon=false}
## ü§ñ
Y quien car***o es diego?
:::


Quiz√° exager√© un poco, pero se entiende. El LLM no tiene la informaci√≥n del documento que necesito considere. Como mencion√© anteriormente, √©sto se soluciona agregando la informaci√≥n como contexto en el prompt, quedando algo por el estilo:

::: {.callout-note icon=false}
## üëµüèΩ 
Utiliza las siguientes piezas de contexto para responder la pregunta al final. Por favor si no sabes la respuesta, s√≥lo di que no sabes, no intentes inventarla.

Diego tiene 25 a√±os, es Ingeniero Civil Industrial, le gusta ver anim√©, jugar videojuegos, ir al gimnasio y jugar p√°del. 

Pregunta: Hola, a diego le gusta hacer deporte?

Respuesta:

:::

Fijemonos que se le agregaron tanto **instrucciones** y **contexto**. Dejando que luego de eso el LLM responda la pregunta deseada tal y como lo pregunt√≥ el/la usuaria.

Hasta ahora bastante sencillo cierto? Apuesto a que imaginabas que por detr√°s se volv√≠a a entrenar el modelo agreg√°ndole preguntas y respuestas del documento ‚Ä¶ blablabla. no? porque yo si lo pensaba üòÜ

Veamos un diagrama de lo que tendr√≠amos por el momento:

![Image by diegulio](LLM%20CV%20Reviewer%208556526ffb61499f8011fb71d42d7d69/Untitled.png)

Hasta ahora todo bien, pero no debemos olvidar una limitante importante en los prompt de los LLM. El famoso `context_length` ! Este canalla (como dir√≠a mi abuela üëµüèº) nos restringe la cantidad de caracteres, tokens, etc que podemos ingresar en el prompt. Entonces cuando tenemos documentos muy grandes y con gran cantidad de caracteres, que hacemos?

## Data Connection

Ya sabemos que suponiendo un documento tan grande que sobrepase el `context_lenght` no podremos agregar esta informaci√≥n como contexto a nuestro prompt. Una opci√≥n v√°lida ser√≠a simplemente tomar un extracto aleatorio del documento e insertarlo como contexto. Lamentablemente ser√° muy probable que el contexto adecuado para responder la pregunta del usuario no se encuentre en el extracto aleatorio.

üí°¬†Una mejor idea seria **extraer partes del documento que se relacionen con la pregunta**. Entonces la estrategia queda como:

1. **Load Documents**: Cargar los documentos
2. **Split Documents**: Dividir los documentos en piezas de texto
3. **Embedding:** Extraer features de las piezas de texto
4. **Vector Store**: Guardar features en una base de datos para utilizarlo despu√©s.

Ac√° les presento un diagrama que simboliza este proceso:

![Image by diegulio](LLM%20CV%20Reviewer%208556526ffb61499f8011fb71d42d7d69/Untitled%201.png)


El objetivo final es poder representar cada pieza (`chunk`) de texto de forma sem√°ntica, para as√≠ poder hacer la relaci√≥n con la pregunta del usuario, y decidir que chunks incluir en el prompt final. Ahora naveguemos un poco por cada paso:

### 1. Load Documents

Esta etapa es bastante simple, s√≥lo requiere poder pasar los documentos a texto. Langchain posee una gran variedad de m√©todos para hacer esto, ac√° te dejo la [documentaci√≥n](https://python.langchain.com/docs/modules/data_connection/document_loaders/).

En nuestro caso es poder tomar un CV en PDF y extraer el texto de este. Tambi√©n podemos extraer texto de DataFrames, Json, Latex, Wikipedia, etc.

### 2. Split Documents

Existen varias formas para dividir documentos, puede ser por caracteres, por elementos Markdowns, tokens, etc. Creo que no vale la pena que yo te lo explique cuando existe una buena [documentaci√≥n](https://python.langchain.com/docs/modules/data_connection/document_transformers/) para aquello.

Ac√° hay dos conceptos que pueden ser importantes, que son el tama√±o del chunk `chunk_size` y el tama√±o del overlap en los distintos chunks `overlap_size`.  Si el tama√±o del chunk es muy peque√±o entonces puede ser dif√≠cil extraer un buen contexto, pero si por el contrario es muy grande puede que estemos extrayendo informaci√≥n poco valiosa, y adem√°s arriesg√°ndonos podemos sobrepasar el `context_length`.

En general, los divisores de texto funcionan de la siguiente manera:

1. Dividir el texto en peque√±os fragmentos sem√°nticamente significativos (a menudo oraciones).
2. Comenzar a combinar estos peque√±os fragmentos en un fragmento m√°s grande hasta que alcance un cierto tama√±o `chunk_size` (seg√∫n lo medido por alguna funci√≥n).
3. Una vez que alcance ese tama√±o, hace que ese fragmento sea su propio fragmento de texto y luego comience a crear un nuevo fragmento de texto con algo de superposici√≥n `overlap_size` (para mantener el contexto entre los fragmentos).

### 3. Embeddings

El concepto de Embeddings es muy importante en Machine Learning en general. Siendo muy utilizados cuando hablamos de textos. Puedes ver como en el post ***[Identificando desastres en Twitter con NLP](https://diegulio.github.io/posts/kaggle_nlp_disaster/main.html)*** utilizamos embeddings para representar texto de forma num√©rica (usando vectores en este caso).

Podr√≠a escribir un blog entero sobre esto, pero creo que encontrar√°s mejor informaci√≥n en internet. Ac√° te dejo con un starter que puede ser la vieja y confiable [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding).

Creo que s√≥lo nos basta con saber que podemos representar tanto palabras, car√°cteres, sentencias, documentos, etc con **vectores**. Adem√°s podemos calcular medidas de similaridad entre estos utilizando operaciones vectoriales, como el conocido producto punto, o calculando el coseno del √°ngulo entre los vectores. Vuelvo a repetir que este concepto es muy importante y que si lo desconoces debes ya ir a darle unas vueltas! üöÄ

### 4. Vector Stores

Imaginemos que tenemos este gran documento, el cual dividimos en distintos `chunks` y transformamos a vectores. Muchas veces el/los documentos ser√°n tan grandes que ni siquiera cabr√°n en la memoria de nuestro computador (en la RAM). Es por esto que se utilizan los llamados Vector Stores, que son almacenamientos de embeddings.  Ac√° podremos guardar cada palabra, sentecia, documento con su respectivo embedding, para luego simplemente consultar esta base de datos y no tener que calcular el embedding todo el tiempo. 

Existen vector stores que almacenan esta informaci√≥n en la nube, podr√°s encontrar compa√±√≠as que ofrecen estos servicios como *Pinecone*, *Weviate*, GCP con *Matching Engine*. Hay otros como *FAISS* por Facebook AI e incluso algunos que almacenan la informaci√≥n en la memoria RAM (si es posible) como *Chroma*.

Adem√°s, estas herramientas no s√≥lo ofrecen almacenar esta informaci√≥n, si no que tambi√©n calcular la relaci√≥n sem√°ntica entre alguna frase, pregunta, prompt y los vectores presentes en la base datos de forma muy eficiente.

![Source: [https://python.langchain.com/docs/modules/data_connection/vectorstores/](https://python.langchain.com/docs/modules/data_connection/vectorstores/)](LLM%20CV%20Reviewer%208556526ffb61499f8011fb71d42d7d69/Untitled%202.png)

Otro punto importante a considerar ac√° son los distintos m√©todos disponibles para calcular la similaridad entre la query y los vectores para obtener aquellos m√°s similares.

## Retrievers

Volvamos al diagrama que teniamos hace un rato:

![Image by diegulio](LLM%20CV%20Reviewer%208556526ffb61499f8011fb71d42d7d69/Untitled.png)


Habiamos quedado estancados con el hecho de que algunos documentos podrian no caber en el contexto entregado al prompt debidos al limitado `context_length`. 

Ahora dividimos el documento, los convertimos a vectores, los almacenamos y adem√°s tenemos t√©cnicas para calcular la similaridad entre una query y los distintos `chunks`.

Entonces nuestra misi√≥n se resume a: **En base a una pregunta, obtener las piezas de texto (chunks) m√°s relevantes para incluirlos en el prompt final.**

Para entenderlo mejor, consideremos como ejemplo nuestro caso de uso para chatear con los curriculums, tenemos la pregunta del usuario:

::: {.callout-note icon=false}
## üëµüèΩ 
What is the candidate name?
:::

E imaginemos que tenemos un CV muy grande (poco recomendable), algo del estilo:

```
Nombre: Diego Machado
Edad: 25 a√±os
Hobbies: Ir al gym, jugar p√°del, jugar videojuegos

.... texto .... bla bla ..

Diego Machado estudi√≥ en .... bla bla

.... texto ...

Trabaj√≥ 1 a√±o en ... luego trabaj√≥ en ...

Sus habilidades son ....

.... texto .....

.... mucho texto ...
```

Podemos ver que en este texto se encuentre la respuesta al prompt de manera expl√≠cita en dos ocasiones.  Lo que esperamos que logre el procedimiento mostrado en ***Data Connection*** ser√° obtener los chunks de texto:

```latex
Nombre: Diego Machado
Edad: 25 a√±os
Hobbies: Ir al gym, jugar p√°del, jugar videojuegos 
```

y

```latex
Diego Machado estudi√≥ en .... bla bla

.... texto ...  
```

As√≠, respetamos el `chunk_size` y adem√°s le entregamos los documentos m√°s relevantes al LLM en base al calculo de similaridad entre la pregunta *‚ÄúWhat is the candidate name?‚Äù* y los distintos chunks.


Entonces ahora tenemos un buen contexto que no incumple alguna norma, por lo que el prompt final se transforma en:

::: {.callout-note icon=false}
## üëµüèΩ 
Utiliza las siguientes piezas de contexto para responder la pregunta al final. Por favor si no sabes la respuesta, s√≥lo di que no sabes, no intentes inventarla.

Nombre: Diego Machado
Edad: 25 a√±os
Hobbies: Ir al gym, jugar p√°del, jugar videojuegos
Diego Machado estudi√≥ en .... bla bla
.... texto ...

Pregunta: What is the candidate name?

Respuesta:

:::

y lo que esperamos que suceda ser√° una respuesta del estilo:

::: {.callout-note icon=false}
## ü§ñ 
The candidate‚Äôs name is Diego Machado
:::


Y listo! Asi podemos resolver el problema de los documentos largos. Entonces el diagrama anterior resulta en:

![Image by diegulio](LLM%20CV%20Reviewer%208556526ffb61499f8011fb71d42d7d69/Untitled%203.png)

::: {.callout-note icon=false}
## ‚òùüèΩ 
La forma en la que se decide que chunks son relevantes se les denomina **retrievers**, y existen diversas metodolog√≠as muy interesantes y robustas. Te invito a buscar m√°s informaci√≥n [ac√°](https://python.langchain.com/docs/modules/data_connection/retrievers/)

:::

### Chain Documents

ALTO AH√ç! A√∫n falta algo. Si nos fijamos bien, lo que hice con los chunks de texto relevantes fue simplemente concatenarlos! uno sobre el otro! 

```
Nombre: Diego Machado
Edad: 25 a√±os
Hobbies: Ir al gym, jugar p√°del, jugar videojuegos
Diego Machado estudi√≥ en .... bla bla
.... texto ...
```

A esto se le llama `stuff`. Pero tambi√©n existen distintas metodolog√≠as para hacer esto de forma muy eficiente y robusta. Algunas de ellas incluso utilizan LLM auxiliares para refinar este contexto! Te invito a leer m√°s sobre eso [ac√°](https://python.langchain.com/docs/modules/chains/document/).

Fiuf! Creo que eso ser√≠a ‚Äútodo‚Äù, por √∫ltimo te dejo un diagrama general, el cual muestra todos los elementos utilizados:

![[https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)](LLM%20CV%20Reviewer%208556526ffb61499f8011fb71d42d7d69/Untitled.jpeg)


# üß†¬† Prototyping: Code Time!

Ahora es el momento de llevar todas estas ideas al c√≥digo. 

Revisitemos el proceso que necesitamos:

1. Load Document: Cargar Curriculum
2. Splitting: Dividir informaci√≥n en chunks
3. Vector Store: Almacenamos embeddings
4. Create Conversational Retrieval Chain: Esto es b√°sicamente la creaci√≥n del bot.

## 1. Load Document

Langchain tiene integraciones con un mont√≥n de tipos de documentos, para nuestro caso, en donde suponemos curriculums en pdf, debemos cargarlo de la siguiente forma:

```python

from langchain.document_loaders import PyPDFLoader
#¬†Langchain loader
loader = PyPDFLoader("../docs/CV_DMV.pdf")
# Load pages
pages = loader.load()
```

Importante notar que tambi√©n podemos extraer metadata:

```python
pages[0].metadata

#¬†{'source': '../docs/CV_DMV.pdf', 'page': 0}
```

## 2. Splitting documents

En este caso utilizaremos el m√©todo Recursive Character Text Splitter.

Este separador de texto es el recomendado para texto gen√©rico. Se parametriza mediante una lista de caracteres. Se Intenta dividirlos en orden hasta que los trozos sean lo suficientemente peque√±os. La lista predeterminada es ["\n\n", "\n", " ", ""]. Esto tiene el efecto de tratar de mantener todos los p√°rrafos (y luego las oraciones y luego las palabras) juntos el mayor tiempo posible, ya que gen√©ricamente parecer√≠an ser los fragmentos de texto m√°s relacionados sem√°nticamente.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 1000,
    chunk_overlap  = 100,
)

texts = text_splitter.split_documents(pages)
```

Notemos que ac√° aparecen los par√°metros mencionados anteriormente: `chunk_size`, `chunk_overlap`

Lo que esta funci√≥n retorna es una lista de Documents 

```python
texts = [
Document(page_content='Nombre:DiegoMachadoEdad:25a√±os..', metadata={..}),
Document(page_content='..texto..', metadata={..}),
Document(page_content='DiegoMachadoestudi√≥en....', metadata={..}),
]
```

## 3. Vector Store

Ahora creamos nuestro vector store, en este caso Chroma. Notar que debemos ingresar como par√°metro la funci√≥n de embeddings a utilizar. En este caso utilizamos Vertex AI Embeddings

```python
from langchain.vectorstores import Chroma
from langchain.llms import VertexAI
from langchain.embeddings import VertexAIEmbeddings

# Embeddings fn
embeddings = VertexAIEmbeddings(project='gcp-project')

# Persist Directory
persist_directory = 'docs/chroma/'

# Vector db
vectordb = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory=persist_directory
)
```

Podemos utilizar persist directory si queremos almacenar el vector store en nuestro local, para asi no tener que cargarlo cada vez que lo instanciamos:

```python
vectordb.persist()
```

Utilizando el vector store podemos ocupar su funcionalidad de `similarity_search` para encontrar los chunks m√°s similares a una query en particular:

```python
# Test embeddings
question = "What is the name of the candidate?"
docs = vectordb.similarity_search(question,k=2)

print(docs[0].page_content[:300]) # Solo imprimiremos los primeros 300 caracteres del primer documento
```

```
DIEGOMACHADO
/_4782ndAugust1997 /_475dmachadovz@gmail.com /phone+56950917953 /map_markerSantiago,Chile
/linkedinwww.linkedin.com/in/DiegulioMachado /githubhttps://github.com/diegulio ·Ωë7diegulio.github.io
BRIEFDESCRIPTION
Industrialengineeringgraduatedwitha
master‚Äôsdegreeinengineeringsciences.
Passio
```

Podemos notar que el nombre del candidato aparece en el documento m√°s similar a la query! esto cumple con nuestras expectativas üòé

## Conversational Retrieval Chain

Ahora es donde podemos crear una cadena personalizada que tome el contexto seg√∫n la query, que cree el prompt final y que incluso vaya conservando la memoria. 

La buena noticia es que Langchain ya tiene una cadena pre-construida que se encarga de todo esto! por lo que utilizarlo es muy f√°cil:

```python
from langchain.memory import ConversationBufferMemory

# Primero instanciamos el tipo de memoria
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
```

Esta cadena personalizada necesita 3 cosas: un LLM, el retriever y la memoria. En este caso utilizaremos el retriever base de Chroma, pero recuerda que podemos plantear otros!

```python
from langchain.chains import ConversationalRetrievalChain

# Instanciamos LLM
llm = VertexAI(project_id = 'gcp-project')

#¬†Retriever
retriever=vectordb.as_retriever()

qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)
```

Y listo! la parte del c√≥digo parece ser lo m√°s sencillo, todo gracias al framework Langchain ü¶ú‚õìÔ∏è‚ù§Ô∏è

Ahora podemos probarlo:

```python
question = "Does the candidate has been teacher assistant?"
result = qa({"question": question})
```

```
{'question': 'Does the candidate has been teacher assistant?',
 'chat_history': [HumanMessage(content='Does the candidate has been teacher assistant?', additional_kwargs={}, example=False),
  AIMessage(content='Yes, the candidate has been a teacher assistant.', additional_kwargs={}, example=False)],
 'answer': 'Yes, the candidate has been a teacher assistant.'}
```

Si volvemos a preguntar, en el chat_history se ir√° guardando autom√°ticamente toda la info del chat, por lo que nuestro bot se acordar√° de preguntas anteriores ! ü§ñ

```python
question = "In which universities?"
result = qa({"question": question})
```

```
{'question': 'In which universities?',
 'chat_history': [HumanMessage(content='Does the candidate has been teacher assistant?', additional_kwargs={}, example=False),
  AIMessage(content='Yes, the candidate has been a teacher assistant.', additional_kwargs={}, example=False),
  HumanMessage(content='In which universities?', additional_kwargs={}, example=False),
  AIMessage(content='The candidate has been a teacher assistant at Universidad de Santiago and Universidad Adolfo Iba√±ez.', additional_kwargs={}, example=False)],
 'answer': 'The candidate has been a teacher assistant at Universidad de Santiago and Universidad Adolfo Iba√±ez.'}
```

# üßê¬†Front-End

Como siempre, nuestra aplicaci√≥n no puede quedar s√≥lo en palabras. En esta ocasi√≥n utilizamos los nuevos componentes de streamlit para chat!

```python
#¬†Input de usuario
prompt = st.chat_input("Ask something")
```

```python
# Respuesta (Puede ser de usuario, assistant, system o m√°s)
with st.chat_message("user", avatar=user_avatar):
	st.write(message.content)
```

Con estos elementos es que podemos construir algo as√≠:

![Front End](LLM%20CV%20Reviewer%208556526ffb61499f8011fb71d42d7d69/Untitled%204.png)

Para ver el c√≥digo utilizado para construir esta aplicaci√≥n puedes visitar el [repositorio](https://github.com/diegulio/llm-cv-helper).

# üöÄ¬†Pr√≥ximos Pasos

Como mencion√© anteriormente, esta soluci√≥n puede mejorarse mucho m√°s. Ac√° una lluvia de ideas:

- Verificar que lo que se suba sea un CV
- Aceptar m√°s tipos de documentos
- Probar m√°s tipos de Retrievers
- Probar m√°s tipos de Documents Chains
- Aceptar m√∫ltiples CVs y poder comparar

 

# ü•≥¬†Conclusi√≥n

En conclusi√≥n, en este blog hemos explorado c√≥mo aprovechar los Large Language Models (LLM) para crear una aplicaci√≥n de chat que interact√∫a con documentos. Utilizamos el modelo PaLM de VertexAI junto con LangChain para orquestar toda la funcionalidad. Nuestro enfoque fue utilizar LLMs para leer y responder preguntas sobre curr√≠culums.

Aprendimos sobre la importancia de los embeddings para representar el texto en forma num√©rica, as√≠ como los Vector Stores para almacenar y recuperar eficientemente estos embeddings. Tambi√©n descubrimos c√≥mo resolver el problema de documentos largos utilizando t√©cnicas de divisi√≥n, embeddings y retrievers para mantener el contexto relevante en los prompts.

Nuestra aplicaci√≥n de chat con curr√≠culums puede ser ampliada para otros usos y aplicaciones, como asistentes para entrevistas de recursos humanos o cualquier caso donde se necesite interactuar con documentos de manera eficiente. Adem√°s, se pueden explorar otras metodolog√≠as de retrievers y document chains para mejorar a√∫n m√°s la experiencia.

En definitiva, esta exploraci√≥n ha sido solo el comienzo, y el potencial de los Large Language Models junto con herramientas como LangChain es emocionante. Con estos avances, podemos crear aplicaciones m√°s inteligentes y personalizadas, que nos ayuden en tareas complejas y mejoren nuestra interacci√≥n con la informaci√≥n.