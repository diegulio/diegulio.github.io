<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.333">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Diegulio">
<meta name="dcterms.date" content="2023-07-25">

<title>Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo3.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum">
<meta property="og:description" content="Creamos una aplicaci√≥n web que te permite chatear con tu CV!">
<meta property="og:image" content="https://diegulio.github.io/posts/llm-cv-assistant/cv-assistant.png">
<meta property="og:image:height" content="896">
<meta property="og:image:width" content="1344">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo3.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/diegulio" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/dieguliomachado/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Data ‚ù§Ô∏è Chat: Chatea con tu Curriculum</h1>
            <p class="subtitle lead">Creamos una aplicaci√≥n web que te permite chatear con tu CV!</p>
                                <div class="quarto-categories">
                <div class="quarto-category">python</div>
                <div class="quarto-category">llm</div>
                <div class="quarto-category">palm</div>
                <div class="quarto-category">streamlit</div>
                <div class="quarto-category">langchain</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Diegulio </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 25, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#llm-cv-reviewer" id="toc-llm-cv-reviewer" class="nav-link active" data-scroll-target="#llm-cv-reviewer">LLM CV Reviewer</a></li>
  <li><a href="#t√≥pico-llm---chat-data" id="toc-t√≥pico-llm---chat-data" class="nav-link" data-scroll-target="#t√≥pico-llm---chat-data">T√≥pico: LLM - Chat ‚ù§Ô∏è Data</a></li>
  <li><a href="#motivaci√≥n-revisi√≥n-de-curr√≠culum" id="toc-motivaci√≥n-revisi√≥n-de-curr√≠culum" class="nav-link" data-scroll-target="#motivaci√≥n-revisi√≥n-de-curr√≠culum">üìÅ&nbsp;Motivaci√≥n: Revisi√≥n de Curr√≠culum</a></li>
  <li><a href="#demo" id="toc-demo" class="nav-link" data-scroll-target="#demo">üë®üèæ‚Äçüíª Demo</a></li>
  <li><a href="#tool-path-que-utilizaremos" id="toc-tool-path-que-utilizaremos" class="nav-link" data-scroll-target="#tool-path-que-utilizaremos">üî®&nbsp;Tool Path: Que utilizaremos</a></li>
  <li><a href="#concept-path-que-aprenderemos" id="toc-concept-path-que-aprenderemos" class="nav-link" data-scroll-target="#concept-path-que-aprenderemos">üí≠&nbsp;Concept Path: Que aprenderemos</a></li>
  <li><a href="#estrategia-como-abordamos" id="toc-estrategia-como-abordamos" class="nav-link" data-scroll-target="#estrategia-como-abordamos">‚ôüÔ∏è&nbsp;Estrategia: Como abordamos</a>
  <ul class="collapse">
  <li><a href="#document-as-context" id="toc-document-as-context" class="nav-link" data-scroll-target="#document-as-context">Document As Context</a></li>
  <li><a href="#data-connection" id="toc-data-connection" class="nav-link" data-scroll-target="#data-connection">Data Connection</a>
  <ul class="collapse">
  <li><a href="#load-documents" id="toc-load-documents" class="nav-link" data-scroll-target="#load-documents">1. Load Documents</a></li>
  <li><a href="#split-documents" id="toc-split-documents" class="nav-link" data-scroll-target="#split-documents">2. Split Documents</a></li>
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings">3. Embeddings</a></li>
  <li><a href="#vector-stores" id="toc-vector-stores" class="nav-link" data-scroll-target="#vector-stores">4. Vector Stores</a></li>
  </ul></li>
  <li><a href="#retrievers" id="toc-retrievers" class="nav-link" data-scroll-target="#retrievers">Retrievers</a>
  <ul class="collapse">
  <li><a href="#chain-documents" id="toc-chain-documents" class="nav-link" data-scroll-target="#chain-documents">Chain Documents</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#prototyping-code-time" id="toc-prototyping-code-time" class="nav-link" data-scroll-target="#prototyping-code-time">üß†&nbsp; Prototyping: Code Time!</a>
  <ul class="collapse">
  <li><a href="#load-document" id="toc-load-document" class="nav-link" data-scroll-target="#load-document">1. Load Document</a></li>
  <li><a href="#splitting-documents" id="toc-splitting-documents" class="nav-link" data-scroll-target="#splitting-documents">2. Splitting documents</a></li>
  <li><a href="#vector-store" id="toc-vector-store" class="nav-link" data-scroll-target="#vector-store">3. Vector Store</a></li>
  <li><a href="#conversational-retrieval-chain" id="toc-conversational-retrieval-chain" class="nav-link" data-scroll-target="#conversational-retrieval-chain">Conversational Retrieval Chain</a></li>
  </ul></li>
  <li><a href="#front-end" id="toc-front-end" class="nav-link" data-scroll-target="#front-end">üßê&nbsp;Front-End</a></li>
  <li><a href="#pr√≥ximos-pasos" id="toc-pr√≥ximos-pasos" class="nav-link" data-scroll-target="#pr√≥ximos-pasos">üöÄ&nbsp;Pr√≥ximos Pasos</a></li>
  <li><a href="#conclusi√≥n" id="toc-conclusi√≥n" class="nav-link" data-scroll-target="#conclusi√≥n">ü•≥&nbsp;Conclusi√≥n</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="llm-cv-reviewer" class="level1">
<h1>LLM CV Reviewer</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://github.com/diegulio/llm-cv-helper"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="t√≥pico-llm---chat-data" class="level1">
<h1>T√≥pico: LLM - Chat ‚ù§Ô∏è Data</h1>
<p>En mi √∫ltimo post: <a href="https://diegulio.github.io/posts/llm-recipe/main.html"><em>Iniciando en LLM: Crea tu primera aplicaci√≥n con LangChain y ChatGPT</em></a> pudimos observar las capacidades que podemos aprovechar de los Large Language Models (LLM).üò≤&nbsp;D√©mos un vistazo a que aprendimos en el blog pasado:</p>
<ul>
<li>Introducci√≥n LLM</li>
<li>OpenAI ChatGPT Model
<ul>
<li>Chat Model</li>
<li>Text Model</li>
</ul></li>
<li>Lanchain
<ul>
<li>Prompts</li>
<li>Chains</li>
</ul></li>
<li>Gradio</li>
</ul>
<p>TLDR (too long to read): En ese blog creamos una aplicaci√≥n con una interfaz gr√°fica utilizando Gradio en donde se le permit√≠a al usuario ingresar una comida y obtener tanto la receta como informaci√≥n de los ingredientes. Para esto utilizamos los poderes de los modelos brindados por OpenAI ü§ñ&nbsp;y orquestando todo con las facilidades que nos entrega Langchain ü¶ú‚õìÔ∏è. La arquitectura fue ‚Äúsimple‚Äù üôÑ, utilizamos una cadena de LLMs en donde cada uno era acompa√±ado por un prompt template.</p>
<p>Un punto importante a mencionar es que la informaci√≥n utilizada para obtener los ingredientes proven√≠a de un mont√≥n de informaci√≥n esparcida por internet con la cual fue entrenado el modelo de OpenAI. Dicho esto, existir√°n muchas ocasiones en donde nos gustar√≠a que nuestros modelos puedan consultar nuestra propia informaci√≥n! Imagina que te hubiese gustado que el modelo solo responda con recetas de tu pa√≠s ü•ò</p>
<p><strong>Adivina que! En este post veremos como resolver esto pero sin comida, no queremos quedar satisfechos de aprender! üìö</strong></p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üìù&nbsp;Nota
</div>
</div>
<div class="callout-body-container callout-body">
<p>Algo importante a mencionar es que el hecho de integrar tu informaci√≥n a un LLM puede hacerse de diversas formas, tales como utilizando Fine-tuning o ingresando tu informaci√≥n como contexto. En este caso nos enfocaremos en la √∫ltima. D√©jame saber si te interesar√≠a que escribiera alg√∫n otro post sobre Fine-tuning!</p>
</div>
</div>
</section>
<section id="motivaci√≥n-revisi√≥n-de-curr√≠culum" class="level1">
<h1>üìÅ&nbsp;Motivaci√≥n: Revisi√≥n de Curr√≠culum</h1>
<p>En una de las empresas que trabaj√© sol√≠a entrevistar a los futuros practicantes y memoristas que se unir√≠an al equipo (tarea que disfrutaba mucho, por cierto). Lamentablemente mi memoria me suele fallar y a veces recordaba que ten√≠a la entrevista unos minutos antes por lo que no alcanzaba a leer el curriculum del candidato (cosa importante). Este problema lo solucionaba leyendo el curriculum mientras entrevistaba al candidato o candidata pero esto generaba poca fluidez en mi conversaci√≥n y a veces p√©rdida de informaci√≥n de lo que la candidata me dec√≠a. Esto era sumamente riesgoso ya que encuentro importante escuchar con atenci√≥n las experiencias de los candidatos, pero tristemente mi multitasking me fallaba.</p>
<p>Entonces ahora pens√©, que entretenido seria tener un asistente que me ayudara a leer los curriculums de los candidatos! Y que mejor si este asistente es un robot (as√≠ puedo explotarlo muajaj üòà)</p>
<p><strong>üß†&nbsp;Soluci√≥n: Utilizar LLM para ‚Äúchatear‚Äù con los CV entregados por los candidatos.</strong></p>
<p>Como siempre, √©sto s√≥lo fue una excusa para aprender sobre como conectar mis documentos con un LLM.</p>
<p>Cabe mencionar que esta soluci√≥n es muy escalable, imaginemos utilizarla para ayudar a una empresa de recursos humanos a entrevistar multiples candidatos a la vez, obteniendo comparaciones, estimaciones de <em>fit</em> con la empresa, etc.</p>
</section>
<section id="demo" class="level1">
<h1>üë®üèæ‚Äçüíª Demo</h1>
<p>A modo de motivaci√≥n, te dejo un simple demo de lo que construiremos! <img src="LLM CV Reviewer 8556526ffb61499f8011fb71d42d7d69/cv-assitant-demo.gif" class="img-fluid" alt="cv-assitant-demo"></p>
</section>
<section id="tool-path-que-utilizaremos" class="level1">
<h1>üî®&nbsp;Tool Path: Que utilizaremos</h1>
<p>A continuaci√≥n les dejo las herramientas que utilizaremos en este post:</p>
<ol type="1">
<li><strong>üå¥&nbsp;PaLM API</strong>: LLM entrenado por Google AI</li>
<li>ü¶ú‚õìÔ∏è&nbsp;<strong>LangChain</strong>: Para poder comunicarme de manera f√°cil con la API de PaLM, adem√°s de aprovechar un mont√≥n de los facilitadores que tiene para construir herramientas basadas en LLM</li>
<li>üêç&nbsp;<strong>Python</strong>: Lenguaje de Programaci√≥n</li>
<li>üëë&nbsp;<strong>Streamlit</strong>: Framework para crear interfaz de usuario</li>
</ol>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üëÄ
</div>
</div>
<div class="callout-body-container callout-body">
<p>Notar un par de diferencias con el post anterior, ahora utilizaremos los poderes de los LLM brindados por Google AI, y aprovecharemos los nuevos componentes que nos entrega Streamlit para poder crear interfaces en base a chats (Antes utilizabamos OpenAI para el LLM y Gradio para la interfaz)</p>
</div>
</div>
</section>
<section id="concept-path-que-aprenderemos" class="level1">
<h1>üí≠&nbsp;Concept Path: Que aprenderemos</h1>
<p>A continuaci√≥n algunos de los conceptos que aprenderemos:</p>
<ol type="1">
<li><strong>LLM</strong>: Modelo de lenguaje</li>
<li><strong>Document Management</strong>: Como procesamos documentos para un LLM</li>
<li><strong>Embeddings</strong>: Como traducimos texto (de documentos) a algo entendible para un LLM</li>
<li><strong>Vector Stores</strong>: Donde almacenamos los embeddings de los textos</li>
<li><strong>Retrievers</strong>: De que forma le entregamos el documento a un LLM</li>
</ol>
</section>
<section id="estrategia-como-abordamos" class="level1">
<h1>‚ôüÔ∏è&nbsp;Estrategia: Como abordamos</h1>
<p>Creo que en esta secci√≥n tendremos muuuuucho que abordar ü´†. Primero entendamos como los LLM suelen usar la informaci√≥n de los documentos, y luego desentra√±aremos las oscuras t√©cnicas que existen para llevar a cabo esto.</p>
<section id="document-as-context" class="level2">
<h2 class="anchored" data-anchor-id="document-as-context">Document As Context</h2>
<p>Desde una vista general, los LLM utilizan la informaci√≥n de documentos como <strong>contexto</strong>. Imaginemos tenemos un documento de texto (un PDF) bastante simple que contiene algo como:</p>
<blockquote class="blockquote">
<p>Diego tiene 25 a√±os, es Ingeniero Civil Industrial, le gusta ver anim√©, jugar videojuegos, ir al gimnasio y jugar p√°del.</p>
</blockquote>
<p>Luego, si un usuario pregunta:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üëµüèΩ
</div>
</div>
<div class="callout-body-container callout-body">
<p>Hola, a diego le gusta hacer deporte?</p>
</div>
</div>
<p>Si usamos el LLM como tal, sin entregarle el documento, el modelo responder√≠a algo como:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
ü§ñ
</div>
</div>
<div class="callout-body-container callout-body">
<p>Y quien car***o es diego?</p>
</div>
</div>
<p>Quiz√° exager√© un poco, pero se entiende. El LLM no tiene la informaci√≥n del documento que necesito considere. Como mencion√© anteriormente, √©sto se soluciona agregando la informaci√≥n como contexto en el prompt, quedando algo por el estilo:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üëµüèΩ
</div>
</div>
<div class="callout-body-container callout-body">
<p>Utiliza las siguientes piezas de contexto para responder la pregunta al final. Por favor si no sabes la respuesta, s√≥lo di que no sabes, no intentes inventarla.</p>
<p>Diego tiene 25 a√±os, es Ingeniero Civil Industrial, le gusta ver anim√©, jugar videojuegos, ir al gimnasio y jugar p√°del.</p>
<p>Pregunta: Hola, a diego le gusta hacer deporte?</p>
<p>Respuesta:</p>
</div>
</div>
<p>Fijemonos que se le agregaron tanto <strong>instrucciones</strong> y <strong>contexto</strong>. Dejando que luego de eso el LLM responda la pregunta deseada tal y como lo pregunt√≥ el/la usuaria.</p>
<p>Hasta ahora bastante sencillo cierto? Apuesto a que imaginabas que por detr√°s se volv√≠a a entrenar el modelo agreg√°ndole preguntas y respuestas del documento ‚Ä¶ blablabla. no? porque yo si lo pensaba üòÜ</p>
<p>Veamos un diagrama de lo que tendr√≠amos por el momento:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM CV Reviewer 8556526ffb61499f8011fb71d42d7d69/Untitled.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Image by diegulio</figcaption>
</figure>
</div>
<p>Hasta ahora todo bien, pero no debemos olvidar una limitante importante en los prompt de los LLM. El famoso <code>context_length</code> ! Este canalla (como dir√≠a mi abuela üëµüèº) nos restringe la cantidad de caracteres, tokens, etc que podemos ingresar en el prompt. Entonces cuando tenemos documentos muy grandes y con gran cantidad de caracteres, que hacemos?</p>
</section>
<section id="data-connection" class="level2">
<h2 class="anchored" data-anchor-id="data-connection">Data Connection</h2>
<p>Ya sabemos que suponiendo un documento tan grande que sobrepase el <code>context_lenght</code> no podremos agregar esta informaci√≥n como contexto a nuestro prompt. Una opci√≥n v√°lida ser√≠a simplemente tomar un extracto aleatorio del documento e insertarlo como contexto. Lamentablemente ser√° muy probable que el contexto adecuado para responder la pregunta del usuario no se encuentre en el extracto aleatorio.</p>
<p>üí°&nbsp;Una mejor idea seria <strong>extraer partes del documento que se relacionen con la pregunta</strong>. Entonces la estrategia queda como:</p>
<ol type="1">
<li><strong>Load Documents</strong>: Cargar los documentos</li>
<li><strong>Split Documents</strong>: Dividir los documentos en piezas de texto</li>
<li><strong>Embedding:</strong> Extraer features de las piezas de texto</li>
<li><strong>Vector Store</strong>: Guardar features en una base de datos para utilizarlo despu√©s.</li>
</ol>
<p>Ac√° les presento un diagrama que simboliza este proceso:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM CV Reviewer 8556526ffb61499f8011fb71d42d7d69/Untitled 1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Image by diegulio</figcaption>
</figure>
</div>
<p>El objetivo final es poder representar cada pieza (<code>chunk</code>) de texto de forma sem√°ntica, para as√≠ poder hacer la relaci√≥n con la pregunta del usuario, y decidir que chunks incluir en el prompt final. Ahora naveguemos un poco por cada paso:</p>
<section id="load-documents" class="level3">
<h3 class="anchored" data-anchor-id="load-documents">1. Load Documents</h3>
<p>Esta etapa es bastante simple, s√≥lo requiere poder pasar los documentos a texto. Langchain posee una gran variedad de m√©todos para hacer esto, ac√° te dejo la <a href="https://python.langchain.com/docs/modules/data_connection/document_loaders/">documentaci√≥n</a>.</p>
<p>En nuestro caso es poder tomar un CV en PDF y extraer el texto de este. Tambi√©n podemos extraer texto de DataFrames, Json, Latex, Wikipedia, etc.</p>
</section>
<section id="split-documents" class="level3">
<h3 class="anchored" data-anchor-id="split-documents">2. Split Documents</h3>
<p>Existen varias formas para dividir documentos, puede ser por caracteres, por elementos Markdowns, tokens, etc. Creo que no vale la pena que yo te lo explique cuando existe una buena <a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/">documentaci√≥n</a> para aquello.</p>
<p>Ac√° hay dos conceptos que pueden ser importantes, que son el tama√±o del chunk <code>chunk_size</code> y el tama√±o del overlap en los distintos chunks <code>overlap_size</code>. Si el tama√±o del chunk es muy peque√±o entonces puede ser dif√≠cil extraer un buen contexto, pero si por el contrario es muy grande puede que estemos extrayendo informaci√≥n poco valiosa, y adem√°s arriesg√°ndonos podemos sobrepasar el <code>context_length</code>.</p>
<p>En general, los divisores de texto funcionan de la siguiente manera:</p>
<ol type="1">
<li>Dividir el texto en peque√±os fragmentos sem√°nticamente significativos (a menudo oraciones).</li>
<li>Comenzar a combinar estos peque√±os fragmentos en un fragmento m√°s grande hasta que alcance un cierto tama√±o <code>chunk_size</code> (seg√∫n lo medido por alguna funci√≥n).</li>
<li>Una vez que alcance ese tama√±o, hace que ese fragmento sea su propio fragmento de texto y luego comience a crear un nuevo fragmento de texto con algo de superposici√≥n <code>overlap_size</code> (para mantener el contexto entre los fragmentos).</li>
</ol>
</section>
<section id="embeddings" class="level3">
<h3 class="anchored" data-anchor-id="embeddings">3. Embeddings</h3>
<p>El concepto de Embeddings es muy importante en Machine Learning en general. Siendo muy utilizados cuando hablamos de textos. Puedes ver como en el post <strong><em><a href="https://diegulio.github.io/posts/kaggle_nlp_disaster/main.html">Identificando desastres en Twitter con NLP</a></em></strong> utilizamos embeddings para representar texto de forma num√©rica (usando vectores en este caso).</p>
<p>Podr√≠a escribir un blog entero sobre esto, pero creo que encontrar√°s mejor informaci√≥n en internet. Ac√° te dejo con un starter que puede ser la vieja y confiable <a href="https://en.wikipedia.org/wiki/Word_embedding">Wikipedia</a>.</p>
<p>Creo que s√≥lo nos basta con saber que podemos representar tanto palabras, car√°cteres, sentencias, documentos, etc con <strong>vectores</strong>. Adem√°s podemos calcular medidas de similaridad entre estos utilizando operaciones vectoriales, como el conocido producto punto, o calculando el coseno del √°ngulo entre los vectores. Vuelvo a repetir que este concepto es muy importante y que si lo desconoces debes ya ir a darle unas vueltas! üöÄ</p>
</section>
<section id="vector-stores" class="level3">
<h3 class="anchored" data-anchor-id="vector-stores">4. Vector Stores</h3>
<p>Imaginemos que tenemos este gran documento, el cual dividimos en distintos <code>chunks</code> y transformamos a vectores. Muchas veces el/los documentos ser√°n tan grandes que ni siquiera cabr√°n en la memoria de nuestro computador (en la RAM). Es por esto que se utilizan los llamados Vector Stores, que son almacenamientos de embeddings. Ac√° podremos guardar cada palabra, sentecia, documento con su respectivo embedding, para luego simplemente consultar esta base de datos y no tener que calcular el embedding todo el tiempo.</p>
<p>Existen vector stores que almacenan esta informaci√≥n en la nube, podr√°s encontrar compa√±√≠as que ofrecen estos servicios como <em>Pinecone</em>, <em>Weviate</em>, GCP con <em>Matching Engine</em>. Hay otros como <em>FAISS</em> por Facebook AI e incluso algunos que almacenan la informaci√≥n en la memoria RAM (si es posible) como <em>Chroma</em>.</p>
<p>Adem√°s, estas herramientas no s√≥lo ofrecen almacenar esta informaci√≥n, si no que tambi√©n calcular la relaci√≥n sem√°ntica entre alguna frase, pregunta, prompt y los vectores presentes en la base datos de forma muy eficiente.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM CV Reviewer 8556526ffb61499f8011fb71d42d7d69/Untitled 2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Source: <a href="https://python.langchain.com/docs/modules/data_connection/vectorstores/">https://python.langchain.com/docs/modules/data_connection/vectorstores/</a></figcaption>
</figure>
</div>
<p>Otro punto importante a considerar ac√° son los distintos m√©todos disponibles para calcular la similaridad entre la query y los vectores para obtener aquellos m√°s similares.</p>
</section>
</section>
<section id="retrievers" class="level2">
<h2 class="anchored" data-anchor-id="retrievers">Retrievers</h2>
<p>Volvamos al diagrama que teniamos hace un rato:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM CV Reviewer 8556526ffb61499f8011fb71d42d7d69/Untitled.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Image by diegulio</figcaption>
</figure>
</div>
<p>Habiamos quedado estancados con el hecho de que algunos documentos podrian no caber en el contexto entregado al prompt debidos al limitado <code>context_length</code>.</p>
<p>Ahora dividimos el documento, los convertimos a vectores, los almacenamos y adem√°s tenemos t√©cnicas para calcular la similaridad entre una query y los distintos <code>chunks</code>.</p>
<p>Entonces nuestra misi√≥n se resume a: <strong>En base a una pregunta, obtener las piezas de texto (chunks) m√°s relevantes para incluirlos en el prompt final.</strong></p>
<p>Para entenderlo mejor, consideremos como ejemplo nuestro caso de uso para chatear con los curriculums, tenemos la pregunta del usuario:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üëµüèΩ
</div>
</div>
<div class="callout-body-container callout-body">
<p>What is the candidate name?</p>
</div>
</div>
<p>E imaginemos que tenemos un CV muy grande (poco recomendable), algo del estilo:</p>
<pre><code>Nombre: Diego Machado
Edad: 25 a√±os
Hobbies: Ir al gym, jugar p√°del, jugar videojuegos

.... texto .... bla bla ..

Diego Machado estudi√≥ en .... bla bla

.... texto ...

Trabaj√≥ 1 a√±o en ... luego trabaj√≥ en ...

Sus habilidades son ....

.... texto .....

.... mucho texto ...</code></pre>
<p>Podemos ver que en este texto se encuentre la respuesta al prompt de manera expl√≠cita en dos ocasiones. Lo que esperamos que logre el procedimiento mostrado en <strong><em>Data Connection</em></strong> ser√° obtener los chunks de texto:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode latex code-with-copy"><code class="sourceCode latex"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Nombre: Diego Machado</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>Edad: 25 a√±os</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>Hobbies: Ir al gym, jugar p√°del, jugar videojuegos </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>y</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode latex code-with-copy"><code class="sourceCode latex"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>Diego Machado estudi√≥ en .... bla bla</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>.... texto ...  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As√≠, respetamos el <code>chunk_size</code> y adem√°s le entregamos los documentos m√°s relevantes al LLM en base al calculo de similaridad entre la pregunta <em>‚ÄúWhat is the candidate name?‚Äù</em> y los distintos chunks.</p>
<p>Entonces ahora tenemos un buen contexto que no incumple alguna norma, por lo que el prompt final se transforma en:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üëµüèΩ
</div>
</div>
<div class="callout-body-container callout-body">
<p>Utiliza las siguientes piezas de contexto para responder la pregunta al final. Por favor si no sabes la respuesta, s√≥lo di que no sabes, no intentes inventarla.</p>
<p>Nombre: Diego Machado Edad: 25 a√±os Hobbies: Ir al gym, jugar p√°del, jugar videojuegos Diego Machado estudi√≥ en ‚Ä¶. bla bla ‚Ä¶. texto ‚Ä¶</p>
<p>Pregunta: What is the candidate name?</p>
<p>Respuesta:</p>
</div>
</div>
<p>y lo que esperamos que suceda ser√° una respuesta del estilo:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
ü§ñ
</div>
</div>
<div class="callout-body-container callout-body">
<p>The candidate‚Äôs name is Diego Machado</p>
</div>
</div>
<p>Y listo! Asi podemos resolver el problema de los documentos largos. Entonces el diagrama anterior resulta en:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM CV Reviewer 8556526ffb61499f8011fb71d42d7d69/Untitled 3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Image by diegulio</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
‚òùüèΩ
</div>
</div>
<div class="callout-body-container callout-body">
<p>La forma en la que se decide que chunks son relevantes se les denomina <strong>retrievers</strong>, y existen diversas metodolog√≠as muy interesantes y robustas. Te invito a buscar m√°s informaci√≥n <a href="https://python.langchain.com/docs/modules/data_connection/retrievers/">ac√°</a></p>
</div>
</div>
<section id="chain-documents" class="level3">
<h3 class="anchored" data-anchor-id="chain-documents">Chain Documents</h3>
<p>ALTO AH√ç! A√∫n falta algo. Si nos fijamos bien, lo que hice con los chunks de texto relevantes fue simplemente concatenarlos! uno sobre el otro!</p>
<pre><code>Nombre: Diego Machado
Edad: 25 a√±os
Hobbies: Ir al gym, jugar p√°del, jugar videojuegos
Diego Machado estudi√≥ en .... bla bla
.... texto ...</code></pre>
<p>A esto se le llama <code>stuff</code>. Pero tambi√©n existen distintas metodolog√≠as para hacer esto de forma muy eficiente y robusta. Algunas de ellas incluso utilizan LLM auxiliares para refinar este contexto! Te invito a leer m√°s sobre eso <a href="https://python.langchain.com/docs/modules/chains/document/">ac√°</a>.</p>
<p>Fiuf! Creo que eso ser√≠a ‚Äútodo‚Äù, por √∫ltimo te dejo un diagrama general, el cual muestra todos los elementos utilizados:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM CV Reviewer 8556526ffb61499f8011fb71d42d7d69/Untitled.jpeg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><a href="https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/">https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/</a></figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="prototyping-code-time" class="level1">
<h1>üß†&nbsp; Prototyping: Code Time!</h1>
<p>Ahora es el momento de llevar todas estas ideas al c√≥digo.</p>
<p>Revisitemos el proceso que necesitamos:</p>
<ol type="1">
<li>Load Document: Cargar Curriculum</li>
<li>Splitting: Dividir informaci√≥n en chunks</li>
<li>Vector Store: Almacenamos embeddings</li>
<li>Create Conversational Retrieval Chain: Esto es b√°sicamente la creaci√≥n del bot.</li>
</ol>
<section id="load-document" class="level2">
<h2 class="anchored" data-anchor-id="load-document">1. Load Document</h2>
<p>Langchain tiene integraciones con un mont√≥n de tipos de documentos, para nuestro caso, en donde suponemos curriculums en pdf, debemos cargarlo de la siguiente forma:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.document_loaders <span class="im">import</span> PyPDFLoader</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;Langchain loader</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> PyPDFLoader(<span class="st">"../docs/CV_DMV.pdf"</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pages</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>pages <span class="op">=</span> loader.load()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Importante notar que tambi√©n podemos extraer metadata:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>pages[<span class="dv">0</span>].metadata</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;{'source': '../docs/CV_DMV.pdf', 'page': 0}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="splitting-documents" class="level2">
<h2 class="anchored" data-anchor-id="splitting-documents">2. Splitting documents</h2>
<p>En este caso utilizaremos el m√©todo Recursive Character Text Splitter.</p>
<p>Este separador de texto es el recomendado para texto gen√©rico. Se parametriza mediante una lista de caracteres. Se Intenta dividirlos en orden hasta que los trozos sean lo suficientemente peque√±os. La lista predeterminada es [‚Äú‚Äù, ‚Äú‚Äù, ‚Äù ‚Äú,‚Äù‚Äù]. Esto tiene el efecto de tratar de mantener todos los p√°rrafos (y luego las oraciones y luego las palabras) juntos el mayor tiempo posible, ya que gen√©ricamente parecer√≠an ser los fragmentos de texto m√°s relacionados sem√°nticamente.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.text_splitter <span class="im">import</span> RecursiveCharacterTextSplitter</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>text_splitter <span class="op">=</span> RecursiveCharacterTextSplitter(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set a really small chunk size, just to show.</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    chunk_size <span class="op">=</span> <span class="dv">1000</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    chunk_overlap  <span class="op">=</span> <span class="dv">100</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> text_splitter.split_documents(pages)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Notemos que ac√° aparecen los par√°metros mencionados anteriormente: <code>chunk_size</code>, <code>chunk_overlap</code></p>
<p>Lo que esta funci√≥n retorna es una lista de Documents</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>Document(page_content<span class="op">=</span><span class="st">'Nombre:DiegoMachadoEdad:25a√±os..'</span>, metadata<span class="op">=</span>{..}),</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>Document(page_content<span class="op">=</span><span class="st">'..texto..'</span>, metadata<span class="op">=</span>{..}),</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>Document(page_content<span class="op">=</span><span class="st">'DiegoMachadoestudi√≥en....'</span>, metadata<span class="op">=</span>{..}),</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="vector-store" class="level2">
<h2 class="anchored" data-anchor-id="vector-store">3. Vector Store</h2>
<p>Ahora creamos nuestro vector store, en este caso Chroma. Notar que debemos ingresar como par√°metro la funci√≥n de embeddings a utilizar. En este caso utilizamos Vertex AI Embeddings</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.vectorstores <span class="im">import</span> Chroma</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.llms <span class="im">import</span> VertexAI</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.embeddings <span class="im">import</span> VertexAIEmbeddings</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Embeddings fn</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> VertexAIEmbeddings(project<span class="op">=</span><span class="st">'gcp-project'</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Persist Directory</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>persist_directory <span class="op">=</span> <span class="st">'docs/chroma/'</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector db</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>vectordb <span class="op">=</span> Chroma.from_documents(</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    documents<span class="op">=</span>texts,</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    embedding<span class="op">=</span>embeddings,</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    persist_directory<span class="op">=</span>persist_directory</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Podemos utilizar persist directory si queremos almacenar el vector store en nuestro local, para asi no tener que cargarlo cada vez que lo instanciamos:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>vectordb.persist()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Utilizando el vector store podemos ocupar su funcionalidad de <code>similarity_search</code> para encontrar los chunks m√°s similares a una query en particular:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test embeddings</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What is the name of the candidate?"</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>docs <span class="op">=</span> vectordb.similarity_search(question,k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(docs[<span class="dv">0</span>].page_content[:<span class="dv">300</span>]) <span class="co"># Solo imprimiremos los primeros 300 caracteres del primer documento</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>DIEGOMACHADO
/_4782ndAugust1997 /_475dmachadovz@gmail.com /phone+56950917953 /map_markerSantiago,Chile
/linkedinwww.linkedin.com/in/DiegulioMachado /githubhttps://github.com/diegulio ·Ωë7diegulio.github.io
BRIEFDESCRIPTION
Industrialengineeringgraduatedwitha
master‚Äôsdegreeinengineeringsciences.
Passio</code></pre>
<p>Podemos notar que el nombre del candidato aparece en el documento m√°s similar a la query! esto cumple con nuestras expectativas üòé</p>
</section>
<section id="conversational-retrieval-chain" class="level2">
<h2 class="anchored" data-anchor-id="conversational-retrieval-chain">Conversational Retrieval Chain</h2>
<p>Ahora es donde podemos crear una cadena personalizada que tome el contexto seg√∫n la query, que cree el prompt final y que incluso vaya conservando la memoria.</p>
<p>La buena noticia es que Langchain ya tiene una cadena pre-construida que se encarga de todo esto! por lo que utilizarlo es muy f√°cil:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.memory <span class="im">import</span> ConversationBufferMemory</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Primero instanciamos el tipo de memoria</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> ConversationBufferMemory(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    memory_key<span class="op">=</span><span class="st">"chat_history"</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    return_messages<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Esta cadena personalizada necesita 3 cosas: un LLM, el retriever y la memoria. En este caso utilizaremos el retriever base de Chroma, pero recuerda que podemos plantear otros!</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.chains <span class="im">import</span> ConversationalRetrievalChain</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Instanciamos LLM</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> VertexAI(project_id <span class="op">=</span> <span class="st">'gcp-project'</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;Retriever</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>retriever<span class="op">=</span>vectordb.as_retriever()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>qa <span class="op">=</span> ConversationalRetrievalChain.from_llm(</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    llm,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    retriever<span class="op">=</span>retriever,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    memory<span class="op">=</span>memory</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Y listo! la parte del c√≥digo parece ser lo m√°s sencillo, todo gracias al framework Langchain ü¶ú‚õìÔ∏è‚ù§Ô∏è</p>
<p>Ahora podemos probarlo:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"Does the candidate has been teacher assistant?"</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> qa({<span class="st">"question"</span>: question})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>{'question': 'Does the candidate has been teacher assistant?',
 'chat_history': [HumanMessage(content='Does the candidate has been teacher assistant?', additional_kwargs={}, example=False),
  AIMessage(content='Yes, the candidate has been a teacher assistant.', additional_kwargs={}, example=False)],
 'answer': 'Yes, the candidate has been a teacher assistant.'}</code></pre>
<p>Si volvemos a preguntar, en el chat_history se ir√° guardando autom√°ticamente toda la info del chat, por lo que nuestro bot se acordar√° de preguntas anteriores ! ü§ñ</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"In which universities?"</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> qa({<span class="st">"question"</span>: question})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>{'question': 'In which universities?',
 'chat_history': [HumanMessage(content='Does the candidate has been teacher assistant?', additional_kwargs={}, example=False),
  AIMessage(content='Yes, the candidate has been a teacher assistant.', additional_kwargs={}, example=False),
  HumanMessage(content='In which universities?', additional_kwargs={}, example=False),
  AIMessage(content='The candidate has been a teacher assistant at Universidad de Santiago and Universidad Adolfo Iba√±ez.', additional_kwargs={}, example=False)],
 'answer': 'The candidate has been a teacher assistant at Universidad de Santiago and Universidad Adolfo Iba√±ez.'}</code></pre>
</section>
</section>
<section id="front-end" class="level1">
<h1>üßê&nbsp;Front-End</h1>
<p>Como siempre, nuestra aplicaci√≥n no puede quedar s√≥lo en palabras. En esta ocasi√≥n utilizamos los nuevos componentes de streamlit para chat!</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;Input de usuario</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> st.chat_input(<span class="st">"Ask something"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Respuesta (Puede ser de usuario, assistant, system o m√°s)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> st.chat_message(<span class="st">"user"</span>, avatar<span class="op">=</span>user_avatar):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    st.write(message.content)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Con estos elementos es que podemos construir algo as√≠:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LLM CV Reviewer 8556526ffb61499f8011fb71d42d7d69/Untitled 4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Front End</figcaption>
</figure>
</div>
<p>Para ver el c√≥digo utilizado para construir esta aplicaci√≥n puedes visitar el <a href="https://github.com/diegulio/llm-cv-helper">repositorio</a>.</p>
</section>
<section id="pr√≥ximos-pasos" class="level1">
<h1>üöÄ&nbsp;Pr√≥ximos Pasos</h1>
<p>Como mencion√© anteriormente, esta soluci√≥n puede mejorarse mucho m√°s. Ac√° una lluvia de ideas:</p>
<ul>
<li>Verificar que lo que se suba sea un CV</li>
<li>Aceptar m√°s tipos de documentos</li>
<li>Probar m√°s tipos de Retrievers</li>
<li>Probar m√°s tipos de Documents Chains</li>
<li>Aceptar m√∫ltiples CVs y poder comparar</li>
</ul>
</section>
<section id="conclusi√≥n" class="level1">
<h1>ü•≥&nbsp;Conclusi√≥n</h1>
<p>En conclusi√≥n, en este blog hemos explorado c√≥mo aprovechar los Large Language Models (LLM) para crear una aplicaci√≥n de chat que interact√∫a con documentos. Utilizamos el modelo PaLM de VertexAI junto con LangChain para orquestar toda la funcionalidad. Nuestro enfoque fue utilizar LLMs para leer y responder preguntas sobre curr√≠culums.</p>
<p>Aprendimos sobre la importancia de los embeddings para representar el texto en forma num√©rica, as√≠ como los Vector Stores para almacenar y recuperar eficientemente estos embeddings. Tambi√©n descubrimos c√≥mo resolver el problema de documentos largos utilizando t√©cnicas de divisi√≥n, embeddings y retrievers para mantener el contexto relevante en los prompts.</p>
<p>Nuestra aplicaci√≥n de chat con curr√≠culums puede ser ampliada para otros usos y aplicaciones, como asistentes para entrevistas de recursos humanos o cualquier caso donde se necesite interactuar con documentos de manera eficiente. Adem√°s, se pueden explorar otras metodolog√≠as de retrievers y document chains para mejorar a√∫n m√°s la experiencia.</p>
<p>En definitiva, esta exploraci√≥n ha sido solo el comienzo, y el potencial de los Large Language Models junto con herramientas como LangChain es emocionante. Con estos avances, podemos crear aplicaciones m√°s inteligentes y personalizadas, que nos ayuden en tareas complejas y mejoren nuestra interacci√≥n con la informaci√≥n.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">diegulioüß°2023</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>